{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Million Songs Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.080575</td>\n",
       "      <td>0.391265</td>\n",
       "      <td>1.826532</td>\n",
       "      <td>0.464657</td>\n",
       "      <td>-0.474730</td>\n",
       "      <td>-0.278204</td>\n",
       "      <td>-1.552371</td>\n",
       "      <td>-1.310845</td>\n",
       "      <td>0.387704</td>\n",
       "      <td>-0.666166</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085335</td>\n",
       "      <td>0.108508</td>\n",
       "      <td>0.142775</td>\n",
       "      <td>-0.237355</td>\n",
       "      <td>0.049233</td>\n",
       "      <td>-0.356182</td>\n",
       "      <td>0.544458</td>\n",
       "      <td>-0.470599</td>\n",
       "      <td>-0.255977</td>\n",
       "      <td>0.042292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.880919</td>\n",
       "      <td>0.332292</td>\n",
       "      <td>1.748539</td>\n",
       "      <td>0.721828</td>\n",
       "      <td>-0.164945</td>\n",
       "      <td>-1.191173</td>\n",
       "      <td>0.765681</td>\n",
       "      <td>0.109626</td>\n",
       "      <td>1.420941</td>\n",
       "      <td>0.414950</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.314250</td>\n",
       "      <td>0.306236</td>\n",
       "      <td>-0.069483</td>\n",
       "      <td>0.052017</td>\n",
       "      <td>-0.632328</td>\n",
       "      <td>-0.436057</td>\n",
       "      <td>0.556448</td>\n",
       "      <td>0.568744</td>\n",
       "      <td>0.206940</td>\n",
       "      <td>1.158587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.247622</td>\n",
       "      <td>0.592600</td>\n",
       "      <td>1.337173</td>\n",
       "      <td>0.750657</td>\n",
       "      <td>-0.001110</td>\n",
       "      <td>-0.702100</td>\n",
       "      <td>-0.060914</td>\n",
       "      <td>-0.069956</td>\n",
       "      <td>1.166254</td>\n",
       "      <td>-0.074608</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.396186</td>\n",
       "      <td>0.566683</td>\n",
       "      <td>-0.756534</td>\n",
       "      <td>-0.284019</td>\n",
       "      <td>-0.024220</td>\n",
       "      <td>0.223128</td>\n",
       "      <td>-0.509789</td>\n",
       "      <td>-0.338457</td>\n",
       "      <td>0.105819</td>\n",
       "      <td>-0.090208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.801044</td>\n",
       "      <td>-0.061805</td>\n",
       "      <td>0.783683</td>\n",
       "      <td>0.087218</td>\n",
       "      <td>0.329180</td>\n",
       "      <td>-1.298429</td>\n",
       "      <td>0.510714</td>\n",
       "      <td>-1.073355</td>\n",
       "      <td>-0.016803</td>\n",
       "      <td>-1.262655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.586237</td>\n",
       "      <td>-0.559427</td>\n",
       "      <td>-0.478689</td>\n",
       "      <td>-0.890161</td>\n",
       "      <td>-0.793906</td>\n",
       "      <td>0.567269</td>\n",
       "      <td>-0.263107</td>\n",
       "      <td>0.408116</td>\n",
       "      <td>0.967862</td>\n",
       "      <td>0.793384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.249775</td>\n",
       "      <td>0.793334</td>\n",
       "      <td>1.657037</td>\n",
       "      <td>0.447460</td>\n",
       "      <td>-0.406775</td>\n",
       "      <td>-0.567138</td>\n",
       "      <td>-0.692498</td>\n",
       "      <td>-0.952197</td>\n",
       "      <td>0.841844</td>\n",
       "      <td>-0.144910</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.181585</td>\n",
       "      <td>0.099672</td>\n",
       "      <td>0.191319</td>\n",
       "      <td>-0.585576</td>\n",
       "      <td>-0.111877</td>\n",
       "      <td>-0.219960</td>\n",
       "      <td>0.448804</td>\n",
       "      <td>0.256882</td>\n",
       "      <td>0.192038</td>\n",
       "      <td>1.241363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463710</th>\n",
       "      <td>0.493427</td>\n",
       "      <td>-0.336316</td>\n",
       "      <td>-0.084681</td>\n",
       "      <td>-0.658594</td>\n",
       "      <td>-1.673199</td>\n",
       "      <td>0.282011</td>\n",
       "      <td>-1.493945</td>\n",
       "      <td>-0.686837</td>\n",
       "      <td>0.831008</td>\n",
       "      <td>-1.058139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124247</td>\n",
       "      <td>0.991760</td>\n",
       "      <td>-0.324729</td>\n",
       "      <td>-0.395601</td>\n",
       "      <td>1.531331</td>\n",
       "      <td>0.085218</td>\n",
       "      <td>-0.001498</td>\n",
       "      <td>-0.322949</td>\n",
       "      <td>-0.616705</td>\n",
       "      <td>-0.697774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463711</th>\n",
       "      <td>-0.124812</td>\n",
       "      <td>0.200480</td>\n",
       "      <td>-0.926193</td>\n",
       "      <td>-0.897601</td>\n",
       "      <td>-1.643501</td>\n",
       "      <td>-0.375450</td>\n",
       "      <td>0.622636</td>\n",
       "      <td>-0.478409</td>\n",
       "      <td>0.758789</td>\n",
       "      <td>-1.016975</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.372135</td>\n",
       "      <td>1.868488</td>\n",
       "      <td>-0.549899</td>\n",
       "      <td>-0.652146</td>\n",
       "      <td>2.410029</td>\n",
       "      <td>0.030487</td>\n",
       "      <td>-1.189134</td>\n",
       "      <td>-0.120523</td>\n",
       "      <td>0.468067</td>\n",
       "      <td>-0.343264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463712</th>\n",
       "      <td>0.162997</td>\n",
       "      <td>0.006509</td>\n",
       "      <td>0.836644</td>\n",
       "      <td>0.067344</td>\n",
       "      <td>-0.367053</td>\n",
       "      <td>0.273406</td>\n",
       "      <td>-2.079877</td>\n",
       "      <td>-0.029351</td>\n",
       "      <td>-0.221383</td>\n",
       "      <td>-0.722086</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.103120</td>\n",
       "      <td>0.440416</td>\n",
       "      <td>-0.406601</td>\n",
       "      <td>-0.822279</td>\n",
       "      <td>0.864693</td>\n",
       "      <td>-0.687515</td>\n",
       "      <td>-0.265901</td>\n",
       "      <td>-0.663702</td>\n",
       "      <td>0.012903</td>\n",
       "      <td>-0.246192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463713</th>\n",
       "      <td>0.247234</td>\n",
       "      <td>0.249282</td>\n",
       "      <td>-0.407312</td>\n",
       "      <td>-1.278516</td>\n",
       "      <td>-2.290479</td>\n",
       "      <td>-0.373539</td>\n",
       "      <td>-0.105010</td>\n",
       "      <td>0.164208</td>\n",
       "      <td>-0.146179</td>\n",
       "      <td>0.030704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367180</td>\n",
       "      <td>1.480824</td>\n",
       "      <td>0.126993</td>\n",
       "      <td>-0.701360</td>\n",
       "      <td>1.256892</td>\n",
       "      <td>0.637414</td>\n",
       "      <td>0.344071</td>\n",
       "      <td>0.532877</td>\n",
       "      <td>0.466592</td>\n",
       "      <td>0.700802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463714</th>\n",
       "      <td>1.142946</td>\n",
       "      <td>0.105125</td>\n",
       "      <td>1.287201</td>\n",
       "      <td>-0.534488</td>\n",
       "      <td>-1.429635</td>\n",
       "      <td>-0.111189</td>\n",
       "      <td>-2.616308</td>\n",
       "      <td>-0.231192</td>\n",
       "      <td>0.269906</td>\n",
       "      <td>-0.135636</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.249371</td>\n",
       "      <td>0.428395</td>\n",
       "      <td>-0.235258</td>\n",
       "      <td>-0.209710</td>\n",
       "      <td>-0.111101</td>\n",
       "      <td>-0.330209</td>\n",
       "      <td>0.099508</td>\n",
       "      <td>-0.051046</td>\n",
       "      <td>-0.339430</td>\n",
       "      <td>-0.057782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>463715 rows Ã— 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6   \\\n",
       "0       1.080575  0.391265  1.826532  0.464657 -0.474730 -0.278204 -1.552371   \n",
       "1       0.880919  0.332292  1.748539  0.721828 -0.164945 -1.191173  0.765681   \n",
       "2       1.247622  0.592600  1.337173  0.750657 -0.001110 -0.702100 -0.060914   \n",
       "3       0.801044 -0.061805  0.783683  0.087218  0.329180 -1.298429  0.510714   \n",
       "4       1.249775  0.793334  1.657037  0.447460 -0.406775 -0.567138 -0.692498   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "463710  0.493427 -0.336316 -0.084681 -0.658594 -1.673199  0.282011 -1.493945   \n",
       "463711 -0.124812  0.200480 -0.926193 -0.897601 -1.643501 -0.375450  0.622636   \n",
       "463712  0.162997  0.006509  0.836644  0.067344 -0.367053  0.273406 -2.079877   \n",
       "463713  0.247234  0.249282 -0.407312 -1.278516 -2.290479 -0.373539 -0.105010   \n",
       "463714  1.142946  0.105125  1.287201 -0.534488 -1.429635 -0.111189 -2.616308   \n",
       "\n",
       "              7         8         9   ...        80        81        82  \\\n",
       "0      -1.310845  0.387704 -0.666166  ... -0.085335  0.108508  0.142775   \n",
       "1       0.109626  1.420941  0.414950  ... -0.314250  0.306236 -0.069483   \n",
       "2      -0.069956  1.166254 -0.074608  ... -0.396186  0.566683 -0.756534   \n",
       "3      -1.073355 -0.016803 -1.262655  ...  0.586237 -0.559427 -0.478689   \n",
       "4      -0.952197  0.841844 -0.144910  ... -0.181585  0.099672  0.191319   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "463710 -0.686837  0.831008 -1.058139  ...  0.124247  0.991760 -0.324729   \n",
       "463711 -0.478409  0.758789 -1.016975  ... -0.372135  1.868488 -0.549899   \n",
       "463712 -0.029351 -0.221383 -0.722086  ... -0.103120  0.440416 -0.406601   \n",
       "463713  0.164208 -0.146179  0.030704  ...  0.367180  1.480824  0.126993   \n",
       "463714 -0.231192  0.269906 -0.135636  ... -0.249371  0.428395 -0.235258   \n",
       "\n",
       "              83        84        85        86        87        88        89  \n",
       "0      -0.237355  0.049233 -0.356182  0.544458 -0.470599 -0.255977  0.042292  \n",
       "1       0.052017 -0.632328 -0.436057  0.556448  0.568744  0.206940  1.158587  \n",
       "2      -0.284019 -0.024220  0.223128 -0.509789 -0.338457  0.105819 -0.090208  \n",
       "3      -0.890161 -0.793906  0.567269 -0.263107  0.408116  0.967862  0.793384  \n",
       "4      -0.585576 -0.111877 -0.219960  0.448804  0.256882  0.192038  1.241363  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "463710 -0.395601  1.531331  0.085218 -0.001498 -0.322949 -0.616705 -0.697774  \n",
       "463711 -0.652146  2.410029  0.030487 -1.189134 -0.120523  0.468067 -0.343264  \n",
       "463712 -0.822279  0.864693 -0.687515 -0.265901 -0.663702  0.012903 -0.246192  \n",
       "463713 -0.701360  1.256892  0.637414  0.344071  0.532877  0.466592  0.700802  \n",
       "463714 -0.209710 -0.111101 -0.330209  0.099508 -0.051046 -0.339430 -0.057782  \n",
       "\n",
       "[463715 rows x 90 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge,RidgeCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "df=pd.read_csv('YearPredictionMSD.txt',header=None,sep = ',')\n",
    "tt=463715\n",
    "ee=51630\n",
    "train=df.iloc[:tt] \n",
    "test=df.iloc[tt:] \n",
    "\n",
    "train_y = train[0]\n",
    "test_y = test[0] #response:year\n",
    "train_x = train\n",
    "test_x = test\n",
    "del train_x[0]\n",
    "del test_x[0]\n",
    "total_x=pd.concat([train_x,test_x])\n",
    "x=preprocessing.StandardScaler().fit(total_x).transform(total_x) #normalize\n",
    "x=pd.DataFrame(x)\n",
    "train_x=x.iloc[:tt]\n",
    "test_x=x.iloc[tt:]\n",
    "\n",
    "train_x.reset_index(drop=True, inplace=True) \n",
    "test_x.reset_index(drop=True, inplace=True) \n",
    "train_y.reset_index(drop=True, inplace=True) \n",
    "test_y.reset_index(drop=True, inplace=True) \n",
    "\n",
    "train_x ##display the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nntrain_x = torch.from_numpy(train_x.to_numpy()).float()\n",
    "nntrain_y = torch.squeeze(torch.from_numpy(train_y.to_numpy()).float()) \n",
    "nntest_x= torch.from_numpy(test_x.to_numpy()).float()\n",
    "nntest_y = torch.squeeze(torch.from_numpy(test_y.to_numpy()).float())\n",
    "\n",
    "class mydataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "        self._len = len(x)\n",
    "\n",
    "    def __getitem__(self, item): \n",
    "        return self._x[item], self._y[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=90, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "cpu\n",
      "epoch 0\n",
      "            Train set - loss: 756400.875\n",
      "            Test  set - loss: 739168.6875\n",
      "            \n",
      "epoch 2\n",
      "            Train set - loss: 20947.265625\n",
      "            Test  set - loss: 18800.251953125\n",
      "            \n",
      "epoch 4\n",
      "            Train set - loss: 1072.22509765625\n",
      "            Test  set - loss: 890.6712036132812\n",
      "            \n",
      "epoch 6\n",
      "            Train set - loss: 294.0543212890625\n",
      "            Test  set - loss: 398.8669738769531\n",
      "            \n",
      "epoch 8\n",
      "            Train set - loss: 235.96817016601562\n",
      "            Test  set - loss: 378.2474670410156\n",
      "            \n",
      "epoch 10\n",
      "            Train set - loss: 209.79269409179688\n",
      "            Test  set - loss: 376.4117736816406\n",
      "            \n",
      "epoch 12\n",
      "            Train set - loss: 210.08470153808594\n",
      "            Test  set - loss: 376.13946533203125\n",
      "            \n",
      "epoch 14\n",
      "            Train set - loss: 223.03128051757812\n",
      "            Test  set - loss: 376.08477783203125\n",
      "            \n",
      "epoch 16\n",
      "            Train set - loss: 217.9310302734375\n",
      "            Test  set - loss: 376.1070556640625\n",
      "            \n",
      "epoch 18\n",
      "            Train set - loss: 224.69923400878906\n",
      "            Test  set - loss: 376.0879211425781\n",
      "            \n",
      "DNN complexity and model fitted in 212.476 s\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(mydataset(nntrain_x, nntrain_y),batch_size=1024, shuffle=True)\n",
    "test_loader = DataLoader(mydataset(nntest_x, nntest_y),batch_size=1024,shuffle=False)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1 = nn.Linear(90, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "\n",
    "#initialize\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.normal_(m.weight,mean=0,std=0.5)\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.uniform_(m.weight,a=0,b=0.1)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "torch.manual_seed(0)\n",
    "net.apply(init_weights)\n",
    "print(net)\n",
    "print(device)\n",
    "criterion=nn.MSELoss() \n",
    "optimizer=optim.SGD(net.parameters(),lr=1e-4,momentum=0.9,weight_decay=1e-2) #optim.Adam(...)\n",
    "dnn_trainloss=[]\n",
    "dnn_testloss=[]\n",
    "t0=time.time()\n",
    "#tb=0\n",
    "#tc=0\n",
    "\n",
    "for epoch in range(20): \n",
    "    for x, y in train_loader: #for batch, (x, y) in enumerate(train_loader): \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        #print(x.shape,y.shape,x.device, y.device,net.fc1.weight.device)\n",
    "        #td=time.time()-tc \n",
    "        #ta=time.time()\n",
    "        # Compute prediction error\n",
    "        y_pred = net(x)\n",
    "        y_pred = torch.squeeze(y_pred)\n",
    "        train_loss = criterion(y_pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(tb/(tb+td))\n",
    "        #tb=time.time()-ta\n",
    "        #tc=time.time()\n",
    "\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_test_pred = net(x)\n",
    "        y_test_pred = torch.squeeze(y_test_pred)\n",
    "    \n",
    "        test_loss = criterion(y_test_pred,y)\n",
    "    \n",
    "    if epoch>5 and float(test_loss)>max(dnn_testloss[-5:-1]):\n",
    "        break\n",
    "    \n",
    "    if epoch % 2 == 0:        \n",
    "        print(f'''epoch {epoch}\n",
    "            Train set - loss: {train_loss}\n",
    "            Test  set - loss: {test_loss}\n",
    "            ''')\n",
    "    \n",
    "    dnn_trainloss.append(float(train_loss))\n",
    "    dnn_testloss.append(float(test_loss))\n",
    "            \n",
    "dnn_fit = time.time() - t0\n",
    "print(\"DNN complexity and model fitted in %.3f s\" % dnn_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1c14cd4a8b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(dnn_trainloss[5:])\n",
    "plt.plot(dnn_testloss[5:])\n",
    "plt.legend([\"train\",\"test\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (rblock1): ResidualBlock(\n",
      "    (fc1): Linear(in_features=90, out_features=256, bias=True)\n",
      "    (fc2): Linear(in_features=90, out_features=256, bias=True)\n",
      "  )\n",
      "  (rblock2): ResidualBlock(\n",
      "    (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (rblock3): ResidualBlock(\n",
      "    (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  )\n",
      "  (fc4): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0\n",
      "            Train set - loss: 558444.125\n",
      "            Test  set - loss: 543718.4375\n",
      "            \n",
      "epoch 2\n",
      "            Train set - loss: 15939.4345703125\n",
      "            Test  set - loss: 13977.3974609375\n",
      "            \n",
      "epoch 4\n",
      "            Train set - loss: 902.9437866210938\n",
      "            Test  set - loss: 764.0151977539062\n",
      "            \n",
      "epoch 6\n",
      "            Train set - loss: 278.7446594238281\n",
      "            Test  set - loss: 394.35369873046875\n",
      "            \n",
      "epoch 8\n",
      "            Train set - loss: 233.9576873779297\n",
      "            Test  set - loss: 377.9190368652344\n",
      "            \n",
      "epoch 10\n",
      "            Train set - loss: 223.919921875\n",
      "            Test  set - loss: 376.3535461425781\n",
      "            \n",
      "epoch 12\n",
      "            Train set - loss: 224.77151489257812\n",
      "            Test  set - loss: 376.1485900878906\n",
      "            \n",
      "epoch 14\n",
      "            Train set - loss: 226.82012939453125\n",
      "            Test  set - loss: 376.1047668457031\n",
      "            \n",
      "epoch 16\n",
      "            Train set - loss: 229.092529296875\n",
      "            Test  set - loss: 376.1102600097656\n",
      "            \n",
      "epoch 18\n",
      "            Train set - loss: 214.8970184326172\n",
      "            Test  set - loss: 376.10296630859375\n",
      "            \n",
      "ResNet complexity and model fitted in 292.899 s\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(mydataset(nntrain_x, nntrain_y),batch_size=1024, shuffle=True)\n",
    "test_loader = DataLoader(mydataset(nntest_x, nntest_y),batch_size=1024, shuffle=False)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,infeatures,outfeatures):\n",
    "        super(ResidualBlock,self).__init__()\n",
    "        self.infeatures = infeatures\n",
    "        self.outfeatures = outfeatures\n",
    "        self.fc1 = nn.Linear(infeatures,outfeatures)\n",
    "        self.fc2 = nn.Linear(infeatures,outfeatures)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.fc1(x)\n",
    "        y= F.relu(y)\n",
    "        x = self.fc2(x)\n",
    "        return F.relu(x+y)\n",
    "\n",
    "\n",
    "class ResNet(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.rblock1 = ResidualBlock(90,256)\n",
    "        self.rblock2 = ResidualBlock(256,128)\n",
    "        self.rblock3 = ResidualBlock(128,64)\n",
    "        self.fc4 = nn.Linear(64,1)\n",
    "    \n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.rblock1(x)\n",
    "        x = self.rblock2(x)\n",
    "        x = self.rblock3(x)\n",
    "        return self.fc4(x)\n",
    "#initialize\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.normal_(m.weight,mean=0,std=0.5)\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.uniform_(m.weight,a=0,b=0.1)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = ResNet()\n",
    "net = net.to(device)\n",
    "torch.manual_seed(0)\n",
    "net.apply(init_weights)\n",
    "print(net)\n",
    "criterion=nn.MSELoss() \n",
    "optimizer=optim.SGD(net.parameters(),lr=1e-4,momentum=0.9,weight_decay=1e-2) #optim.Adam(...)\n",
    "res_trainloss=[]\n",
    "res_testloss=[]\n",
    "t0=time.time()\n",
    "for epoch in range(20): \n",
    "    for x, y in train_loader: #for batch, (x, y) in enumerate(train_loader): \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        y_pred = net(x)\n",
    "        y_pred = torch.squeeze(y_pred)\n",
    "        train_loss = criterion(y_pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_test_pred = net(x)\n",
    "        y_test_pred = torch.squeeze(y_test_pred)\n",
    "    \n",
    "        test_loss = criterion(y_test_pred,y)\n",
    "    \n",
    "    if epoch>5 and float(test_loss)>max(res_testloss[-5:-1]):\n",
    "        break\n",
    "    \n",
    "    if epoch % 2 == 0:        \n",
    "        print(f'''epoch {epoch}\n",
    "            Train set - loss: {train_loss}\n",
    "            Test  set - loss: {test_loss}\n",
    "            ''')\n",
    "    res_trainloss.append(float(train_loss))\n",
    "    res_testloss.append(float(test_loss))\n",
    "            \n",
    "dnn_fit = time.time() - t0\n",
    "print(\"ResNet complexity and model fitted in %.3f s\" % dnn_fit)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_1d(pdf, gamma, device):\n",
    "    if pdf == 'G':\n",
    "        w = torch.randn(1, device=device) * gamma\n",
    "        return w\n",
    "    elif pdf == 'L':\n",
    "        w = torch.distributions.laplace.Laplace(torch.tensor([0.0], device=device), torch.tensor([1.0], device=device)).sample() * gamma\n",
    "        return w\n",
    "    elif pdf == 'C':\n",
    "        w = torch.distributions.cauchy.Cauchy(torch.tensor([0.0], device=device), torch.tensor([1.0], device=device)).sample() * gamma\n",
    "        return w\n",
    "    \n",
    "def sample(pdf, gamma, d, device):\n",
    "    return torch.tensor([sample_1d(pdf, gamma, device) for _ in range(d)], device=device)\n",
    "\n",
    "class RandomFourierFeature:\n",
    "    \"\"\"Random Fourier Feature\n",
    "    Parameters\n",
    "    ----------\n",
    "    d : int\n",
    "        Input space dimension\n",
    "    D : int\n",
    "        Feature space dimension\n",
    "    W : shape (D,d)\n",
    "    b : shape (D)\n",
    "    kernel : char\n",
    "        Kernel to use; 'G', 'L', or 'C'\n",
    "    gamma : float\n",
    "        pdf parameter\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d, D, W=None, b=None, kernel='G', gamma=1, device='cpu'):\n",
    "        self.d = d\n",
    "        self.D = D\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "\n",
    "        kernel = kernel.upper()\n",
    "        if kernel not in ['G', 'L', 'C']:\n",
    "            raise Exception('Invalid Kernel')\n",
    "        self.kernel = kernel\n",
    "\n",
    "        if W is None or b is None:\n",
    "            self.create()\n",
    "        else:\n",
    "            self.__load(W, b)\n",
    "\n",
    "    def __load(self, W, b):\n",
    "        \"\"\"Load from existing Arrays\"\"\"\n",
    "\n",
    "        self.W = W.reshape([self.D, self.d])\n",
    "        self.b = b\n",
    "    \n",
    "    \n",
    "    def create(self):\n",
    "        \"\"\"Create a d->D fourier random feature\"\"\"\n",
    "        self.b = torch.rand(self.D, device=self.device) * 2 * torch.pi\n",
    "        self.W = sample(self.kernel, self.gamma, self.d * self.D, self.device).reshape(self.D, self.d)\n",
    "\n",
    "    def transform(self, x):\n",
    "        \"\"\"Transform a vector using this feature\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (shape=(n,d))\n",
    "            to transform; must be single dimension vector\n",
    "        Returns\n",
    "        -------\n",
    "        x : (shape=(n,D))\n",
    "            Feature space transformation of x\n",
    "        \"\"\"\n",
    "        #print(self.W.shape,self.b.reshape(-1,1).shape,x.shape)\n",
    "        #print((self.W @ x.T).shape)\n",
    "       \n",
    "\n",
    "        result = torch.sqrt(torch.tensor([2.0 / self.D], device=x.device)) * torch.cos(\n",
    "            self.W @ x.T + (self.b.reshape(-1, 1) @ torch.ones((1,len(x)), device=x.device))\n",
    "        )\n",
    "        return result.T\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KernelNet(\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0\n",
      "            Train set - loss: 1622537.875\n",
      "            Test  set - loss: 1607737.5\n",
      "            \n",
      "epoch 2\n",
      "            Train set - loss: 249715.890625\n",
      "            Test  set - loss: 241782.375\n",
      "            \n",
      "epoch 4\n",
      "            Train set - loss: 2375.78759765625\n",
      "            Test  set - loss: 1864.0576171875\n",
      "            \n",
      "epoch 6\n",
      "            Train set - loss: 173.62278747558594\n",
      "            Test  set - loss: 408.0314025878906\n",
      "            \n",
      "epoch 8\n",
      "            Train set - loss: 124.6273193359375\n",
      "            Test  set - loss: 361.9786071777344\n",
      "            \n",
      "epoch 10\n",
      "            Train set - loss: 123.11907196044922\n",
      "            Test  set - loss: 368.41131591796875\n",
      "            \n",
      "epoch 12\n",
      "            Train set - loss: 121.05491638183594\n",
      "            Test  set - loss: 341.9067077636719\n",
      "            \n",
      "epoch 14\n",
      "            Train set - loss: 117.5800552368164\n",
      "            Test  set - loss: 344.879150390625\n",
      "            \n",
      "KernelNet complexity and model fitted in 169.651 s\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(mydataset(nntrain_x, nntrain_y),batch_size=1024, shuffle=True)\n",
    "test_loader = DataLoader(mydataset(nntest_x, nntest_y),batch_size=1024, shuffle=False)\n",
    "\n",
    "rff0=RandomFourierFeature(90,256,kernel='C',gamma=0.02)\n",
    "rff1=RandomFourierFeature(128,128,kernel='G',gamma=0.1)\n",
    "rff2=RandomFourierFeature(64,64,kernel='G',gamma=0.5)\n",
    "\n",
    "class KernelNet(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(KernelNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = rff0.transform(x)\n",
    "        x=self.fc1(x)\n",
    "        x = rff1.transform(x)\n",
    "        x=self.fc2(x)\n",
    "        x = rff2.transform(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "#initialize\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.normal_(m.weight,mean=0,std=0.5)\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.uniform_(m.weight,a=0,b=1)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = KernelNet()\n",
    "net = net.to(device)\n",
    "torch.manual_seed(1)\n",
    "net.apply(init_weights)\n",
    "print(net)\n",
    "criterion=nn.MSELoss() \n",
    "optimizer=optim.SGD(net.parameters(),lr=5e-5,momentum=0.9,weight_decay=1e-2) #optim.Adam(...)\n",
    "\n",
    "loss=[]\n",
    "kernelnn_trainloss=[]\n",
    "kernelnn_testloss=[]\n",
    "t0 = time.time()\n",
    "for epoch in range(20): \n",
    "    for x, y in train_loader: #for batch, (x, y) in enumerate(train_loader): \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        y_pred = net(x)\n",
    "        y_pred = torch.squeeze(y_pred)\n",
    "        train_loss = criterion(y_pred, y)\n",
    "        loss.append(train_loss)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_test_pred = net(x)\n",
    "        y_test_pred = torch.squeeze(y_test_pred)\n",
    "\n",
    "        test_loss = criterion(y_test_pred,y)\n",
    "            \n",
    "    if epoch>5 and float(test_loss)>max(kernelnn_testloss[-5:-1]):\n",
    "        break\n",
    "    \n",
    "    \n",
    "    if epoch % 2 == 0:         \n",
    "        print(f'''epoch {epoch}\n",
    "            Train set - loss: {train_loss}\n",
    "            Test  set - loss: {test_loss}\n",
    "            ''')\n",
    "   \n",
    "    kernelnn_trainloss.append(float(train_loss))\n",
    "    kernelnn_testloss.append(float(test_loss))\n",
    "        \n",
    "    \n",
    "dnn_fit = time.time() - t0\n",
    "print(\"KernelNet complexity and model fitted in %.3f s\" % dnn_fit)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResKernelNet(\n",
      "  (rblock1): ResidualBlock(\n",
      "    (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (rblock2): ResidualBlock(\n",
      "    (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0\n",
      "            Train set - loss: 115.25847625732422\n",
      "            Test  set - loss: 433.5244445800781\n",
      "            \n",
      "epoch 2\n",
      "            Train set - loss: 110.26060485839844\n",
      "            Test  set - loss: 423.6630554199219\n",
      "            \n",
      "epoch 4\n",
      "            Train set - loss: 111.69669342041016\n",
      "            Test  set - loss: 433.60968017578125\n",
      "            \n",
      "epoch 6\n",
      "            Train set - loss: 115.14805603027344\n",
      "            Test  set - loss: 412.3328552246094\n",
      "            \n",
      "epoch 8\n",
      "            Train set - loss: 104.0451431274414\n",
      "            Test  set - loss: 403.60015869140625\n",
      "            \n",
      "epoch 10\n",
      "            Train set - loss: 102.02762603759766\n",
      "            Test  set - loss: 395.99481201171875\n",
      "            \n",
      "epoch 12\n",
      "            Train set - loss: 105.70458221435547\n",
      "            Test  set - loss: 379.203125\n",
      "            \n",
      "epoch 14\n",
      "            Train set - loss: 100.95287322998047\n",
      "            Test  set - loss: 369.4557189941406\n",
      "            \n",
      "epoch 16\n",
      "            Train set - loss: 107.42433166503906\n",
      "            Test  set - loss: 364.2837219238281\n",
      "            \n",
      "epoch 18\n",
      "            Train set - loss: 98.3117904663086\n",
      "            Test  set - loss: 373.77685546875\n",
      "            \n",
      "Residual KernelNet complexity and model fitted in 270.103 s\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(mydataset(nntrain_x, nntrain_y),batch_size=1024, shuffle=True)\n",
    "test_loader = DataLoader(mydataset(nntest_x, nntest_y),batch_size=1024, shuffle=False)\n",
    "\n",
    "rff0=RandomFourierFeature(90,256,kernel='C',gamma=0.02)\n",
    "rff1=RandomFourierFeature(128,128,kernel='G',gamma=0.1)\n",
    "rff2=RandomFourierFeature(64,64,kernel='G',gamma=0.5)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,infeatures,outfeatures,rff):\n",
    "        super(ResidualBlock,self).__init__()\n",
    "        self.infeatures = infeatures\n",
    "        self.outfeatures = outfeatures\n",
    "        self.rff=rff\n",
    "        \n",
    "        self.fc1 = nn.Linear(infeatures,outfeatures)\n",
    "        self.fc2 = nn.Linear(outfeatures,outfeatures)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        rff=self.rff\n",
    "        x = self.fc1(x)\n",
    "        y = rff.transform(x)\n",
    "        y = self.fc2(y)\n",
    "        return x+y\n",
    "\n",
    "class ResKernelNet(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(ResKernelNet, self).__init__()\n",
    "        self.rblock1 = ResidualBlock(256,128,rff1)\n",
    "        self.rblock2 = ResidualBlock(128,64,rff2)\n",
    "        self.fc3 =nn.Linear(64,1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = rff0.transform(x)\n",
    "        x = self.rblock1(x)\n",
    "        x = self.rblock2(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "#initialize\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.normal_(m.weight,mean=0,std=0.5)\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.uniform_(m.weight,a=0,b=1)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = ResKernelNet()\n",
    "net = net.to(device)\n",
    "torch.manual_seed(1)\n",
    "#net.apply(init_weights)\n",
    "print(net)\n",
    "criterion=nn.MSELoss() \n",
    "optimizer=optim.SGD(net.parameters(),lr=1e-6,momentum=0.9,weight_decay=1e-2) #optim.Adam(...)\n",
    "\n",
    "loss=[]\n",
    "reskernel_trainloss=[]\n",
    "reskernel_testloss=[]\n",
    "t0 = time.time()\n",
    "for epoch in range(20): \n",
    "    for x, y in train_loader: #for batch, (x, y) in enumerate(train_loader): \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        y_pred = net(x)\n",
    "        y_pred = torch.squeeze(y_pred)\n",
    "        train_loss = criterion(y_pred, y)\n",
    "        loss.append(train_loss)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_test_pred = net(x)\n",
    "        y_test_pred = torch.squeeze(y_test_pred)\n",
    "\n",
    "        test_loss = criterion(y_test_pred,y)\n",
    "            \n",
    "    if epoch>5 and float(test_loss)>max(reskernel_testloss[-5:-1]):\n",
    "        break\n",
    "    \n",
    "    \n",
    "    if epoch % 2 == 0:         \n",
    "        print(f'''epoch {epoch}\n",
    "            Train set - loss: {train_loss}\n",
    "            Test  set - loss: {test_loss}\n",
    "            ''')\n",
    "    reskernel_trainloss.append(float(train_loss))\n",
    "    reskernel_testloss.append(float(test_loss))\n",
    "        \n",
    "    \n",
    "dnn_fit = time.time() - t0\n",
    "print(\"Residual KernelNet complexity and model fitted in %.3f s\" % dnn_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376.08343505859375 376.1024475097656 347.1639404296875 351.1676940917969\n"
     ]
    }
   ],
   "source": [
    "print(dnn_testloss[-1],res_testloss[-1],kernelnn_testloss[-1],reskernel_testloss[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
