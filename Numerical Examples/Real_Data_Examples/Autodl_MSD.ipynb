{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Million Songs Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.080575</td>\n",
       "      <td>0.391265</td>\n",
       "      <td>1.826532</td>\n",
       "      <td>0.464657</td>\n",
       "      <td>-0.474730</td>\n",
       "      <td>-0.278204</td>\n",
       "      <td>-1.552371</td>\n",
       "      <td>-1.310845</td>\n",
       "      <td>0.387704</td>\n",
       "      <td>-0.666166</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085335</td>\n",
       "      <td>0.108508</td>\n",
       "      <td>0.142775</td>\n",
       "      <td>-0.237355</td>\n",
       "      <td>0.049233</td>\n",
       "      <td>-0.356182</td>\n",
       "      <td>0.544458</td>\n",
       "      <td>-0.470599</td>\n",
       "      <td>-0.255977</td>\n",
       "      <td>0.042292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.880919</td>\n",
       "      <td>0.332292</td>\n",
       "      <td>1.748539</td>\n",
       "      <td>0.721828</td>\n",
       "      <td>-0.164945</td>\n",
       "      <td>-1.191173</td>\n",
       "      <td>0.765681</td>\n",
       "      <td>0.109626</td>\n",
       "      <td>1.420941</td>\n",
       "      <td>0.414950</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.314250</td>\n",
       "      <td>0.306236</td>\n",
       "      <td>-0.069483</td>\n",
       "      <td>0.052017</td>\n",
       "      <td>-0.632328</td>\n",
       "      <td>-0.436057</td>\n",
       "      <td>0.556448</td>\n",
       "      <td>0.568744</td>\n",
       "      <td>0.206940</td>\n",
       "      <td>1.158587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.247622</td>\n",
       "      <td>0.592600</td>\n",
       "      <td>1.337173</td>\n",
       "      <td>0.750657</td>\n",
       "      <td>-0.001110</td>\n",
       "      <td>-0.702100</td>\n",
       "      <td>-0.060914</td>\n",
       "      <td>-0.069956</td>\n",
       "      <td>1.166254</td>\n",
       "      <td>-0.074608</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.396186</td>\n",
       "      <td>0.566683</td>\n",
       "      <td>-0.756534</td>\n",
       "      <td>-0.284019</td>\n",
       "      <td>-0.024220</td>\n",
       "      <td>0.223128</td>\n",
       "      <td>-0.509789</td>\n",
       "      <td>-0.338457</td>\n",
       "      <td>0.105819</td>\n",
       "      <td>-0.090208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.801044</td>\n",
       "      <td>-0.061805</td>\n",
       "      <td>0.783683</td>\n",
       "      <td>0.087218</td>\n",
       "      <td>0.329180</td>\n",
       "      <td>-1.298429</td>\n",
       "      <td>0.510714</td>\n",
       "      <td>-1.073355</td>\n",
       "      <td>-0.016803</td>\n",
       "      <td>-1.262655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.586237</td>\n",
       "      <td>-0.559427</td>\n",
       "      <td>-0.478689</td>\n",
       "      <td>-0.890161</td>\n",
       "      <td>-0.793906</td>\n",
       "      <td>0.567269</td>\n",
       "      <td>-0.263107</td>\n",
       "      <td>0.408116</td>\n",
       "      <td>0.967862</td>\n",
       "      <td>0.793384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.249775</td>\n",
       "      <td>0.793334</td>\n",
       "      <td>1.657037</td>\n",
       "      <td>0.447460</td>\n",
       "      <td>-0.406775</td>\n",
       "      <td>-0.567138</td>\n",
       "      <td>-0.692498</td>\n",
       "      <td>-0.952197</td>\n",
       "      <td>0.841844</td>\n",
       "      <td>-0.144910</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.181585</td>\n",
       "      <td>0.099672</td>\n",
       "      <td>0.191319</td>\n",
       "      <td>-0.585576</td>\n",
       "      <td>-0.111877</td>\n",
       "      <td>-0.219960</td>\n",
       "      <td>0.448804</td>\n",
       "      <td>0.256882</td>\n",
       "      <td>0.192038</td>\n",
       "      <td>1.241363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463710</th>\n",
       "      <td>0.493427</td>\n",
       "      <td>-0.336316</td>\n",
       "      <td>-0.084681</td>\n",
       "      <td>-0.658594</td>\n",
       "      <td>-1.673199</td>\n",
       "      <td>0.282011</td>\n",
       "      <td>-1.493945</td>\n",
       "      <td>-0.686837</td>\n",
       "      <td>0.831008</td>\n",
       "      <td>-1.058139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124247</td>\n",
       "      <td>0.991760</td>\n",
       "      <td>-0.324729</td>\n",
       "      <td>-0.395601</td>\n",
       "      <td>1.531331</td>\n",
       "      <td>0.085218</td>\n",
       "      <td>-0.001498</td>\n",
       "      <td>-0.322949</td>\n",
       "      <td>-0.616705</td>\n",
       "      <td>-0.697774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463711</th>\n",
       "      <td>-0.124812</td>\n",
       "      <td>0.200480</td>\n",
       "      <td>-0.926193</td>\n",
       "      <td>-0.897601</td>\n",
       "      <td>-1.643501</td>\n",
       "      <td>-0.375450</td>\n",
       "      <td>0.622636</td>\n",
       "      <td>-0.478409</td>\n",
       "      <td>0.758789</td>\n",
       "      <td>-1.016975</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.372135</td>\n",
       "      <td>1.868488</td>\n",
       "      <td>-0.549899</td>\n",
       "      <td>-0.652146</td>\n",
       "      <td>2.410029</td>\n",
       "      <td>0.030487</td>\n",
       "      <td>-1.189134</td>\n",
       "      <td>-0.120523</td>\n",
       "      <td>0.468067</td>\n",
       "      <td>-0.343264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463712</th>\n",
       "      <td>0.162997</td>\n",
       "      <td>0.006509</td>\n",
       "      <td>0.836644</td>\n",
       "      <td>0.067344</td>\n",
       "      <td>-0.367053</td>\n",
       "      <td>0.273406</td>\n",
       "      <td>-2.079877</td>\n",
       "      <td>-0.029351</td>\n",
       "      <td>-0.221383</td>\n",
       "      <td>-0.722086</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.103120</td>\n",
       "      <td>0.440416</td>\n",
       "      <td>-0.406601</td>\n",
       "      <td>-0.822279</td>\n",
       "      <td>0.864693</td>\n",
       "      <td>-0.687515</td>\n",
       "      <td>-0.265901</td>\n",
       "      <td>-0.663702</td>\n",
       "      <td>0.012903</td>\n",
       "      <td>-0.246192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463713</th>\n",
       "      <td>0.247234</td>\n",
       "      <td>0.249282</td>\n",
       "      <td>-0.407312</td>\n",
       "      <td>-1.278516</td>\n",
       "      <td>-2.290479</td>\n",
       "      <td>-0.373539</td>\n",
       "      <td>-0.105010</td>\n",
       "      <td>0.164208</td>\n",
       "      <td>-0.146179</td>\n",
       "      <td>0.030704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367180</td>\n",
       "      <td>1.480824</td>\n",
       "      <td>0.126993</td>\n",
       "      <td>-0.701360</td>\n",
       "      <td>1.256892</td>\n",
       "      <td>0.637414</td>\n",
       "      <td>0.344071</td>\n",
       "      <td>0.532877</td>\n",
       "      <td>0.466592</td>\n",
       "      <td>0.700802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463714</th>\n",
       "      <td>1.142946</td>\n",
       "      <td>0.105125</td>\n",
       "      <td>1.287201</td>\n",
       "      <td>-0.534488</td>\n",
       "      <td>-1.429635</td>\n",
       "      <td>-0.111189</td>\n",
       "      <td>-2.616308</td>\n",
       "      <td>-0.231192</td>\n",
       "      <td>0.269906</td>\n",
       "      <td>-0.135636</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.249371</td>\n",
       "      <td>0.428395</td>\n",
       "      <td>-0.235258</td>\n",
       "      <td>-0.209710</td>\n",
       "      <td>-0.111101</td>\n",
       "      <td>-0.330209</td>\n",
       "      <td>0.099508</td>\n",
       "      <td>-0.051046</td>\n",
       "      <td>-0.339430</td>\n",
       "      <td>-0.057782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>463715 rows Ã— 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6   \\\n",
       "0       1.080575  0.391265  1.826532  0.464657 -0.474730 -0.278204 -1.552371   \n",
       "1       0.880919  0.332292  1.748539  0.721828 -0.164945 -1.191173  0.765681   \n",
       "2       1.247622  0.592600  1.337173  0.750657 -0.001110 -0.702100 -0.060914   \n",
       "3       0.801044 -0.061805  0.783683  0.087218  0.329180 -1.298429  0.510714   \n",
       "4       1.249775  0.793334  1.657037  0.447460 -0.406775 -0.567138 -0.692498   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "463710  0.493427 -0.336316 -0.084681 -0.658594 -1.673199  0.282011 -1.493945   \n",
       "463711 -0.124812  0.200480 -0.926193 -0.897601 -1.643501 -0.375450  0.622636   \n",
       "463712  0.162997  0.006509  0.836644  0.067344 -0.367053  0.273406 -2.079877   \n",
       "463713  0.247234  0.249282 -0.407312 -1.278516 -2.290479 -0.373539 -0.105010   \n",
       "463714  1.142946  0.105125  1.287201 -0.534488 -1.429635 -0.111189 -2.616308   \n",
       "\n",
       "              7         8         9   ...        80        81        82  \\\n",
       "0      -1.310845  0.387704 -0.666166  ... -0.085335  0.108508  0.142775   \n",
       "1       0.109626  1.420941  0.414950  ... -0.314250  0.306236 -0.069483   \n",
       "2      -0.069956  1.166254 -0.074608  ... -0.396186  0.566683 -0.756534   \n",
       "3      -1.073355 -0.016803 -1.262655  ...  0.586237 -0.559427 -0.478689   \n",
       "4      -0.952197  0.841844 -0.144910  ... -0.181585  0.099672  0.191319   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "463710 -0.686837  0.831008 -1.058139  ...  0.124247  0.991760 -0.324729   \n",
       "463711 -0.478409  0.758789 -1.016975  ... -0.372135  1.868488 -0.549899   \n",
       "463712 -0.029351 -0.221383 -0.722086  ... -0.103120  0.440416 -0.406601   \n",
       "463713  0.164208 -0.146179  0.030704  ...  0.367180  1.480824  0.126993   \n",
       "463714 -0.231192  0.269906 -0.135636  ... -0.249371  0.428395 -0.235258   \n",
       "\n",
       "              83        84        85        86        87        88        89  \n",
       "0      -0.237355  0.049233 -0.356182  0.544458 -0.470599 -0.255977  0.042292  \n",
       "1       0.052017 -0.632328 -0.436057  0.556448  0.568744  0.206940  1.158587  \n",
       "2      -0.284019 -0.024220  0.223128 -0.509789 -0.338457  0.105819 -0.090208  \n",
       "3      -0.890161 -0.793906  0.567269 -0.263107  0.408116  0.967862  0.793384  \n",
       "4      -0.585576 -0.111877 -0.219960  0.448804  0.256882  0.192038  1.241363  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "463710 -0.395601  1.531331  0.085218 -0.001498 -0.322949 -0.616705 -0.697774  \n",
       "463711 -0.652146  2.410029  0.030487 -1.189134 -0.120523  0.468067 -0.343264  \n",
       "463712 -0.822279  0.864693 -0.687515 -0.265901 -0.663702  0.012903 -0.246192  \n",
       "463713 -0.701360  1.256892  0.637414  0.344071  0.532877  0.466592  0.700802  \n",
       "463714 -0.209710 -0.111101 -0.330209  0.099508 -0.051046 -0.339430 -0.057782  \n",
       "\n",
       "[463715 rows x 90 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge,RidgeCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "\n",
    "df=pd.read_csv('/root/autodl-tmp/YearPredictionMSD.txt',header=None,sep = ',')\n",
    "#smaller dataset\n",
    "tt=463715\n",
    "ee=51630\n",
    "train=df.iloc[:tt] \n",
    "test=df.iloc[tt:] \n",
    "\n",
    "train_y = train[0]\n",
    "test_y = test[0] #response:year\n",
    "train_x = train\n",
    "test_x = test\n",
    "del train_x[0]\n",
    "del test_x[0]\n",
    "total_x=pd.concat([train_x,test_x])\n",
    "x=preprocessing.StandardScaler().fit(total_x).transform(total_x) #normalize\n",
    "x=pd.DataFrame(x)\n",
    "train_x=x.iloc[:tt]\n",
    "test_x=x.iloc[tt:]\n",
    "\n",
    "train_x.reset_index(drop=True, inplace=True) \n",
    "test_x.reset_index(drop=True, inplace=True) \n",
    "train_y.reset_index(drop=True, inplace=True) \n",
    "test_y.reset_index(drop=True, inplace=True) \n",
    "\n",
    "train_x ##display the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nntrain_x = torch.from_numpy(train_x.to_numpy()).float()\n",
    "nntrain_y = torch.squeeze(torch.from_numpy(train_y.to_numpy()).float()) \n",
    "nntest_x= torch.from_numpy(test_x.to_numpy()).float()\n",
    "nntest_y = torch.squeeze(torch.from_numpy(test_y.to_numpy()).float())\n",
    "\n",
    "class mydataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "        self._len = len(x)\n",
    "\n",
    "    def __getitem__(self, item): \n",
    "        return self._x[item], self._y[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=90, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "cuda\n",
      "epoch 0\n",
      "            Train set - loss: 769754.125\n",
      "            Test  set - loss: 751078.6875\n",
      "            \n",
      "epoch 2\n",
      "            Train set - loss: 21000.783203125\n",
      "            Test  set - loss: 19093.583984375\n",
      "            \n",
      "epoch 4\n",
      "            Train set - loss: 1068.667724609375\n",
      "            Test  set - loss: 898.3678588867188\n",
      "            \n",
      "epoch 6\n",
      "            Train set - loss: 289.8102722167969\n",
      "            Test  set - loss: 399.1617126464844\n",
      "            \n",
      "epoch 8\n",
      "            Train set - loss: 226.97195434570312\n",
      "            Test  set - loss: 378.2677917480469\n",
      "            \n",
      "epoch 10\n",
      "            Train set - loss: 224.43519592285156\n",
      "            Test  set - loss: 376.4065246582031\n",
      "            \n",
      "epoch 12\n",
      "            Train set - loss: 220.74098205566406\n",
      "            Test  set - loss: 376.1688232421875\n",
      "            \n",
      "epoch 14\n",
      "            Train set - loss: 226.09800720214844\n",
      "            Test  set - loss: 376.1139221191406\n",
      "            \n",
      "epoch 16\n",
      "            Train set - loss: 225.18763732910156\n",
      "            Test  set - loss: 376.10247802734375\n",
      "            \n",
      "epoch 18\n",
      "            Train set - loss: 204.67227172851562\n",
      "            Test  set - loss: 376.10882568359375\n",
      "            \n",
      "DNN complexity and model fitted in 90.400 s\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(mydataset(nntrain_x, nntrain_y),batch_size=1024,pin_memory=True,num_workers=16, shuffle=True)\n",
    "test_loader = DataLoader(mydataset(nntest_x, nntest_y),batch_size=1024, pin_memory=True,num_workers=16,shuffle=False)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1 = nn.Linear(90, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "\n",
    "#initialize\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.normal_(m.weight,mean=0,std=0.5)\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.uniform_(m.weight,a=0,b=0.1)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "torch.manual_seed(0)\n",
    "net.apply(init_weights)\n",
    "print(net)\n",
    "print(device)\n",
    "criterion=nn.MSELoss() \n",
    "optimizer=optim.SGD(net.parameters(),lr=1e-4,momentum=0.9,weight_decay=1e-2) #optim.Adam(...)\n",
    "dnn_trainloss=[]\n",
    "dnn_testloss=[]\n",
    "t0=time.time()\n",
    "#tb=0\n",
    "#tc=0\n",
    "\n",
    "for epoch in range(20): \n",
    "    for x, y in train_loader: #for batch, (x, y) in enumerate(train_loader): \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        #print(x.shape,y.shape,x.device, y.device,net.fc1.weight.device)\n",
    "        #td=time.time()-tc \n",
    "        #ta=time.time()\n",
    "        # Compute prediction error\n",
    "        y_pred = net(x)\n",
    "        y_pred = torch.squeeze(y_pred)\n",
    "        train_loss = criterion(y_pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(tb/(tb+td))\n",
    "        #tb=time.time()-ta\n",
    "        #tc=time.time()\n",
    "\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_test_pred = net(x)\n",
    "        y_test_pred = torch.squeeze(y_test_pred)\n",
    "    \n",
    "        test_loss = criterion(y_test_pred,y)\n",
    "    \n",
    "    if epoch>5 and float(test_loss)>max(dnn_testloss[-5:-1]):\n",
    "        break\n",
    "    \n",
    "    if epoch % 2 == 0:        \n",
    "        print(f'''epoch {epoch}\n",
    "            Train set - loss: {train_loss}\n",
    "            Test  set - loss: {test_loss}\n",
    "            ''')\n",
    "    \n",
    "    dnn_trainloss.append(float(train_loss))\n",
    "    dnn_testloss.append(float(test_loss))\n",
    "            \n",
    "dnn_fit = time.time() - t0\n",
    "print(\"DNN complexity and model fitted in %.3f s\" % dnn_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f951828cfa0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(dnn_trainloss[5:])\n",
    "plt.plot(dnn_testloss[5:])\n",
    "plt.legend([\"train\",\"test\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (rblock1): ResidualBlock(\n",
      "    (fc1): Linear(in_features=90, out_features=256, bias=True)\n",
      "    (fc2): Linear(in_features=90, out_features=256, bias=True)\n",
      "  )\n",
      "  (rblock2): ResidualBlock(\n",
      "    (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (rblock3): ResidualBlock(\n",
      "    (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  )\n",
      "  (fc4): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "            Train set - loss: 636574.6875\n",
      "            Test  set - loss: 619884.375\n",
      "            \n",
      "epoch 2\n",
      "            Train set - loss: 17568.75390625\n",
      "            Test  set - loss: 15857.2802734375\n",
      "            \n",
      "epoch 4\n",
      "            Train set - loss: 953.4149169921875\n",
      "            Test  set - loss: 813.4408569335938\n",
      "            \n",
      "epoch 6\n",
      "            Train set - loss: 281.87847900390625\n",
      "            Test  set - loss: 396.14984130859375\n",
      "            \n",
      "epoch 8\n",
      "            Train set - loss: 225.96533203125\n",
      "            Test  set - loss: 378.0484313964844\n",
      "            \n",
      "epoch 10\n",
      "            Train set - loss: 224.26026916503906\n",
      "            Test  set - loss: 376.375244140625\n",
      "            \n",
      "epoch 12\n",
      "            Train set - loss: 220.71034240722656\n",
      "            Test  set - loss: 376.1632995605469\n",
      "            \n",
      "epoch 14\n",
      "            Train set - loss: 226.09547424316406\n",
      "            Test  set - loss: 376.1134033203125\n",
      "            \n",
      "epoch 16\n",
      "            Train set - loss: 225.18533325195312\n",
      "            Test  set - loss: 376.1020812988281\n",
      "            \n",
      "epoch 18\n",
      "            Train set - loss: 204.67227172851562\n",
      "            Test  set - loss: 376.10882568359375\n",
      "            \n",
      "ResNet complexity and model fitted in 150.169 s\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(mydataset(nntrain_x, nntrain_y),batch_size=1024,pin_memory=True,num_workers=16, shuffle=True)\n",
    "test_loader = DataLoader(mydataset(nntest_x, nntest_y),batch_size=1024, pin_memory=True,num_workers=16,shuffle=False)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,infeatures,outfeatures):\n",
    "        super(ResidualBlock,self).__init__()\n",
    "        self.infeatures = infeatures\n",
    "        self.outfeatures = outfeatures\n",
    "        self.fc1 = nn.Linear(infeatures,outfeatures)\n",
    "        self.fc2 = nn.Linear(infeatures,outfeatures)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.fc1(x)\n",
    "        y= F.relu(y)\n",
    "        x = self.fc2(x)\n",
    "        return F.relu(x+y)\n",
    "\n",
    "\n",
    "class ResNet(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.rblock1 = ResidualBlock(90,256)\n",
    "        self.rblock2 = ResidualBlock(256,128)\n",
    "        self.rblock3 = ResidualBlock(128,64)\n",
    "        self.fc4 = nn.Linear(64,1)\n",
    "    \n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.rblock1(x)\n",
    "        x = self.rblock2(x)\n",
    "        x = self.rblock3(x)\n",
    "        return self.fc4(x)\n",
    "#initialize\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.normal_(m.weight,mean=0,std=0.5)\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.uniform_(m.weight,a=0,b=0.1)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = ResNet()\n",
    "net = net.to(device)\n",
    "torch.manual_seed(0)\n",
    "net.apply(init_weights)\n",
    "print(net)\n",
    "criterion=nn.MSELoss() \n",
    "optimizer=optim.SGD(net.parameters(),lr=1e-4,momentum=0.9,weight_decay=1e-2) #optim.Adam(...)\n",
    "res_trainloss=[]\n",
    "res_testloss=[]\n",
    "t0=time.time()\n",
    "for epoch in range(20): \n",
    "    for x, y in train_loader: #for batch, (x, y) in enumerate(train_loader): \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        y_pred = net(x)\n",
    "        y_pred = torch.squeeze(y_pred)\n",
    "        train_loss = criterion(y_pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_test_pred = net(x)\n",
    "        y_test_pred = torch.squeeze(y_test_pred)\n",
    "    \n",
    "        test_loss = criterion(y_test_pred,y)\n",
    "    \n",
    "    if epoch>5 and float(test_loss)>max(res_testloss[-5:-1]):\n",
    "        break\n",
    "    \n",
    "    if epoch % 2 == 0:        \n",
    "        print(f'''epoch {epoch}\n",
    "            Train set - loss: {train_loss}\n",
    "            Test  set - loss: {test_loss}\n",
    "            ''')\n",
    "    res_trainloss.append(float(train_loss))\n",
    "    res_testloss.append(float(test_loss))\n",
    "            \n",
    "dnn_fit = time.time() - t0\n",
    "print(\"ResNet complexity and model fitted in %.3f s\" % dnn_fit)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_1d(pdf, gamma, device):\n",
    "    if pdf == 'G':\n",
    "        w = torch.randn(1, device=device) * gamma\n",
    "        return w\n",
    "    elif pdf == 'L':\n",
    "        w = torch.distributions.laplace.Laplace(torch.tensor([0.0], device=device), torch.tensor([1.0], device=device)).sample() * gamma\n",
    "        return w\n",
    "    elif pdf == 'C':\n",
    "        w = torch.distributions.cauchy.Cauchy(torch.tensor([0.0], device=device), torch.tensor([1.0], device=device)).sample() * gamma\n",
    "        return w\n",
    "    \n",
    "def sample(pdf, gamma, d, device):\n",
    "    return torch.tensor([sample_1d(pdf, gamma, device) for _ in range(d)], device=device)\n",
    "\n",
    "class RandomFourierFeature:\n",
    "    \"\"\"Random Fourier Feature\n",
    "    Parameters\n",
    "    ----------\n",
    "    d : int\n",
    "        Input space dimension\n",
    "    D : int\n",
    "        Feature space dimension\n",
    "    W : shape (D,d)\n",
    "    b : shape (D)\n",
    "    kernel : char\n",
    "        Kernel to use; 'G', 'L', or 'C'\n",
    "    gamma : float\n",
    "        pdf parameter\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d, D, W=None, b=None, kernel='G', gamma=1, device='cuda:0'):\n",
    "        self.d = d\n",
    "        self.D = D\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "\n",
    "        kernel = kernel.upper()\n",
    "        if kernel not in ['G', 'L', 'C']:\n",
    "            raise Exception('Invalid Kernel')\n",
    "        self.kernel = kernel\n",
    "\n",
    "        if W is None or b is None:\n",
    "            self.create()\n",
    "        else:\n",
    "            self.__load(W, b)\n",
    "\n",
    "    def __load(self, W, b):\n",
    "        \"\"\"Load from existing Arrays\"\"\"\n",
    "\n",
    "        self.W = W.reshape([self.D, self.d])\n",
    "        self.b = b\n",
    "    \n",
    "    \n",
    "    def create(self):\n",
    "        \"\"\"Create a d->D fourier random feature\"\"\"\n",
    "        self.b = torch.rand(self.D, device=self.device) * 2 * torch.pi\n",
    "        self.W = sample(self.kernel, self.gamma, self.d * self.D, self.device).reshape(self.D, self.d)\n",
    "\n",
    "    def transform(self, x):\n",
    "        \"\"\"Transform a vector using this feature\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (shape=(n,d))\n",
    "            to transform; must be single dimension vector\n",
    "        Returns\n",
    "        -------\n",
    "        x : (shape=(n,D))\n",
    "            Feature space transformation of x\n",
    "        \"\"\"\n",
    "        #print(self.W.shape,self.b.reshape(-1,1).shape,x.shape)\n",
    "        #print((self.W @ x.T).shape)\n",
    "       \n",
    "\n",
    "        result = torch.sqrt(torch.tensor([2.0 / self.D], device=x.device)) * torch.cos(\n",
    "            self.W @ x.T + (self.b.reshape(-1, 1) @ torch.ones((1,len(x)), device=x.device))\n",
    "        )\n",
    "        return result.T\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KernelNet(\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0\n",
      "            Train set - loss: 16284.0205078125\n",
      "            Test  set - loss: 14118.5537109375\n",
      "            \n",
      "epoch 2\n",
      "            Train set - loss: 258.36846923828125\n",
      "            Test  set - loss: 390.3016662597656\n",
      "            \n",
      "epoch 4\n",
      "            Train set - loss: 253.39892578125\n",
      "            Test  set - loss: 387.63629150390625\n",
      "            \n",
      "epoch 6\n",
      "            Train set - loss: 242.29151916503906\n",
      "            Test  set - loss: 399.5004577636719\n",
      "            \n",
      "epoch 8\n",
      "            Train set - loss: 224.7009735107422\n",
      "            Test  set - loss: 392.871826171875\n",
      "            \n",
      "epoch 10\n",
      "            Train set - loss: 231.08924865722656\n",
      "            Test  set - loss: 388.36395263671875\n",
      "            \n",
      "epoch 12\n",
      "            Train set - loss: 222.09104919433594\n",
      "            Test  set - loss: 374.72222900390625\n",
      "            \n",
      "KernelNet complexity and model fitted in 80.622 s\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(mydataset(nntrain_x, nntrain_y),batch_size=1024,pin_memory=True,num_workers=16, shuffle=True)\n",
    "test_loader = DataLoader(mydataset(nntest_x, nntest_y),batch_size=1024, pin_memory=True,num_workers=16,shuffle=False)\n",
    "\n",
    "rff0=RandomFourierFeature(90,256,kernel='C',gamma=0.1)\n",
    "rff1=RandomFourierFeature(128,128,kernel='G',gamma=0.2)\n",
    "rff2=RandomFourierFeature(64,64,kernel='G',gamma=0.4)\n",
    "\n",
    "class KernelNet(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(KernelNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = rff0.transform(x)\n",
    "        x=self.fc1(x)\n",
    "        x = rff1.transform(x)\n",
    "        x=self.fc2(x)\n",
    "        x = rff2.transform(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "#initialize\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.normal_(m.weight,mean=0,std=0.5)\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.uniform_(m.weight,a=0,b=0.1)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = KernelNet()\n",
    "net = net.to(device)\n",
    "torch.manual_seed(1)\n",
    "net.apply(init_weights)\n",
    "print(net)\n",
    "criterion=nn.MSELoss() \n",
    "optimizer=optim.SGD(net.parameters(),lr=3e-4,momentum=0.9,weight_decay=1e-2) #optim.Adam(...)\n",
    "\n",
    "loss=[]\n",
    "kernelnn_trainloss=[]\n",
    "kernelnn_testloss=[]\n",
    "t0 = time.time()\n",
    "for epoch in range(20): \n",
    "    for x, y in train_loader: #for batch, (x, y) in enumerate(train_loader): \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        y_pred = net(x)\n",
    "        y_pred = torch.squeeze(y_pred)\n",
    "        train_loss = criterion(y_pred, y)\n",
    "        loss.append(train_loss)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_test_pred = net(x)\n",
    "        y_test_pred = torch.squeeze(y_test_pred)\n",
    "\n",
    "        test_loss = criterion(y_test_pred,y)\n",
    "            \n",
    "    if epoch>5 and float(test_loss)>max(kernelnn_testloss[-5:-1]):\n",
    "        break\n",
    "    \n",
    "    \n",
    "    if epoch % 2 == 0:         \n",
    "        print(f'''epoch {epoch}\n",
    "            Train set - loss: {train_loss}\n",
    "            Test  set - loss: {test_loss}\n",
    "            ''')\n",
    "   \n",
    "    kernelnn_trainloss.append(float(train_loss))\n",
    "    kernelnn_testloss.append(float(test_loss))\n",
    "        \n",
    "    \n",
    "dnn_fit = time.time() - t0\n",
    "print(\"KernelNet complexity and model fitted in %.3f s\" % dnn_fit)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResKernelNet(\n",
      "  (rblock1): ResidualBlock(\n",
      "    (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (rblock2): ResidualBlock(\n",
      "    (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0\n",
      "            Train set - loss: 118.857666015625\n",
      "            Test  set - loss: 438.92486572265625\n",
      "            \n",
      "epoch 2\n",
      "            Train set - loss: 108.98763275146484\n",
      "            Test  set - loss: 417.9961242675781\n",
      "            \n",
      "epoch 4\n",
      "            Train set - loss: 104.75946044921875\n",
      "            Test  set - loss: 405.84844970703125\n",
      "            \n",
      "epoch 6\n",
      "            Train set - loss: 110.37870788574219\n",
      "            Test  set - loss: 367.8404846191406\n",
      "            \n",
      "epoch 8\n",
      "            Train set - loss: 90.0863037109375\n",
      "            Test  set - loss: 353.6864929199219\n",
      "            \n",
      "epoch 10\n",
      "            Train set - loss: 95.75540161132812\n",
      "            Test  set - loss: 348.0726013183594\n",
      "            \n",
      "epoch 12\n",
      "            Train set - loss: 101.92547607421875\n",
      "            Test  set - loss: 336.3481750488281\n",
      "            \n",
      "epoch 14\n",
      "            Train set - loss: 91.36256408691406\n",
      "            Test  set - loss: 337.0558166503906\n",
      "            \n",
      "epoch 16\n",
      "            Train set - loss: 99.40116119384766\n",
      "            Test  set - loss: 338.2347412109375\n",
      "            \n",
      "Residual KernelNet complexity and model fitted in 97.510 s\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(mydataset(nntrain_x, nntrain_y),batch_size=1024,pin_memory=True,num_workers=16, shuffle=True)\n",
    "test_loader = DataLoader(mydataset(nntest_x, nntest_y),batch_size=1024, pin_memory=True,num_workers=16,shuffle=False)\n",
    "\n",
    "rff0=RandomFourierFeature(90,256,kernel='C',gamma=0.02)\n",
    "rff1=RandomFourierFeature(128,128,kernel='G',gamma=0.1)\n",
    "rff2=RandomFourierFeature(64,64,kernel='G',gamma=0.5)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,infeatures,outfeatures,rff):\n",
    "        super(ResidualBlock,self).__init__()\n",
    "        self.infeatures = infeatures\n",
    "        self.outfeatures = outfeatures\n",
    "        self.rff=rff\n",
    "        \n",
    "        self.fc1 = nn.Linear(infeatures,outfeatures)\n",
    "        self.fc2 = nn.Linear(outfeatures,outfeatures)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        rff=self.rff\n",
    "        x = self.fc1(x)\n",
    "        y = rff.transform(x)\n",
    "        y = self.fc2(y)\n",
    "        return x+y\n",
    "\n",
    "class ResKernelNet(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(ResKernelNet, self).__init__()\n",
    "        self.rblock1 = ResidualBlock(256,128,rff1)\n",
    "        self.rblock2 = ResidualBlock(128,64,rff2)\n",
    "        self.fc3 =nn.Linear(64,1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = rff0.transform(x)\n",
    "        x = self.rblock1(x)\n",
    "        x = self.rblock2(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "#initialize\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.normal_(m.weight,mean=0,std=0.5)\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.uniform_(m.weight,a=0,b=1)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = ResKernelNet()\n",
    "net = net.to(device)\n",
    "torch.manual_seed(1)\n",
    "#net.apply(init_weights)\n",
    "print(net)\n",
    "criterion=nn.MSELoss() \n",
    "optimizer=optim.SGD(net.parameters(),lr=1e-6,momentum=0.9,weight_decay=1e-2) #optim.Adam(...)\n",
    "\n",
    "loss=[]\n",
    "reskernel_trainloss=[]\n",
    "reskernel_testloss=[]\n",
    "t0 = time.time()\n",
    "for epoch in range(20): \n",
    "    for x, y in train_loader: #for batch, (x, y) in enumerate(train_loader): \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        y_pred = net(x)\n",
    "        y_pred = torch.squeeze(y_pred)\n",
    "        train_loss = criterion(y_pred, y)\n",
    "        loss.append(train_loss)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_test_pred = net(x)\n",
    "        y_test_pred = torch.squeeze(y_test_pred)\n",
    "\n",
    "        test_loss = criterion(y_test_pred,y)\n",
    "            \n",
    "    if epoch>5 and float(test_loss)>max(reskernel_testloss[-5:-1]):\n",
    "        break\n",
    "    \n",
    "    \n",
    "    if epoch % 2 == 0:         \n",
    "        print(f'''epoch {epoch}\n",
    "            Train set - loss: {train_loss}\n",
    "            Test  set - loss: {test_loss}\n",
    "            ''')\n",
    "    reskernel_trainloss.append(float(train_loss))\n",
    "    reskernel_testloss.append(float(test_loss))\n",
    "        \n",
    "    \n",
    "dnn_fit = time.time() - t0\n",
    "print(\"Residual KernelNet complexity and model fitted in %.3f s\" % dnn_fit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
