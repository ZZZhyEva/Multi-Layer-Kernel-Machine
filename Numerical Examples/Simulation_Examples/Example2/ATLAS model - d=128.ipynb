{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation 4 - ATLAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_1d(pdf, gamma):\n",
    "    if pdf=='G':\n",
    "        w=torch.randn(1)*gamma\n",
    "        return w\n",
    "    elif pdf=='L':\n",
    "        w=torch.distributions.laplace.Laplace(torch.tensor([0.0]), torch.tensor([1.0])).sample()*gamma\n",
    "        return w\n",
    "    elif pdf=='C':\n",
    "        w=torch.distributions.cauchy.Cauchy(torch.tensor([0.0]), torch.tensor([1.0])).sample()*gamma\n",
    "        return w\n",
    "    \n",
    "def sample(pdf, gamma, d):\n",
    "    return torch.tensor([sample_1d(pdf, gamma) for _ in range(d)])\n",
    "\n",
    "class RandomFourierFeature:\n",
    "    \"\"\"Random Fourier Feature\n",
    "    Parameters\n",
    "    ----------\n",
    "    d : int\n",
    "        Input space dimension\n",
    "    D : int\n",
    "        Feature space dimension\n",
    "    W : shape (D,d)\n",
    "    b : shape (D)\n",
    "    kernel : char\n",
    "        Kernel to use; 'G', 'L', or 'C'\n",
    "    gamma : float\n",
    "        pdf parameter\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d, D, W=None, b=None, kernel='G', gamma=1):\n",
    "\n",
    "        self.d = d\n",
    "        self.D = D\n",
    "        self.gamma = gamma\n",
    "\n",
    "        kernel = kernel.upper()\n",
    "        if kernel not in ['G', 'L', 'C']:\n",
    "            raise Exception('Invalid Kernel')\n",
    "        self.kernel = kernel\n",
    "\n",
    "        if W is None or b is None:\n",
    "            self.create()\n",
    "        else:\n",
    "            self.__load(W, b)\n",
    "\n",
    "    def __load(self, W, b):\n",
    "        \"\"\"Load from existing Arrays\"\"\"\n",
    "\n",
    "        self.W = W.reshape([self.D, self.d])\n",
    "        self.b = b\n",
    "    \n",
    "\n",
    "    def create(self):\n",
    "        \"\"\"Create a d->D fourier random feature\"\"\"\n",
    "\n",
    "        self.b = torch.rand(self.D)*2*torch.pi\n",
    "        self.W = sample(self.kernel, self.gamma, self.d*self.D).reshape(self.D,self.d)\n",
    "\n",
    "    def transform(self, x):\n",
    "        \"\"\"Transform a vector using this feature\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (shape=(n,d))\n",
    "            to transform; must be single dimension vector\n",
    "        Returns\n",
    "        -------\n",
    "        x : (shape=(n,D))\n",
    "            Feature space transformation of x\n",
    "        \"\"\"\n",
    "       \n",
    "        result=torch.sqrt(torch.tensor([2.0/self.D])) * torch.cos( self.W @ x.T  + (self.b.reshape(-1,1) @ torch.ones(len(x)).reshape(1,-1))) \n",
    "        return result.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=2000\n",
    "p=128\n",
    "np.random.seed(0)\n",
    "data=np.random.uniform(0,1, (n,p)) #n points\n",
    "np.random.seed(1)\n",
    "noise=np.random.normal(0,1,n)\n",
    "\n",
    "#function\n",
    "def g1(x):\n",
    "    return -2*np.sin(2*np.pi*x)\n",
    "def g2(x):\n",
    "    return x**2-1/3\n",
    "def g3(x):\n",
    "    return x-1/2\n",
    "def g4(x):\n",
    "    return np.exp(x)+np.exp(1)-1\n",
    "a1=1\n",
    "a4=4\n",
    "def a2(x):\n",
    "    return 2/np.sqrt(2*np.pi)*np.exp(-(x-1)**2/2)\n",
    "def a3(x):\n",
    "    return 3*np.cos(2*np.pi*x)\n",
    "\n",
    "y=a1*g1(data[:,0])+a2(data[:,0])*g2(data[:,1])+a3(data[:,0])*g3(data[:,2])+a4*g4(data[:,3])+noise\n",
    "\n",
    "\n",
    "np.random.seed(2)\n",
    "calibration_x=np.random.uniform(0,1, (n,p)) #n points\n",
    "np.random.seed(3)\n",
    "calibration_noise=np.random.normal(0,1,n)\n",
    "calibration_y=a1*g1(calibration_x[:,0])+a2(calibration_x[:,0])*g2(calibration_x[:,1])+a3(calibration_x[:,0])*g3(calibration_x[:,2])+a4*g4(calibration_x[:,3])+calibration_noise\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(4)\n",
    "test_x=np.random.uniform(0,1, (2*n,p)) #n points\n",
    "np.random.seed(5)\n",
    "test_noise=np.random.normal(0,1,2*n)\n",
    "test_y=a1*g1(test_x[:,0])+a2(test_x[:,0])*g2(test_x[:,1])+a3(test_x[:,0])*g3(test_x[:,2])+a4*g4(test_x[:,3])+test_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=data\n",
    "train_y=y\n",
    "\n",
    "total_x=np.vstack((train_x,calibration_x))\n",
    "total_y=np.hstack((train_y,calibration_y))\n",
    "\n",
    "nntrain_x = torch.from_numpy(train_x).float()\n",
    "nntrain_y = torch.squeeze(torch.from_numpy(train_y).float()) \n",
    "nntest_x= torch.from_numpy(test_x).float()\n",
    "nntest_y = torch.squeeze(torch.from_numpy(test_y).float())\n",
    "\n",
    "class mydataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "        self._len = len(x)\n",
    "\n",
    "    def __getitem__(self, item): \n",
    "        return self._x[item], self._y[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'alpha': 0.0001}\n",
      "148.89884748097947\n",
      "Time: 7.1270911693573\n",
      "1.48317044913515e-06\n"
     ]
    }
   ],
   "source": [
    "### over-fitting in high-dim\n",
    "t0=time.time()\n",
    "krr = KernelRidge(kernel='rbf',gamma=1/2)\n",
    "param_grid = {\n",
    "    'alpha': [1e-4,1e-3]\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)  \n",
    "grid_search = GridSearchCV(krr, param_grid, cv=kf)\n",
    "grid_search.fit(total_x, total_y)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "best_krr = grid_search.best_estimator_\n",
    "krr_pred = best_krr.predict(test_x)\n",
    "\n",
    "t1=time.time()-t0\n",
    "print(mean_squared_error(test_y,krr_pred))\n",
    "print(\"Time:\",t1)\n",
    "\n",
    "krr_pred = best_krr.predict(total_x)\n",
    "print(mean_squared_error(total_y,krr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'alpha': 1e-07}\n",
      "148.8951327735091\n",
      "Time: 12.95745301246643\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "krr = KernelRidge(kernel='rbf',gamma=1/2)\n",
    "param_grid = {\n",
    "    'alpha': [1e-7,1e-6,1e-5,1e-4,1e-3]\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)  \n",
    "grid_search = GridSearchCV(krr, param_grid, cv=kf)\n",
    "grid_search.fit(total_x, total_y)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "best_krr = grid_search.best_estimator_\n",
    "krr_pred = best_krr.predict(test_x)\n",
    "\n",
    "t1=time.time()-t0\n",
    "print(mean_squared_error(test_y,krr_pred))\n",
    "print(\"Time:\",t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'alpha': 1}\n",
      "8.164069507828035\n",
      "Time: 1.831078290939331\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "model=Ridge()\n",
    "param_grid = {\n",
    "    'alpha': [1e-4,1e-3,1e-2,1e-1,1]\n",
    "}\n",
    "rff=RandomFourierFeature(p,500,kernel='G',gamma=1)\n",
    "total_feature=rff.transform(total_x)\n",
    "test_feature=rff.transform(test_x)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)  \n",
    "grid_search = GridSearchCV(model, param_grid, cv=kf)\n",
    "grid_search.fit(total_feature, total_y)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "rf_pred = best_model.predict(test_feature)\n",
    "\n",
    "t1=time.time()-t0\n",
    "print(mean_squared_error(test_y,rf_pred))\n",
    "print(\"Time:\",t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLKM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [19:43<00:00, 236.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.589434, 9.70761, 8.321691, 8.347389, 11.105855]\n",
      "0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rff1=RandomFourierFeature(p,256,kernel='G',gamma=1)\n",
    "rff2=RandomFourierFeature(16,16,kernel='G',gamma=1)\n",
    "\n",
    "class KernelNet(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(KernelNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(256,16)\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = rff1.transform(x)\n",
    "        x=self.fc1(x)\n",
    "        x = rff2.transform(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "#initialize\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.normal_(m.weight,mean=0,std=0.5)\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.uniform_(m.weight,a=-0.1,b=0.1)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "\n",
    "def MultiLayerKfold(kfold,param_grid):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    param_error=[]\n",
    "    for param in tqdm(param_grid):\n",
    "        k_error=[]\n",
    "        for k in range(kfold):\n",
    "            Kfold_val_x, Kfold_val_y=nntrain_x[int(n/5)*k:int(n/5)*(k+1)], nntrain_y[int(n/5)*k:int(n/5)*(k+1)]\n",
    "            Kfold_train_x = torch.cat((nntrain_x[:int(n/5)*k],nntrain_x[int(n/5)*(k+1):]),dim = 0)\n",
    "            Kfold_train_y = torch.cat((nntrain_y[:int(n/5)*k],nntrain_y[int(n/5)*(k+1):]),dim = 0)\n",
    "           \n",
    "            net = KernelNet()  #### KernelNet!\n",
    "            net = net.to(device)\n",
    "            torch.manual_seed(1)\n",
    "            net.apply(init_weights)\n",
    "            criterion=nn.MSELoss() \n",
    "            optimizer=optim.SGD(net.parameters(),lr=1e-3,momentum=0.9,weight_decay=param) #optim.Adam(...)\n",
    "            kfoldtrain_loader = DataLoader(mydataset(Kfold_train_x, Kfold_train_y),batch_size=128, shuffle=True)\n",
    "            \n",
    "            for epoch in range(2000): \n",
    "                for x, y in kfoldtrain_loader: #for batch, (x, y) in enumerate(train_loader): \n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    # Compute prediction error\n",
    "                    y_pred = net(x)\n",
    "                    y_pred = torch.squeeze(y_pred)\n",
    "                    train_loss = criterion(y_pred, y)\n",
    "                    # Backpropagation\n",
    "                    optimizer.zero_grad() \n",
    "                    train_loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            x0=Kfold_val_x[:].float()\n",
    "            with torch.no_grad():\n",
    "                x0 = x0.to(device)\n",
    "                val_pred = net(x0)\n",
    "            \n",
    "            k_error.append(mean_squared_error(val_pred,Kfold_val_y))\n",
    "        \n",
    "        param_error.append(np.mean(k_error))\n",
    "        \n",
    "    print(param_error) \n",
    "    return param_grid[np.argmin(param_error)]  \n",
    "\n",
    "bestweight=MultiLayerKfold(5,[1e-2,5e-2,1e-1,5e-1,1])     \n",
    "print(bestweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KernelNet(\n",
      "  (fc1): Linear(in_features=256, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0\n",
      "            Train set - loss: 91.49512417067105\n",
      "            Test set - loss: 92.03883480928617\n",
      "            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100\n",
      "            Train set - loss: 5.8071472045174835\n",
      "            Test set - loss: 7.587067672578962\n",
      "            \n",
      "epoch 200\n",
      "            Train set - loss: 4.882626249948497\n",
      "            Test set - loss: 7.804970720085797\n",
      "            \n",
      "epoch 300\n",
      "            Train set - loss: 4.598258152860839\n",
      "            Test set - loss: 7.999639653779151\n",
      "            \n",
      "epoch 400\n",
      "            Train set - loss: 4.467019710147934\n",
      "            Test set - loss: 8.073041627297142\n",
      "            \n",
      "epoch 500\n",
      "            Train set - loss: 4.403460426675076\n",
      "            Test set - loss: 8.210037399841227\n",
      "            \n",
      "epoch 600\n",
      "            Train set - loss: 4.26959337768238\n",
      "            Test set - loss: 8.127538547468102\n",
      "            \n",
      "epoch 700\n",
      "            Train set - loss: 4.22016142203932\n",
      "            Test set - loss: 8.081953147633088\n",
      "            \n",
      "epoch 800\n",
      "            Train set - loss: 4.173269498406894\n",
      "            Test set - loss: 8.141564714049865\n",
      "            \n",
      "epoch 900\n",
      "            Train set - loss: 4.190003533997982\n",
      "            Test set - loss: 8.221424986114872\n",
      "            \n",
      "epoch 1000\n",
      "            Train set - loss: 4.113216701997502\n",
      "            Test set - loss: 8.120443239337858\n",
      "            \n",
      "epoch 1100\n",
      "            Train set - loss: 4.103002528871206\n",
      "            Test set - loss: 8.163798414277057\n",
      "            \n",
      "epoch 1200\n",
      "            Train set - loss: 4.091781428066241\n",
      "            Test set - loss: 8.149989095237204\n",
      "            \n",
      "epoch 1300\n",
      "            Train set - loss: 4.083858088525136\n",
      "            Test set - loss: 8.08900129502383\n",
      "            \n",
      "epoch 1400\n",
      "            Train set - loss: 4.074076482235113\n",
      "            Test set - loss: 8.160624162007817\n",
      "            \n",
      "epoch 1500\n",
      "            Train set - loss: 4.060662590918585\n",
      "            Test set - loss: 8.143663423514191\n",
      "            \n",
      "epoch 1600\n",
      "            Train set - loss: 4.063326156291989\n",
      "            Test set - loss: 8.11455595137518\n",
      "            \n",
      "epoch 1700\n",
      "            Train set - loss: 4.061255516874654\n",
      "            Test set - loss: 8.170148062360155\n",
      "            \n",
      "epoch 1800\n",
      "            Train set - loss: 4.05377042219136\n",
      "            Test set - loss: 8.151747578892808\n",
      "            \n",
      "epoch 1900\n",
      "            Train set - loss: 4.061818843119241\n",
      "            Test set - loss: 8.146029125760172\n",
      "            \n",
      "KernelNet complexity and model fitted in 93.242 s\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(mydataset(nntrain_x, nntrain_y),batch_size=128, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = KernelNet()\n",
    "net = net.to(device)\n",
    "torch.manual_seed(1)\n",
    "net.apply(init_weights)\n",
    "print(net)\n",
    "criterion=nn.MSELoss() \n",
    "optimizer=optim.SGD(net.parameters(),lr=1e-3,momentum=0.9,weight_decay=bestweight) #optim.Adam(...)\n",
    "\n",
    "loss=[]\n",
    "kernelnn_trainloss=[]\n",
    "kernelnn_testloss=[]\n",
    "t0 = time.time()\n",
    "for epoch in range(2000): \n",
    "    for x, y in train_loader: #for batch, (x, y) in enumerate(train_loader): \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        y_pred = net(x)\n",
    "        y_pred = torch.squeeze(y_pred)\n",
    "        train_loss = criterion(y_pred, y)\n",
    "        loss.append(train_loss)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    x0=torch.from_numpy(train_x[:]).float()\n",
    "    with torch.no_grad():\n",
    "        x0 = x0.to(device)\n",
    "        train_pred = net(x0)\n",
    "    \n",
    "    x0=torch.from_numpy(test_x[:]).float()\n",
    "    with torch.no_grad():\n",
    "        x0 = x0.to(device)\n",
    "        test_pred = net(x0)\n",
    "    \n",
    "    kernelnn_trainloss.append(mean_squared_error(train_pred,train_y))\n",
    "    kernelnn_testloss.append(mean_squared_error(test_pred,test_y))\n",
    "    \n",
    "    if epoch % 100 == 0:        \n",
    "        print(f'''epoch {epoch}\n",
    "            Train set - loss: {kernelnn_trainloss[-1]}\n",
    "            Test set - loss: {kernelnn_testloss[-1]}\n",
    "            ''')\n",
    "        \n",
    "    \n",
    "dnn_fit = time.time() - t0\n",
    "print(\"KernelNet complexity and model fitted in %.3f s\" % dnn_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_x0=torch.from_numpy(test_x[:]).float()\n",
    "with torch.no_grad():\n",
    "    kernel_x0 = kernel_x0.to(device)\n",
    "    kernel_pred = net(kernel_x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conformal confidence bands for MLKM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1901.0\n",
      "length tensor(10.9357, dtype=torch.float64)\n",
      "95 coverage 0.9495\n"
     ]
    }
   ],
   "source": [
    "##conformal prediction comparison\n",
    "#predict\n",
    "x0=torch.from_numpy(calibration_x[:]).float()\n",
    "with torch.no_grad():\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    score=np.abs(pred.reshape(-1)-calibration_y[:])\n",
    "sorted_score, sorted_indices=torch.sort(score)\n",
    "q=(len(calibration_x)+1)*0.95\n",
    "print(np.ceil(q))\n",
    "a=sorted_score[int(np.ceil(q))]\n",
    "\n",
    "coverage=0\n",
    "x0=torch.from_numpy(test_x[:]).float()\n",
    "with torch.no_grad():\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "for i in range(len(test_x)):\n",
    "    if pred.detach().numpy()[i][0]-a<test_y[i] and pred.detach().numpy()[i][0]+a>test_y[i]:\n",
    "        coverage=coverage+1\n",
    "coverage=coverage/len(test_x)\n",
    "\n",
    "print(\"length\",2*a)\n",
    "print(\"95 coverage\",coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [22:02<00:00, 264.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19.745476, 15.647827, 13.649408, 7.5327163, 7.4134703]\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(mydataset(nntrain_x, nntrain_y),batch_size=128, shuffle=True)\n",
    "\n",
    "rff0=RandomFourierFeature(p,256,kernel='G',gamma=1)\n",
    "rff1=RandomFourierFeature(16,16,kernel='G',gamma=1)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,infeatures,outfeatures,rff):\n",
    "        super(ResidualBlock,self).__init__()\n",
    "        self.infeatures = infeatures\n",
    "        self.outfeatures = outfeatures\n",
    "        self.rff=rff\n",
    "        \n",
    "        self.fc1 = nn.Linear(infeatures,outfeatures)\n",
    "        self.fc2 = nn.Linear(outfeatures,outfeatures)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        rff=self.rff\n",
    "        x = self.fc1(x)\n",
    "        y = rff.transform(x)\n",
    "        y = self.fc2(y)\n",
    "        return x+y\n",
    "\n",
    "class ResKernelNet(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(ResKernelNet, self).__init__()\n",
    "        self.rblock1 = ResidualBlock(256,16,rff1)\n",
    "        self.fc2 =nn.Linear(16,1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = rff0.transform(x)\n",
    "        x = self.rblock1(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "#initialize\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.normal_(m.weight,mean=0,std=0.5)\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.uniform_(m.weight,a=-0.1,b=0.1)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "def MultiLayerKfold(kfold,param_grid):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    param_error=[]\n",
    "    for param in tqdm(param_grid):\n",
    "        k_error=[]\n",
    "        for k in range(kfold):\n",
    "            Kfold_val_x, Kfold_val_y=nntrain_x[int(n/5)*k:int(n/5)*(k+1)], nntrain_y[int(n/5)*k:int(n/5)*(k+1)]\n",
    "            Kfold_train_x = torch.cat((nntrain_x[:int(n/5)*k],nntrain_x[int(n/5)*(k+1):]),dim = 0)\n",
    "            Kfold_train_y = torch.cat((nntrain_y[:int(n/5)*k],nntrain_y[int(n/5)*(k+1):]),dim = 0)\n",
    "           \n",
    "            net = ResKernelNet()  #### ResKernelNet!\n",
    "            net = net.to(device)\n",
    "            torch.manual_seed(1)\n",
    "            net.apply(init_weights)\n",
    "            criterion=nn.MSELoss() \n",
    "            optimizer=optim.SGD(net.parameters(),lr=1e-3,momentum=0.9,weight_decay=param) #optim.Adam(...)\n",
    "            kfoldtrain_loader = DataLoader(mydataset(Kfold_train_x, Kfold_train_y),batch_size=128, shuffle=True)\n",
    "            \n",
    "            for epoch in range(2000): \n",
    "                for x, y in kfoldtrain_loader: #for batch, (x, y) in enumerate(train_loader): \n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    # Compute prediction error\n",
    "                    y_pred = net(x)\n",
    "                    y_pred = torch.squeeze(y_pred)\n",
    "                    train_loss = criterion(y_pred, y)\n",
    "                    # Backpropagation\n",
    "                    optimizer.zero_grad() \n",
    "                    train_loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            x0=Kfold_val_x[:].float()\n",
    "            with torch.no_grad():\n",
    "                x0 = x0.to(device)\n",
    "                val_pred = net(x0)\n",
    "            \n",
    "            k_error.append(mean_squared_error(val_pred,Kfold_val_y))\n",
    "        \n",
    "        param_error.append(np.mean(k_error))\n",
    "        \n",
    "    print(param_error) \n",
    "    return param_grid[np.argmin(param_error)]  \n",
    "\n",
    "bestweight=MultiLayerKfold(5,[1e-2,5e-2,1e-1,5e-1,1])     \n",
    "print(bestweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResKernelNet(\n",
      "  (rblock1): ResidualBlock(\n",
      "    (fc1): Linear(in_features=256, out_features=16, bias=True)\n",
      "    (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  )\n",
      "  (fc2): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "            Train set - loss: 102.13534836111357\n",
      "            Test set - loss: 102.72259852400295\n",
      "            \n",
      "epoch 100\n",
      "            Train set - loss: 7.5448670411906695\n",
      "            Test set - loss: 7.756988758235942\n",
      "            \n",
      "epoch 200\n",
      "            Train set - loss: 7.117017758529597\n",
      "            Test set - loss: 7.296580325869634\n",
      "            \n",
      "epoch 300\n",
      "            Train set - loss: 7.148051776491106\n",
      "            Test set - loss: 7.329094997944564\n",
      "            \n",
      "epoch 400\n",
      "            Train set - loss: 7.278905944753941\n",
      "            Test set - loss: 7.474755101131765\n",
      "            \n",
      "epoch 500\n",
      "            Train set - loss: 7.082713929708004\n",
      "            Test set - loss: 7.2457159481989635\n",
      "            \n",
      "epoch 600\n",
      "            Train set - loss: 7.157566554500037\n",
      "            Test set - loss: 7.341435422435338\n",
      "            \n",
      "epoch 700\n",
      "            Train set - loss: 7.134712933458665\n",
      "            Test set - loss: 7.313344723057472\n",
      "            \n",
      "epoch 800\n",
      "            Train set - loss: 7.084893974216241\n",
      "            Test set - loss: 7.25676301242796\n",
      "            \n",
      "epoch 900\n",
      "            Train set - loss: 7.093154858848542\n",
      "            Test set - loss: 7.2643813191102335\n",
      "            \n",
      "epoch 1000\n",
      "            Train set - loss: 7.25229302429744\n",
      "            Test set - loss: 7.445367387034242\n",
      "            \n",
      "epoch 1100\n",
      "            Train set - loss: 7.150250421776752\n",
      "            Test set - loss: 7.330676599081851\n",
      "            \n",
      "epoch 1200\n",
      "            Train set - loss: 7.077615905401629\n",
      "            Test set - loss: 7.247591173267985\n",
      "            \n",
      "epoch 1300\n",
      "            Train set - loss: 7.09436979077537\n",
      "            Test set - loss: 7.265923969514417\n",
      "            \n",
      "epoch 1400\n",
      "            Train set - loss: 7.218928485245843\n",
      "            Test set - loss: 7.411274776236791\n",
      "            \n",
      "epoch 1500\n",
      "            Train set - loss: 7.108394104754485\n",
      "            Test set - loss: 7.283231639498317\n",
      "            \n",
      "epoch 1600\n",
      "            Train set - loss: 7.1741547200959905\n",
      "            Test set - loss: 7.362152297391932\n",
      "            \n",
      "epoch 1700\n",
      "            Train set - loss: 7.319562209167363\n",
      "            Test set - loss: 7.5186647591009965\n",
      "            \n",
      "epoch 1800\n",
      "            Train set - loss: 7.244060194705972\n",
      "            Test set - loss: 7.443643053309967\n",
      "            \n",
      "epoch 1900\n",
      "            Train set - loss: 7.218238180129125\n",
      "            Test set - loss: 7.411707418103064\n",
      "            \n",
      "Residual KernelNet complexity and model fitted in 93.964 s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = ResKernelNet()\n",
    "net = net.to(device)\n",
    "torch.manual_seed(1)\n",
    "net.apply(init_weights)\n",
    "print(net)\n",
    "criterion=nn.MSELoss() \n",
    "optimizer=optim.SGD(net.parameters(),lr=1e-3,momentum=0.9,weight_decay=bestweight) #optim.Adam(...)\n",
    "\n",
    "loss=[]\n",
    "reskernel_trainloss=[]\n",
    "reskernel_testloss=[]\n",
    "t0 = time.time()\n",
    "for epoch in range(2000): \n",
    "    for x, y in train_loader: #for batch, (x, y) in enumerate(train_loader): \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        y_pred = net(x)\n",
    "        y_pred = torch.squeeze(y_pred)\n",
    "        train_loss = criterion(y_pred, y)\n",
    "        loss.append(train_loss)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    x0=torch.from_numpy(train_x[:]).float()\n",
    "    with torch.no_grad():\n",
    "        x0 = x0.to(device)\n",
    "        train_pred = net(x0)\n",
    "    \n",
    "    x0=torch.from_numpy(test_x[:]).float()\n",
    "    with torch.no_grad():\n",
    "        x0 = x0.to(device)\n",
    "        test_pred = net(x0)\n",
    "    \n",
    "    reskernel_trainloss.append(mean_squared_error(train_pred,train_y))\n",
    "    reskernel_testloss.append(mean_squared_error(test_pred,test_y))\n",
    "    \n",
    "    if epoch % 100 == 0:        \n",
    "        print(f'''epoch {epoch}\n",
    "            Train set - loss: {reskernel_trainloss[-1]}\n",
    "            Test set - loss: {reskernel_testloss[-1]}\n",
    "            ''')\n",
    "        \n",
    "    \n",
    "dnn_fit = time.time() - t0\n",
    "print(\"Residual KernelNet complexity and model fitted in %.3f s\" % dnn_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rk_x0=torch.from_numpy(test_x[:]).float()\n",
    "with torch.no_grad():\n",
    "    rk_x0 = rk_x0.to(device)\n",
    "    rk_pred = net(rk_x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1901.0\n",
      "length tensor(10.4494, dtype=torch.float64)\n",
      "95 coverage 0.94575\n"
     ]
    }
   ],
   "source": [
    "##conformal prediction comparison\n",
    "#predict\n",
    "x0=torch.from_numpy(calibration_x[:]).float()\n",
    "with torch.no_grad():\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    score=np.abs(pred.reshape(-1)-calibration_y[:])\n",
    "sorted_score, sorted_indices=torch.sort(score)\n",
    "q=(len(calibration_x)+1)*0.95\n",
    "print(np.ceil(q))\n",
    "a=sorted_score[int(np.ceil(q))]\n",
    "\n",
    "coverage=0\n",
    "x0=torch.from_numpy(test_x[:]).float()\n",
    "with torch.no_grad():\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "for i in range(len(test_x)):\n",
    "    if pred.detach().numpy()[i][0]-a<test_y[i] and pred.detach().numpy()[i][0]+a>test_y[i]:\n",
    "        coverage=coverage+1\n",
    "coverage=coverage/len(test_x)\n",
    "\n",
    "print(\"length\",2*a)\n",
    "print(\"95 coverage\",coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148.8951327735091\n",
      "8.164069507828035\n",
      "8.13121249300229\n",
      "7.53666075054831\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(krr_pred,test_y))\n",
    "print(mean_squared_error(rf_pred,test_y))\n",
    "print(mean_squared_error(kernel_pred,test_y))\n",
    "print(mean_squared_error(rk_pred,test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
