{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: California Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.503149</td>\n",
       "      <td>-1.083767</td>\n",
       "      <td>-0.462001</td>\n",
       "      <td>0.018689</td>\n",
       "      <td>1.535208</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>-0.689106</td>\n",
       "      <td>0.648722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.364659</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>-0.418034</td>\n",
       "      <td>-0.088311</td>\n",
       "      <td>0.205330</td>\n",
       "      <td>0.054431</td>\n",
       "      <td>-0.857653</td>\n",
       "      <td>0.653714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.715648</td>\n",
       "      <td>1.617807</td>\n",
       "      <td>-0.219152</td>\n",
       "      <td>-0.279204</td>\n",
       "      <td>-0.119633</td>\n",
       "      <td>-0.035265</td>\n",
       "      <td>0.523497</td>\n",
       "      <td>-0.089991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.700173</td>\n",
       "      <td>-1.083767</td>\n",
       "      <td>0.410511</td>\n",
       "      <td>0.687172</td>\n",
       "      <td>1.008909</td>\n",
       "      <td>-0.065649</td>\n",
       "      <td>-0.735924</td>\n",
       "      <td>1.502236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.333002</td>\n",
       "      <td>-1.719432</td>\n",
       "      <td>-0.055459</td>\n",
       "      <td>0.009491</td>\n",
       "      <td>0.276858</td>\n",
       "      <td>-0.050146</td>\n",
       "      <td>0.762273</td>\n",
       "      <td>-1.128183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14443</th>\n",
       "      <td>-1.099272</td>\n",
       "      <td>-0.924851</td>\n",
       "      <td>-0.555332</td>\n",
       "      <td>-0.020087</td>\n",
       "      <td>-0.305957</td>\n",
       "      <td>-0.057375</td>\n",
       "      <td>-0.876380</td>\n",
       "      <td>0.843383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14444</th>\n",
       "      <td>2.468780</td>\n",
       "      <td>1.061601</td>\n",
       "      <td>0.549474</td>\n",
       "      <td>-0.179651</td>\n",
       "      <td>-0.593833</td>\n",
       "      <td>-0.016802</td>\n",
       "      <td>-0.567377</td>\n",
       "      <td>-0.030095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14445</th>\n",
       "      <td>-1.441260</td>\n",
       "      <td>1.061601</td>\n",
       "      <td>-0.217538</td>\n",
       "      <td>-0.189596</td>\n",
       "      <td>-0.574406</td>\n",
       "      <td>-0.040979</td>\n",
       "      <td>-0.796789</td>\n",
       "      <td>0.653714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14446</th>\n",
       "      <td>-0.651904</td>\n",
       "      <td>-1.481058</td>\n",
       "      <td>-0.176785</td>\n",
       "      <td>-0.305747</td>\n",
       "      <td>-0.090492</td>\n",
       "      <td>-0.028604</td>\n",
       "      <td>-0.890426</td>\n",
       "      <td>1.202758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14447</th>\n",
       "      <td>-0.824872</td>\n",
       "      <td>0.505394</td>\n",
       "      <td>-0.326778</td>\n",
       "      <td>0.169335</td>\n",
       "      <td>-0.367771</td>\n",
       "      <td>-0.046552</td>\n",
       "      <td>1.010411</td>\n",
       "      <td>-1.312861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14448 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0     -0.503149 -1.083767 -0.462001  0.018689  1.535208 -0.036385 -0.689106   \n",
       "1     -0.364659  0.982143 -0.418034 -0.088311  0.205330  0.054431 -0.857653   \n",
       "2     -0.715648  1.617807 -0.219152 -0.279204 -0.119633 -0.035265  0.523497   \n",
       "3     -0.700173 -1.083767  0.410511  0.687172  1.008909 -0.065649 -0.735924   \n",
       "4      0.333002 -1.719432 -0.055459  0.009491  0.276858 -0.050146  0.762273   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "14443 -1.099272 -0.924851 -0.555332 -0.020087 -0.305957 -0.057375 -0.876380   \n",
       "14444  2.468780  1.061601  0.549474 -0.179651 -0.593833 -0.016802 -0.567377   \n",
       "14445 -1.441260  1.061601 -0.217538 -0.189596 -0.574406 -0.040979 -0.796789   \n",
       "14446 -0.651904 -1.481058 -0.176785 -0.305747 -0.090492 -0.028604 -0.890426   \n",
       "14447 -0.824872  0.505394 -0.326778  0.169335 -0.367771 -0.046552  1.010411   \n",
       "\n",
       "              7  \n",
       "0      0.648722  \n",
       "1      0.653714  \n",
       "2     -0.089991  \n",
       "3      1.502236  \n",
       "4     -1.128183  \n",
       "...         ...  \n",
       "14443  0.843383  \n",
       "14444 -0.030095  \n",
       "14445  0.653714  \n",
       "14446  1.202758  \n",
       "14447 -1.312861  \n",
       "\n",
       "[14448 rows x 8 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge,RidgeCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import resample\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing  \n",
    "\n",
    "housing = fetch_california_housing() \n",
    "x = housing.data\n",
    "y = housing.target.reshape(-1,1)\n",
    "x=preprocessing.StandardScaler().fit(x).transform(x) #normalize\n",
    "x=pd.DataFrame(x)\n",
    "y=pd.DataFrame(y)\n",
    "data=pd.concat([y,x],axis=1)\n",
    "train,test=train_test_split(data,test_size=0.3, random_state=1)\n",
    "\n",
    "train_y = train.iloc[:,0]\n",
    "train_x = train.iloc[:,1:]\n",
    "test_y = test.iloc[:,0]\n",
    "test_x = test.iloc[:,1:]\n",
    "\n",
    "train_x.reset_index(drop=True, inplace=True) \n",
    "test_x.reset_index(drop=True, inplace=True) \n",
    "train_y.reset_index(drop=True, inplace=True) \n",
    "test_y.reset_index(drop=True, inplace=True) \n",
    "\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nntrain_x = torch.from_numpy(train_x.to_numpy()).float()\n",
    "nntrain_y = torch.squeeze(torch.from_numpy(train_y.to_numpy()).float()) \n",
    "nntest_x= torch.from_numpy(test_x.to_numpy()).float()\n",
    "nntest_y = torch.squeeze(torch.from_numpy(test_y.to_numpy()).float())\n",
    "\n",
    "class mydataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "        self._len = len(x)\n",
    "\n",
    "    def __getitem__(self, item): \n",
    "        return self._x[item], self._y[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "definition of random feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_1d(pdf, gamma):\n",
    "    if pdf=='G':\n",
    "        w=torch.randn(1)*gamma\n",
    "        return w\n",
    "    elif pdf=='L':\n",
    "        w=torch.distributions.laplace.Laplace(torch.tensor([0.0]), torch.tensor([1.0])).sample()*gamma\n",
    "        return w\n",
    "    elif pdf=='C':\n",
    "        w=torch.distributions.cauchy.Cauchy(torch.tensor([0.0]), torch.tensor([1.0])).sample()*gamma\n",
    "        return w\n",
    "    \n",
    "def sample(pdf, gamma, d):\n",
    "    return torch.tensor([sample_1d(pdf, gamma) for _ in range(d)])\n",
    "\n",
    "class RandomFourierFeature:\n",
    "    \"\"\"Random Fourier Feature\n",
    "    Parameters\n",
    "    ----------\n",
    "    d : int\n",
    "        Input space dimension\n",
    "    D : int\n",
    "        Feature space dimension\n",
    "    W : shape (D,d)\n",
    "    b : shape (D)\n",
    "    kernel : char\n",
    "        Kernel to use; 'G', 'L', or 'C'\n",
    "    gamma : float\n",
    "        pdf parameter\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d, D, W=None, b=None, kernel='G', gamma=1):\n",
    "\n",
    "        self.d = d\n",
    "        self.D = D\n",
    "        self.gamma = gamma\n",
    "\n",
    "        kernel = kernel.upper()\n",
    "        if kernel not in ['G', 'L', 'C']:\n",
    "            raise Exception('Invalid Kernel')\n",
    "        self.kernel = kernel\n",
    "\n",
    "        if W is None or b is None:\n",
    "            self.create()\n",
    "        else:\n",
    "            self.__load(W, b)\n",
    "\n",
    "    def __load(self, W, b):\n",
    "        \"\"\"Load from existing Arrays\"\"\"\n",
    "\n",
    "        self.W = W.reshape([self.D, self.d])\n",
    "        self.b = b\n",
    "    \n",
    "\n",
    "    def create(self):\n",
    "        \"\"\"Create a d->D fourier random feature\"\"\"\n",
    "\n",
    "        self.b = torch.rand(self.D)*2*torch.pi\n",
    "        self.W = sample(self.kernel, self.gamma, self.d*self.D).reshape(self.D,self.d)\n",
    "\n",
    "    def transform(self, x):\n",
    "        \"\"\"Transform a vector using this feature\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (shape=(n,d))\n",
    "            to transform; must be single dimension vector\n",
    "        Returns\n",
    "        -------\n",
    "        x : (shape=(n,D))\n",
    "            Feature space transformation of x\n",
    "        \"\"\"\n",
    "        #print(self.W.shape,self.b.reshape(-1,1).shape,x.shape)\n",
    "        #print((self.W @ x.T).shape)\n",
    "       \n",
    "        result=torch.sqrt(torch.tensor([2.0/self.D])) * torch.cos( self.W @ x.T  + (self.b.reshape(-1,1) @ torch.ones(len(x)).reshape(1,-1))) \n",
    "        #print(result.T.shape)\n",
    "        return result.T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLKM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KernelNet(\n",
      "  (fc1): Linear(in_features=32, out_features=8, bias=True)\n",
      "  (fc2): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0\n",
      "            Train set - loss: 2.6665494633133524\n",
      "            Test  set - loss: 2.5766404366248956\n",
      "            \n",
      "epoch 50\n",
      "            Train set - loss: 0.6938056412308787\n",
      "            Test  set - loss: 0.6959013202317642\n",
      "            \n",
      "epoch 100\n",
      "            Train set - loss: 0.6332139090461644\n",
      "            Test  set - loss: 0.6424973726122649\n",
      "            \n",
      "epoch 150\n",
      "            Train set - loss: 0.6049812387435732\n",
      "            Test  set - loss: 0.62211246661266\n",
      "            \n",
      "epoch 200\n",
      "            Train set - loss: 0.5924283802088788\n",
      "            Test  set - loss: 0.6129705762782863\n",
      "            \n",
      "epoch 250\n",
      "            Train set - loss: 0.586343404990532\n",
      "            Test  set - loss: 0.6086706876824728\n",
      "            \n",
      "KernelNet complexity and model fitted in 85.625 s\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(mydataset(nntrain_x, nntrain_y),batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(mydataset(nntest_x, nntest_y),batch_size=100, shuffle=False)\n",
    "\n",
    "rff1=RandomFourierFeature(8,32,kernel='G',gamma=1)\n",
    "rff2=RandomFourierFeature(8,8,kernel='G',gamma=5)\n",
    "\n",
    "class KernelNet(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(KernelNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(32, 8)\n",
    "        self.fc2 = nn.Linear(8, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = rff1.transform(x)\n",
    "        x=self.fc1(x)\n",
    "        x = rff2.transform(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "#initialize\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.normal_(m.weight,mean=0,std=0.5)\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.uniform_(m.weight,a=-0.1,b=0.1)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = KernelNet()\n",
    "net = net.to(device)\n",
    "torch.manual_seed(1)\n",
    "net.apply(init_weights)\n",
    "print(net)\n",
    "criterion=nn.MSELoss() \n",
    "optimizer=optim.SGD(net.parameters(),lr=1e-4,momentum=0.9,weight_decay=1e-2) #optim.Adam(...)\n",
    "loss=[]\n",
    "kernelnn_trainloss=[]\n",
    "kernelnn_testloss=[]\n",
    "t0 = time.time()\n",
    "for epoch in range(300): \n",
    "    for x, y in train_loader: #for batch, (x, y) in enumerate(train_loader): \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        y_pred = net(x)\n",
    "        y_pred = torch.squeeze(y_pred)\n",
    "        train_loss = criterion(y_pred, y)\n",
    "        loss.append(train_loss)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    x0=torch.from_numpy(train_x[:].to_numpy()).float()\n",
    "    with torch.no_grad():\n",
    "        x0 = x0.to(device)\n",
    "        pred = net(x0)\n",
    "    kernelnn_trainloss.append(mean_squared_error(pred,train_y))\n",
    "    \n",
    "    x1=torch.from_numpy(test_x[:].to_numpy()).float()\n",
    "    with torch.no_grad():\n",
    "        x1 = x1.to(device)\n",
    "        pred = net(x1)\n",
    "    kernelnn_testloss.append(mean_squared_error(pred,test_y))\n",
    "    \n",
    "    if epoch>50 and float(kernelnn_trainloss[-1])>max(kernelnn_trainloss[-50:-1]):\n",
    "        break\n",
    "    \n",
    "    if epoch % 50 == 0:        \n",
    "        print(f'''epoch {epoch}\n",
    "            Train set - loss: {kernelnn_trainloss[-1]}\n",
    "            Test  set - loss: {kernelnn_testloss[-1]}\n",
    "            ''')\n",
    "        \n",
    "    \n",
    "dnn_fit = time.time() - t0\n",
    "print(\"KernelNet complexity and model fitted in %.3f s\" % dnn_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.2746758  0.55670595 2.8003778  ... 1.4973042  1.8283037  2.1257336 ]\n",
      "[3.55  0.707 2.294 ... 1.098 1.625 1.667]\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "x0=torch.from_numpy(test_x[:].to_numpy()).float()\n",
    "with torch.no_grad():\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    print(np.array(pred).reshape(-1))\n",
    "    print(test_y[:].to_numpy())\n",
    "    bootbase=np.array(pred.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14448/14448 [00:43<00:00, 331.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14448, 273])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6192/6192 [00:03<00:00, 1781.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-p: 14175  mark: 6192\n",
      "length 3.0183808035289035\n",
      "95 coverage 0.9387919896640827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "par=optimizer.param_groups[0]['params']\n",
    "\n",
    "for i in tqdm(range(len(train_x))):\n",
    "    x0=torch.from_numpy(train_x[i:1+i].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    fi=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()   \n",
    "    for j in range(len(par)): \n",
    "        fi=torch.cat([fi,par[j].grad.reshape(-1)])\n",
    "    fi=fi.reshape(1,-1)\n",
    "    if i==0:\n",
    "        Fi=fi\n",
    "    else:\n",
    "        Fi=torch.cat([Fi,fi])   \n",
    "print(Fi.shape)\n",
    "\n",
    "temp=torch.linalg.inv(Fi.T @ Fi)\n",
    "\n",
    "length=[]\n",
    "coverage=0\n",
    "mark=0\n",
    "for i in tqdm(range(len(test_x))):\n",
    "    x0=torch.from_numpy(test_x[i:i+1].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    #print(pred.detach().numpy()[0][0],test_y[i])\n",
    "    par=optimizer.param_groups[0]['params']\n",
    "    f0=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()\n",
    "    for j in range(len(par)):\n",
    "        f0=torch.cat([f0,par[j].grad.reshape(-1)])\n",
    "    f0=f0.reshape(-1,1)\n",
    "\n",
    "    fFFf=f0.T @ temp @ f0\n",
    "    #print(2*1.96*np.sqrt(float(fFFf+1))) #approximate with df(infinity)\n",
    "    \n",
    "    if fFFf < 0:\n",
    "        continue\n",
    "    mark=mark+1\n",
    "    length.append(2*1.96*np.sqrt(float(fFFf+1))*np.sqrt(kernelnn_trainloss[-1]))\n",
    "    \n",
    "    #coverage\n",
    "    if pred.detach().numpy()[0][0]-1.96*np.sqrt(float(fFFf+1))*np.sqrt(kernelnn_trainloss[-1])<test_y[i] and pred.detach().numpy()[0][0]+1.96*np.sqrt(float(fFFf+1))*np.sqrt(kernelnn_trainloss[-1])>test_y[i]:\n",
    "        coverage=coverage+1\n",
    "coverage=coverage/mark\n",
    "\n",
    "print(\"n-p:\",len(train_x)-f0.shape[0],\" mark:\",mark) \n",
    "print(\"length\",np.mean(length))\n",
    "print(\"95 coverage\",coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14448/14448 [00:40<00:00, 354.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14448, 273])\n",
      "14175.000122070312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6192/6192 [00:03<00:00, 1806.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-p: 14175  mark: 6192\n",
      "length 3.018383146315269\n",
      "95 coverage 0.9387919896640827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#penalty\n",
    "#predict\n",
    "wei=1e-4\n",
    "par=optimizer.param_groups[0]['params']\n",
    "\n",
    "for i in tqdm(range(len(train_x))):\n",
    "    x0=torch.from_numpy(train_x[i:1+i].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    fi=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()   \n",
    "    for j in range(len(par)): \n",
    "        fi=torch.cat([fi,par[j].grad.reshape(-1)])\n",
    "    fi=fi.reshape(1,-1)\n",
    "    if i==0:\n",
    "        Fi=fi\n",
    "    else:\n",
    "        Fi=torch.cat([Fi,fi])   \n",
    "print(Fi.shape)\n",
    "\n",
    "\n",
    "temp2=torch.linalg.inv(Fi.T @ Fi+wei *np.eye(Fi.T.shape[0]))\n",
    "temp2=temp2.float()\n",
    "temp=temp2@Fi.T @ Fi@temp2\n",
    "p=Fi @temp2 @Fi.T\n",
    "aa=len(train_x)-np.trace(2*p-p@p)\n",
    "print(aa)\n",
    "corr=(len(train_x)-f0.shape[0])/aa\n",
    "\n",
    "length=[]\n",
    "coverage=0\n",
    "mark=0\n",
    "for i in tqdm(range(len(test_x))):\n",
    "    x0=torch.from_numpy(test_x[i:i+1].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    #print(pred.detach().numpy()[0][0],test_y[i])\n",
    "    par=optimizer.param_groups[0]['params']\n",
    "    f0=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()\n",
    "    for j in range(len(par)):\n",
    "        f0=torch.cat([f0,par[j].grad.reshape(-1)])\n",
    "    f0=f0.reshape(-1,1)\n",
    "\n",
    "    fFFf=f0.T @ temp @ f0\n",
    "    #print(2*1.96*np.sqrt(float(fFFf+1))) #approximate with df(infinity)\n",
    "    \n",
    "    if fFFf < 0:\n",
    "        continue\n",
    "    mark=mark+1\n",
    "    dd=1.96*np.sqrt(float(fFFf+1))*np.sqrt(kernelnn_trainloss[-1])*corr\n",
    "    length.append(2*dd)\n",
    "    \n",
    "    #coverage\n",
    "    if pred.detach().numpy()[0][0]-dd<test_y[i] and pred.detach().numpy()[0][0]+dd>test_y[i]:\n",
    "        coverage=coverage+1\n",
    "coverage=coverage/mark\n",
    "\n",
    "print(\"n-p:\",len(train_x)-f0.shape[0],\" mark:\",mark) \n",
    "print(\"length\",np.mean(length))\n",
    "print(\"95 coverage\",coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14448/14448 [00:43<00:00, 334.15it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14448/14448 [00:09<00:00, 1450.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13727.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6192/6192 [00:04<00:00, 1302.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length 3.1685171665609344\n",
      "95 coverage 0.9465340426339895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##conformal prediction\n",
    "#predict\n",
    "par=optimizer.param_groups[0]['params']\n",
    "for i in tqdm(range(len(train_x))):\n",
    "    x0=torch.from_numpy(train_x[i:1+i].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    fi=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()   \n",
    "    for j in range(len(par)): \n",
    "        fi=torch.cat([fi,par[j].grad.reshape(-1)])\n",
    "    fi=fi.reshape(1,-1)\n",
    "    if i==0:\n",
    "        Fi=fi\n",
    "    else:\n",
    "        Fi=torch.cat([Fi,fi])   \n",
    "temp=torch.linalg.inv(Fi.T @ Fi)\n",
    "\n",
    "mark=0\n",
    "score=torch.tensor([])\n",
    "for i in tqdm(range(len(train_x))):\n",
    "    x0=torch.from_numpy(train_x[i:i+1].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    par=optimizer.param_groups[0]['params']\n",
    "    f0=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()\n",
    "    for j in range(len(par)):\n",
    "        f0=torch.cat([f0,par[j].grad.reshape(-1)])\n",
    "    f0=f0.reshape(-1,1)\n",
    "    fFFf=f0.T @ temp @ f0\n",
    "    score=torch.cat([score,np.abs(pred.detach().numpy()[0][0]-train_y[i])/np.sqrt(kernelnn_trainloss[-1])/np.sqrt(fFFf+1)])\n",
    "    if fFFf < 0:\n",
    "        continue\n",
    "    mark=mark+1\n",
    "score=score.reshape(-1)\n",
    "sorted_score, sorted_indices=torch.sort(score)\n",
    "q=(len(train_x)+1)*0.95\n",
    "print(np.ceil(q))\n",
    "a=sorted_score[int(np.ceil(q))]\n",
    "\n",
    "\n",
    "mark=0\n",
    "length=[]\n",
    "for i in tqdm(range(len(test_x))):\n",
    "    x0=torch.from_numpy(test_x[i:i+1].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    par=optimizer.param_groups[0]['params']\n",
    "    f0=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()\n",
    "    for j in range(len(par)):\n",
    "        f0=torch.cat([f0,par[j].grad.reshape(-1)])\n",
    "    f0=f0.reshape(-1,1)\n",
    "\n",
    "    fFFf=f0.T @ temp @ f0\n",
    "    \n",
    "    if fFFf < 0:\n",
    "        continue\n",
    "    mark=mark+1\n",
    "    dd=(np.sqrt(kernelnn_trainloss[-1])*np.sqrt(fFFf+1)*a).detach().numpy()[0][0]\n",
    "    length.append(2*dd)\n",
    "    \n",
    "    #coverage\n",
    "    if pred.detach().numpy()[0][0]-dd<test_y[i] and pred.detach().numpy()[0][0]+dd>test_y[i]:\n",
    "        coverage=coverage+1\n",
    "coverage=coverage/mark\n",
    "\n",
    "\n",
    "print(\"length\",np.mean(length))\n",
    "print(\"95 coverage\",coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13727.0\n",
      "length tensor(3.1678, dtype=torch.float64)\n",
      "95 coverage 0.8480297157622739\n"
     ]
    }
   ],
   "source": [
    "##conformal prediction\n",
    "#predict\n",
    "x0=torch.from_numpy(train_x[:].to_numpy()).float()\n",
    "with torch.no_grad():\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    score=np.abs(pred.reshape(-1)-train_y[:].to_numpy())\n",
    "sorted_score, sorted_indices=torch.sort(score)\n",
    "q=(len(train_x)+1)*0.95\n",
    "print(np.ceil(q))\n",
    "a=sorted_score[int(np.ceil(q))]\n",
    "\n",
    "coverage=0\n",
    "for i in range(len(test_x)):\n",
    "    if pred.detach().numpy()[0][0]-a<test_y[i] and pred.detach().numpy()[0][0]+a>test_y[i]:\n",
    "        coverage=coverage+1\n",
    "coverage=coverage/len(test_x)\n",
    "\n",
    "print(\"length\",2*a)\n",
    "print(\"95 coverage\",coverage)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RKM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResKernelNet(\n",
      "  (rblock1): ResidualBlock(\n",
      "    (fc1): Linear(in_features=32, out_features=8, bias=True)\n",
      "    (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "  )\n",
      "  (fc2): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0\n",
      "            Train set - loss: 1.6695821465984029\n",
      "            Test  set - loss: 1.6146207947575923\n",
      "            \n",
      "epoch 50\n",
      "            Train set - loss: 1.1826632142822484\n",
      "            Test  set - loss: 1.1583738858408261\n",
      "            \n",
      "epoch 100\n",
      "            Train set - loss: 0.8649669027312847\n",
      "            Test  set - loss: 0.8461270673294462\n",
      "            \n",
      "epoch 150\n",
      "            Train set - loss: 0.6793853031073221\n",
      "            Test  set - loss: 0.6706750408398231\n",
      "            \n",
      "epoch 200\n",
      "            Train set - loss: 0.6314902793675504\n",
      "            Test  set - loss: 0.6261299676320806\n",
      "            \n",
      "epoch 250\n",
      "            Train set - loss: 0.6039875700590801\n",
      "            Test  set - loss: 0.5999646097640938\n",
      "            \n",
      "Residual KernelNet complexity and model fitted in 95.876 s\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(mydataset(nntrain_x, nntrain_y),batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(mydataset(nntest_x, nntest_y),batch_size=100, shuffle=False)\n",
    "\n",
    "rff0=RandomFourierFeature(8,32,kernel='G',gamma=0.1)\n",
    "rff1=RandomFourierFeature(8,8,kernel='G',gamma=0.5)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,infeatures,outfeatures,rff):\n",
    "        super(ResidualBlock,self).__init__()\n",
    "        self.infeatures = infeatures\n",
    "        self.outfeatures = outfeatures\n",
    "        self.rff=rff\n",
    "        \n",
    "        self.fc1 = nn.Linear(infeatures,outfeatures)\n",
    "        self.fc2 = nn.Linear(outfeatures,outfeatures)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        rff=self.rff\n",
    "        x = self.fc1(x)\n",
    "        y = rff.transform(x)\n",
    "        y = self.fc2(y)\n",
    "        return x+y\n",
    "\n",
    "class ResKernelNet(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(ResKernelNet, self).__init__()\n",
    "        self.rblock1 = ResidualBlock(32,8,rff1)\n",
    "        self.fc2 =nn.Linear(8,1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = rff0.transform(x)\n",
    "        x = self.rblock1(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "#initialize\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.normal_(m.weight,mean=0,std=0.5)\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.uniform_(m.weight,a=0,b=0.1)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = ResKernelNet()\n",
    "net = net.to(device)\n",
    "torch.manual_seed(1)\n",
    "#net.apply(init_weights)\n",
    "print(net)\n",
    "criterion=nn.MSELoss() \n",
    "optimizer=optim.SGD(net.parameters(),lr=1e-4,momentum=0.9,weight_decay=1e-2) #optim.Adam(...)\n",
    "\n",
    "loss=[]\n",
    "reskernel_trainloss=[]\n",
    "reskernel_testloss=[]\n",
    "t0 = time.time()\n",
    "for epoch in range(300): \n",
    "    for x, y in train_loader: #for batch, (x, y) in enumerate(train_loader): \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        y_pred = net(x)\n",
    "        y_pred = torch.squeeze(y_pred)\n",
    "        train_loss = criterion(y_pred, y)\n",
    "        loss.append(train_loss)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    x0=torch.from_numpy(train_x[:].to_numpy()).float()\n",
    "    with torch.no_grad():\n",
    "        x0 = x0.to(device)\n",
    "        pred = net(x0)\n",
    "    reskernel_trainloss.append(mean_squared_error(pred,train_y))\n",
    "    \n",
    "    x1=torch.from_numpy(test_x[:].to_numpy()).float()\n",
    "    with torch.no_grad():\n",
    "        x1 = x1.to(device)\n",
    "        pred = net(x1)\n",
    "    reskernel_testloss.append(mean_squared_error(pred,test_y))\n",
    "    \n",
    "    if epoch>50 and float(reskernel_trainloss[-1])>max(reskernel_trainloss[-50:-1]):\n",
    "        break\n",
    "    \n",
    "    if epoch % 50 == 0:        \n",
    "        print(f'''epoch {epoch}\n",
    "            Train set - loss: {reskernel_trainloss[-1]}\n",
    "            Test  set - loss: {reskernel_testloss[-1]}\n",
    "            ''')\n",
    "        \n",
    "        \n",
    "    \n",
    "dnn_fit = time.time() - t0\n",
    "print(\"Residual KernelNet complexity and model fitted in %.3f s\" % dnn_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0665967 1.2428577 2.4723632 ... 1.7999063 1.3877339 1.1615189]\n",
      "[3.55  0.707 2.294 ... 1.098 1.625 1.667]\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "x0=torch.from_numpy(test_x[:].to_numpy()).float()\n",
    "with torch.no_grad():\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    print(np.array(pred).reshape(-1))\n",
    "    print(test_y[:].to_numpy())\n",
    "    bootbase=np.array(pred.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14448/14448 [00:51<00:00, 282.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14448, 345])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6192/6192 [00:04<00:00, 1449.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-p: 14103  mark: 6192\n",
      "length 2.990294263663804\n",
      "95 coverage 0.9478359173126615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "par=optimizer.param_groups[0]['params']\n",
    "\n",
    "for i in tqdm(range(len(train_x))):\n",
    "    x0=torch.from_numpy(train_x[i:1+i].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    fi=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()   \n",
    "    for j in range(len(par)): \n",
    "        fi=torch.cat([fi,par[j].grad.reshape(-1)])\n",
    "    fi=fi.reshape(1,-1)\n",
    "    if i==0:\n",
    "        Fi=fi\n",
    "    else:\n",
    "        Fi=torch.cat([Fi,fi])   \n",
    "print(Fi.shape)\n",
    "\n",
    "temp=torch.linalg.pinv(Fi.T @ Fi)\n",
    "\n",
    "length=[]\n",
    "coverage=0\n",
    "mark=0\n",
    "for i in tqdm(range(len(test_x))):\n",
    "    x0=torch.from_numpy(test_x[i:i+1].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    #print(pred.detach().numpy()[0][0],test_y[i])\n",
    "    par=optimizer.param_groups[0]['params']\n",
    "    f0=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()\n",
    "    for j in range(len(par)):\n",
    "        f0=torch.cat([f0,par[j].grad.reshape(-1)])\n",
    "    f0=f0.reshape(-1,1)\n",
    "\n",
    "    fFFf=f0.T @ temp @ f0\n",
    "    #print(2*1.96*np.sqrt(float(fFFf+1))) #approximate with df(infinity)\n",
    "    \n",
    "    if fFFf < 0:\n",
    "        continue\n",
    "    mark=mark+1\n",
    "    length.append(2*1.96*np.sqrt(float(fFFf+1))*np.sqrt(reskernel_trainloss[-1]))\n",
    "    \n",
    "    #coverage\n",
    "    if pred.detach().numpy()[0][0]-1.96*np.sqrt(float(fFFf+1))*np.sqrt(reskernel_trainloss[-1])<test_y[i] and pred.detach().numpy()[0][0]+1.96*np.sqrt(float(fFFf+1))*np.sqrt(reskernel_trainloss[-1])>test_y[i]:\n",
    "        coverage=coverage+1\n",
    "coverage=coverage/mark\n",
    "\n",
    "print(\"n-p:\",len(train_x)-f0.shape[0],\" mark:\",mark) \n",
    "print(\"length\",np.mean(length))\n",
    "print(\"95 coverage\",coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14448/14448 [00:49<00:00, 294.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14448, 345])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6192/6192 [00:04<00:00, 1348.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-p: 14103  mark: 6192\n",
      "length 2.990294263663804\n",
      "95 coverage 0.9478359173126615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "par=optimizer.param_groups[0]['params']\n",
    "\n",
    "for i in tqdm(range(len(train_x))):\n",
    "    x0=torch.from_numpy(train_x[i:1+i].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    fi=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()   \n",
    "    for j in range(len(par)): \n",
    "        fi=torch.cat([fi,par[j].grad.reshape(-1)])\n",
    "    fi=fi.reshape(1,-1)\n",
    "    if i==0:\n",
    "        Fi=fi\n",
    "    else:\n",
    "        Fi=torch.cat([Fi,fi])   \n",
    "print(Fi.shape)\n",
    "\n",
    "temp=torch.linalg.pinv(Fi.T @ Fi)\n",
    "\n",
    "length=[]\n",
    "coverage=0\n",
    "mark=0\n",
    "for i in tqdm(range(len(test_x))):\n",
    "    x0=torch.from_numpy(test_x[i:i+1].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    #print(pred.detach().numpy()[0][0],test_y[i])\n",
    "    par=optimizer.param_groups[0]['params']\n",
    "    f0=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()\n",
    "    for j in range(len(par)):\n",
    "        f0=torch.cat([f0,par[j].grad.reshape(-1)])\n",
    "    f0=f0.reshape(-1,1)\n",
    "\n",
    "    fFFf=f0.T @ temp @ f0\n",
    "    #print(2*1.96*np.sqrt(float(fFFf+1))) #approximate with df(infinity)\n",
    "    \n",
    "    if fFFf < 0:\n",
    "        continue\n",
    "    mark=mark+1\n",
    "    length.append(2*1.96*np.sqrt(float(fFFf+1))*np.sqrt(reskernel_trainloss[-1]))\n",
    "    \n",
    "    #coverage\n",
    "    if pred.detach().numpy()[0][0]-1.96*np.sqrt(float(fFFf+1))*np.sqrt(reskernel_trainloss[-1])<test_y[i] and pred.detach().numpy()[0][0]+1.96*np.sqrt(float(fFFf+1))*np.sqrt(reskernel_trainloss[-1])>test_y[i]:\n",
    "        coverage=coverage+1\n",
    "coverage=coverage/mark\n",
    "\n",
    "print(\"n-p:\",len(train_x)-f0.shape[0],\" mark:\",mark) \n",
    "print(\"length\",np.mean(length))\n",
    "print(\"95 coverage\",coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14448/14448 [00:53<00:00, 271.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14448, 345])\n",
      "14376.837501525879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6192/6192 [00:04<00:00, 1487.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-p: 14103  mark: 855\n",
      "length 2.998809057723576\n",
      "95 coverage 0.9625730994152046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#penalty\n",
    "#predict\n",
    "wei=1e-4\n",
    "par=optimizer.param_groups[0]['params']\n",
    "\n",
    "for i in tqdm(range(len(train_x))):\n",
    "    x0=torch.from_numpy(train_x[i:1+i].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    fi=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()   \n",
    "    for j in range(len(par)): \n",
    "        fi=torch.cat([fi,par[j].grad.reshape(-1)])\n",
    "    fi=fi.reshape(1,-1)\n",
    "    if i==0:\n",
    "        Fi=fi\n",
    "    else:\n",
    "        Fi=torch.cat([Fi,fi])   \n",
    "print(Fi.shape)\n",
    "\n",
    "\n",
    "temp2=torch.linalg.inv(Fi.T @ Fi+wei *np.eye(Fi.T.shape[0]))\n",
    "temp2=temp2.float()\n",
    "temp=temp2@Fi.T @ Fi@temp2\n",
    "p=Fi @temp2 @Fi.T\n",
    "aa=len(train_x)-np.trace(2*p-p@p)\n",
    "print(aa)\n",
    "corr=(len(train_x)-f0.shape[0])/aa\n",
    "\n",
    "length=[]\n",
    "coverage=0\n",
    "mark=0\n",
    "for i in tqdm(range(len(test_x))):\n",
    "    x0=torch.from_numpy(test_x[i:i+1].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    #print(pred.detach().numpy()[0][0],test_y[i])\n",
    "    par=optimizer.param_groups[0]['params']\n",
    "    f0=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()\n",
    "    for j in range(len(par)):\n",
    "        f0=torch.cat([f0,par[j].grad.reshape(-1)])\n",
    "    f0=f0.reshape(-1,1)\n",
    "\n",
    "    fFFf=f0.T @ temp @ f0\n",
    "    #print(2*1.96*np.sqrt(float(fFFf+1))) #approximate with df(infinity)\n",
    "    \n",
    "    if fFFf < 0:\n",
    "        continue\n",
    "    mark=mark+1\n",
    "    dd=1.96*np.sqrt(float(fFFf+1))*np.sqrt(reskernel_trainloss[-1])*corr\n",
    "    length.append(2*dd)\n",
    "    \n",
    "    #coverage\n",
    "    if pred.detach().numpy()[0][0]-dd<test_y[i] and pred.detach().numpy()[0][0]+dd>test_y[i]:\n",
    "        coverage=coverage+1\n",
    "coverage=coverage/mark\n",
    "\n",
    "print(\"n-p:\",len(train_x)-f0.shape[0],\" mark:\",mark) \n",
    "print(\"length\",np.mean(length))\n",
    "print(\"95 coverage\",coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14448/14448 [00:51<00:00, 278.62it/s]\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 8047/14448 [00:06<00:05, 1225.46it/s]C:\\Users\\64982\\AppData\\Local\\Temp\\ipykernel_11812\\986494917.py:36: RuntimeWarning: invalid value encountered in sqrt\n",
      "  score=torch.cat([score,np.abs(pred.detach().numpy()[0][0]-train_y[i])/np.sqrt(reskernel_trainloss[-1])/np.sqrt(fFFf+1)])\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14448/14448 [00:12<00:00, 1168.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13727.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6192/6192 [00:04<00:00, 1347.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length 3.129474967633794\n",
      "95 coverage 0.9524512091396194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##conformal prediction\n",
    "#predict\n",
    "par=optimizer.param_groups[0]['params']\n",
    "for i in tqdm(range(len(train_x))):\n",
    "    x0=torch.from_numpy(train_x[i:1+i].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    fi=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()   \n",
    "    for j in range(len(par)): \n",
    "        fi=torch.cat([fi,par[j].grad.reshape(-1)])\n",
    "    fi=fi.reshape(1,-1)\n",
    "    if i==0:\n",
    "        Fi=fi\n",
    "    else:\n",
    "        Fi=torch.cat([Fi,fi])   \n",
    "temp=torch.linalg.inv(Fi.T @ Fi)\n",
    "\n",
    "mark=0\n",
    "score=torch.tensor([])\n",
    "for i in tqdm(range(len(train_x))):\n",
    "    x0=torch.from_numpy(train_x[i:i+1].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    par=optimizer.param_groups[0]['params']\n",
    "    f0=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()\n",
    "    for j in range(len(par)):\n",
    "        f0=torch.cat([f0,par[j].grad.reshape(-1)])\n",
    "    f0=f0.reshape(-1,1)\n",
    "    fFFf=f0.T @ temp @ f0\n",
    "    score=torch.cat([score,np.abs(pred.detach().numpy()[0][0]-train_y[i])/np.sqrt(reskernel_trainloss[-1])/np.sqrt(fFFf+1)])\n",
    "    if fFFf < 0:\n",
    "        continue\n",
    "    mark=mark+1\n",
    "score=score.reshape(-1)\n",
    "sorted_score, sorted_indices=torch.sort(score)\n",
    "q=(len(train_x)+1)*0.95\n",
    "print(np.ceil(q))\n",
    "a=sorted_score[int(np.ceil(q))]\n",
    "\n",
    "\n",
    "\n",
    "mark=0\n",
    "length=[]\n",
    "for i in tqdm(range(len(test_x))):\n",
    "    x0=torch.from_numpy(test_x[i:i+1].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    par=optimizer.param_groups[0]['params']\n",
    "    f0=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()\n",
    "    for j in range(len(par)):\n",
    "        f0=torch.cat([f0,par[j].grad.reshape(-1)])\n",
    "    f0=f0.reshape(-1,1)\n",
    "\n",
    "    fFFf=f0.T @ temp @ f0\n",
    "    \n",
    "    if fFFf < 0:\n",
    "        continue\n",
    "    mark=mark+1\n",
    "    dd=(np.sqrt(reskernel_trainloss[-1])*np.sqrt(fFFf+1)*a).detach().numpy()[0][0]\n",
    "    length.append(2*dd)\n",
    "    \n",
    "    #coverage\n",
    "    if pred.detach().numpy()[0][0]-dd<test_y[i] and pred.detach().numpy()[0][0]+dd>test_y[i]:\n",
    "        coverage=coverage+1\n",
    "coverage=coverage/mark\n",
    "\n",
    "\n",
    "print(\"length\",np.mean(length))\n",
    "print(\"95 coverage\",coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13727.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length tensor(3.0983, dtype=torch.float64)\n",
      "95 coverage 0.8292958656330749\n"
     ]
    }
   ],
   "source": [
    "##conformal prediction\n",
    "#predict\n",
    "x0=torch.from_numpy(train_x[:].to_numpy()).float()\n",
    "with torch.no_grad():\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    score=np.abs(pred.reshape(-1)-train_y[:].to_numpy())\n",
    "sorted_score, sorted_indices=torch.sort(score)\n",
    "q=(len(train_x)+1)*0.95\n",
    "print(np.ceil(q))\n",
    "a=sorted_score[int(np.ceil(q))]\n",
    "\n",
    "coverage=0\n",
    "for i in range(len(test_x)):\n",
    "    if pred.detach().numpy()[0][0]-a<test_y[i] and pred.detach().numpy()[0][0]+a>test_y[i]:\n",
    "        coverage=coverage+1\n",
    "coverage=coverage/len(test_x)\n",
    "\n",
    "print(\"length\",2*a)\n",
    "print(\"95 coverage\",coverage)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
