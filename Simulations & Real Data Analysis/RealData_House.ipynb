{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: California Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.503149</td>\n",
       "      <td>-1.083767</td>\n",
       "      <td>-0.462001</td>\n",
       "      <td>0.018689</td>\n",
       "      <td>1.535208</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>-0.689106</td>\n",
       "      <td>0.648722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.364659</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>-0.418034</td>\n",
       "      <td>-0.088311</td>\n",
       "      <td>0.205330</td>\n",
       "      <td>0.054431</td>\n",
       "      <td>-0.857653</td>\n",
       "      <td>0.653714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.715648</td>\n",
       "      <td>1.617807</td>\n",
       "      <td>-0.219152</td>\n",
       "      <td>-0.279204</td>\n",
       "      <td>-0.119633</td>\n",
       "      <td>-0.035265</td>\n",
       "      <td>0.523497</td>\n",
       "      <td>-0.089991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.700173</td>\n",
       "      <td>-1.083767</td>\n",
       "      <td>0.410511</td>\n",
       "      <td>0.687172</td>\n",
       "      <td>1.008909</td>\n",
       "      <td>-0.065649</td>\n",
       "      <td>-0.735924</td>\n",
       "      <td>1.502236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.333002</td>\n",
       "      <td>-1.719432</td>\n",
       "      <td>-0.055459</td>\n",
       "      <td>0.009491</td>\n",
       "      <td>0.276858</td>\n",
       "      <td>-0.050146</td>\n",
       "      <td>0.762273</td>\n",
       "      <td>-1.128183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14443</th>\n",
       "      <td>-1.099272</td>\n",
       "      <td>-0.924851</td>\n",
       "      <td>-0.555332</td>\n",
       "      <td>-0.020087</td>\n",
       "      <td>-0.305957</td>\n",
       "      <td>-0.057375</td>\n",
       "      <td>-0.876380</td>\n",
       "      <td>0.843383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14444</th>\n",
       "      <td>2.468780</td>\n",
       "      <td>1.061601</td>\n",
       "      <td>0.549474</td>\n",
       "      <td>-0.179651</td>\n",
       "      <td>-0.593833</td>\n",
       "      <td>-0.016802</td>\n",
       "      <td>-0.567377</td>\n",
       "      <td>-0.030095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14445</th>\n",
       "      <td>-1.441260</td>\n",
       "      <td>1.061601</td>\n",
       "      <td>-0.217538</td>\n",
       "      <td>-0.189596</td>\n",
       "      <td>-0.574406</td>\n",
       "      <td>-0.040979</td>\n",
       "      <td>-0.796789</td>\n",
       "      <td>0.653714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14446</th>\n",
       "      <td>-0.651904</td>\n",
       "      <td>-1.481058</td>\n",
       "      <td>-0.176785</td>\n",
       "      <td>-0.305747</td>\n",
       "      <td>-0.090492</td>\n",
       "      <td>-0.028604</td>\n",
       "      <td>-0.890426</td>\n",
       "      <td>1.202758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14447</th>\n",
       "      <td>-0.824872</td>\n",
       "      <td>0.505394</td>\n",
       "      <td>-0.326778</td>\n",
       "      <td>0.169335</td>\n",
       "      <td>-0.367771</td>\n",
       "      <td>-0.046552</td>\n",
       "      <td>1.010411</td>\n",
       "      <td>-1.312861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14448 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0     -0.503149 -1.083767 -0.462001  0.018689  1.535208 -0.036385 -0.689106   \n",
       "1     -0.364659  0.982143 -0.418034 -0.088311  0.205330  0.054431 -0.857653   \n",
       "2     -0.715648  1.617807 -0.219152 -0.279204 -0.119633 -0.035265  0.523497   \n",
       "3     -0.700173 -1.083767  0.410511  0.687172  1.008909 -0.065649 -0.735924   \n",
       "4      0.333002 -1.719432 -0.055459  0.009491  0.276858 -0.050146  0.762273   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "14443 -1.099272 -0.924851 -0.555332 -0.020087 -0.305957 -0.057375 -0.876380   \n",
       "14444  2.468780  1.061601  0.549474 -0.179651 -0.593833 -0.016802 -0.567377   \n",
       "14445 -1.441260  1.061601 -0.217538 -0.189596 -0.574406 -0.040979 -0.796789   \n",
       "14446 -0.651904 -1.481058 -0.176785 -0.305747 -0.090492 -0.028604 -0.890426   \n",
       "14447 -0.824872  0.505394 -0.326778  0.169335 -0.367771 -0.046552  1.010411   \n",
       "\n",
       "              7  \n",
       "0      0.648722  \n",
       "1      0.653714  \n",
       "2     -0.089991  \n",
       "3      1.502236  \n",
       "4     -1.128183  \n",
       "...         ...  \n",
       "14443  0.843383  \n",
       "14444 -0.030095  \n",
       "14445  0.653714  \n",
       "14446  1.202758  \n",
       "14447 -1.312861  \n",
       "\n",
       "[14448 rows x 8 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge,RidgeCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import resample\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing  \n",
    "\n",
    "housing = fetch_california_housing() \n",
    "x = housing.data\n",
    "y = housing.target.reshape(-1,1)\n",
    "x=preprocessing.StandardScaler().fit(x).transform(x) #normalize\n",
    "x=pd.DataFrame(x)\n",
    "y=pd.DataFrame(y)\n",
    "data=pd.concat([y,x],axis=1)\n",
    "train,test=train_test_split(data,test_size=0.3, random_state=1)\n",
    "\n",
    "train_y = train.iloc[:,0]\n",
    "train_x = train.iloc[:,1:]\n",
    "test_y = test.iloc[:,0]\n",
    "test_x = test.iloc[:,1:]\n",
    "\n",
    "train_x.reset_index(drop=True, inplace=True) \n",
    "test_x.reset_index(drop=True, inplace=True) \n",
    "train_y.reset_index(drop=True, inplace=True) \n",
    "test_y.reset_index(drop=True, inplace=True) \n",
    "\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nntrain_x = torch.from_numpy(train_x.to_numpy()).float()\n",
    "nntrain_y = torch.squeeze(torch.from_numpy(train_y.to_numpy()).float()) \n",
    "nntest_x= torch.from_numpy(test_x.to_numpy()).float()\n",
    "nntest_y = torch.squeeze(torch.from_numpy(test_y.to_numpy()).float())\n",
    "\n",
    "class mydataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "        self._len = len(x)\n",
    "\n",
    "    def __getitem__(self, item): \n",
    "        return self._x[item], self._y[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=8, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0\n",
      "            Train set - loss: 14.096611022949219\n",
      "            Test  set - loss: 0.6995125412940979\n",
      "            \n",
      "epoch 10\n",
      "            Train set - loss: 0.4128504693508148\n",
      "            Test  set - loss: 0.4473537802696228\n",
      "            \n",
      "epoch 20\n",
      "            Train set - loss: 0.32872501015663147\n",
      "            Test  set - loss: 0.42410022020339966\n",
      "            \n",
      "epoch 30\n",
      "            Train set - loss: 0.6147395968437195\n",
      "            Test  set - loss: 0.4103561043739319\n",
      "            \n",
      "epoch 40\n",
      "            Train set - loss: 0.2830987870693207\n",
      "            Test  set - loss: 0.38816186785697937\n",
      "            \n",
      "epoch 50\n",
      "            Train set - loss: 0.35466766357421875\n",
      "            Test  set - loss: 0.37112322449684143\n",
      "            \n",
      "epoch 60\n",
      "            Train set - loss: 0.24381917715072632\n",
      "            Test  set - loss: 0.38009369373321533\n",
      "            \n",
      "epoch 70\n",
      "            Train set - loss: 0.2532535195350647\n",
      "            Test  set - loss: 0.3609306812286377\n",
      "            \n",
      "epoch 80\n",
      "            Train set - loss: 0.2816902697086334\n",
      "            Test  set - loss: 0.35379481315612793\n",
      "            \n",
      "epoch 90\n",
      "            Train set - loss: 0.6332873702049255\n",
      "            Test  set - loss: 0.3500942289829254\n",
      "            \n",
      "epoch 100\n",
      "            Train set - loss: 0.3473266661167145\n",
      "            Test  set - loss: 0.3418392539024353\n",
      "            \n",
      "epoch 110\n",
      "            Train set - loss: 0.5649215579032898\n",
      "            Test  set - loss: 0.32734090089797974\n",
      "            \n",
      "epoch 120\n",
      "            Train set - loss: 0.26605334877967834\n",
      "            Test  set - loss: 0.34143996238708496\n",
      "            \n",
      "epoch 130\n",
      "            Train set - loss: 0.26845425367355347\n",
      "            Test  set - loss: 0.3403781056404114\n",
      "            \n",
      "epoch 140\n",
      "            Train set - loss: 0.20904457569122314\n",
      "            Test  set - loss: 0.33077481389045715\n",
      "            \n",
      "epoch 150\n",
      "            Train set - loss: 0.4294116199016571\n",
      "            Test  set - loss: 0.3491103947162628\n",
      "            \n",
      "epoch 160\n",
      "            Train set - loss: 0.1623660773038864\n",
      "            Test  set - loss: 0.3446206748485565\n",
      "            \n",
      "epoch 170\n",
      "            Train set - loss: 0.3521476686000824\n",
      "            Test  set - loss: 0.3349972069263458\n",
      "            \n",
      "epoch 180\n",
      "            Train set - loss: 0.3629431426525116\n",
      "            Test  set - loss: 0.33261746168136597\n",
      "            \n",
      "epoch 190\n",
      "            Train set - loss: 0.4458404779434204\n",
      "            Test  set - loss: 0.3453120291233063\n",
      "            \n",
      "epoch 200\n",
      "            Train set - loss: 0.3722724914550781\n",
      "            Test  set - loss: 0.33611252903938293\n",
      "            \n",
      "epoch 210\n",
      "            Train set - loss: 0.29438549280166626\n",
      "            Test  set - loss: 0.3395470678806305\n",
      "            \n",
      "epoch 220\n",
      "            Train set - loss: 0.5364126563072205\n",
      "            Test  set - loss: 0.334748774766922\n",
      "            \n",
      "epoch 230\n",
      "            Train set - loss: 0.35194334387779236\n",
      "            Test  set - loss: 0.33586084842681885\n",
      "            \n",
      "epoch 240\n",
      "            Train set - loss: 0.21335835754871368\n",
      "            Test  set - loss: 0.33270639181137085\n",
      "            \n",
      "epoch 250\n",
      "            Train set - loss: 0.32901957631111145\n",
      "            Test  set - loss: 0.3330092132091522\n",
      "            \n",
      "epoch 260\n",
      "            Train set - loss: 0.4343348443508148\n",
      "            Test  set - loss: 0.3373540937900543\n",
      "            \n",
      "epoch 270\n",
      "            Train set - loss: 0.2614939212799072\n",
      "            Test  set - loss: 0.33431586623191833\n",
      "            \n",
      "epoch 280\n",
      "            Train set - loss: 0.4290355443954468\n",
      "            Test  set - loss: 0.33169251680374146\n",
      "            \n",
      "epoch 290\n",
      "            Train set - loss: 0.5440158247947693\n",
      "            Test  set - loss: 0.32456135749816895\n",
      "            \n",
      "DNN complexity and model fitted in 86.111 s\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(mydataset(nntrain_x, nntrain_y),batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(mydataset(nntest_x, nntest_y),batch_size=100, shuffle=False)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1 = nn.Linear(8, 32)\n",
    "        self.fc2 = nn.Linear(32, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "#initialize\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.normal_(m.weight,mean=0,std=0.5)\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.uniform_(m.weight,a=0,b=0.1)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "torch.manual_seed(0)\n",
    "net.apply(init_weights)\n",
    "print(net)\n",
    "criterion=nn.MSELoss() \n",
    "optimizer=optim.SGD([{'params': net.parameters(), 'initial_lr': 5e-4}],lr=5e-4,momentum=0.9,weight_decay=1e-2) #optim.Adam(...)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=100,gamma=0.5,last_epoch=300) \n",
    "dnn_trainloss=[]\n",
    "dnn_testloss=[]\n",
    "t0=time.time()\n",
    "for epoch in range(500): \n",
    "    for x, y in train_loader: #for batch, (x, y) in enumerate(train_loader): \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        y_pred = net(x)\n",
    "        y_pred = torch.squeeze(y_pred)\n",
    "        train_loss = criterion(y_pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_test_pred = net(x)\n",
    "        y_test_pred = torch.squeeze(y_test_pred)\n",
    "    \n",
    "        test_loss = criterion(y_test_pred,y)\n",
    "    \n",
    "    if epoch>50 and float(test_loss)>max(dnn_testloss[-50:-1]):\n",
    "        break\n",
    "    \n",
    "    if epoch % 10 == 0:        \n",
    "        print(f'''epoch {epoch}\n",
    "            Train set - loss: {train_loss}\n",
    "            Test  set - loss: {test_loss}\n",
    "            ''')\n",
    "    \n",
    "    dnn_trainloss.append(float(train_loss))\n",
    "    dnn_testloss.append(float(test_loss))\n",
    "            \n",
    "dnn_fit = time.time() - t0\n",
    "print(\"DNN complexity and model fitted in %.3f s\" % dnn_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.6611528 0.7628807 2.4197874 ... 1.579648  1.411746  1.9770093]\n",
      "[3.55  0.707 2.294 ... 1.098 1.625 1.667]\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "x0=torch.from_numpy(test_x[:].to_numpy()).float()\n",
    "with torch.no_grad():\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    print(np.array(pred).reshape(-1))\n",
    "    print(test_y[:].to_numpy())\n",
    "    bootbase=np.array(pred.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14448/14448 [01:20<00:00, 180.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14448, 561])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6192/6192 [00:04<00:00, 1454.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-p: 13887  mark: 6192\n",
      "length 2.284675678485441\n",
      "95 coverage 0.9326550387596899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "par=optimizer.param_groups[0]['params']\n",
    "\n",
    "for i in tqdm(range(len(train_x))):\n",
    "    x0=torch.from_numpy(train_x[i:1+i].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    fi=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()   \n",
    "    for j in range(len(par)): \n",
    "        fi=torch.cat([fi,par[j].grad.reshape(-1)])\n",
    "    fi=fi.reshape(1,-1)\n",
    "    if i==0:\n",
    "        Fi=fi\n",
    "    else:\n",
    "        Fi=torch.cat([Fi,fi])   \n",
    "print(Fi.shape)\n",
    "\n",
    "temp=torch.linalg.pinv(Fi.T @ Fi)\n",
    "\n",
    "length=[]\n",
    "coverage=0\n",
    "mark=0\n",
    "for i in tqdm(range(len(test_x))):\n",
    "    x0=torch.from_numpy(test_x[i:i+1].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    #print(pred.detach().numpy()[0][0],test_y[i])\n",
    "    par=optimizer.param_groups[0]['params']\n",
    "    f0=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()\n",
    "    for j in range(len(par)):\n",
    "        f0=torch.cat([f0,par[j].grad.reshape(-1)])\n",
    "    f0=f0.reshape(-1,1)\n",
    "\n",
    "    fFFf=f0.T @ temp @ f0\n",
    "    #print(2*1.96*np.sqrt(float(fFFf+1))) #approximate with df(infinity)\n",
    "    \n",
    "    if fFFf < 0:\n",
    "        continue\n",
    "    mark=mark+1\n",
    "    length.append(2*1.96*np.sqrt(float(fFFf+1))*np.sqrt(dnn_testloss[-1]))\n",
    "    \n",
    "    #coverage\n",
    "    if pred.detach().numpy()[0][0]-1.96*np.sqrt(float(fFFf+1))*np.sqrt(dnn_testloss[-1])<test_y[i] and pred.detach().numpy()[0][0]+1.96*np.sqrt(float(fFFf+1))*np.sqrt(dnn_testloss[-1])>test_y[i]:\n",
    "        coverage=coverage+1\n",
    "coverage=coverage/mark\n",
    "\n",
    "print(\"n-p:\",len(train_x)-f0.shape[0],\" mark:\",mark) \n",
    "print(\"length\",np.mean(length))\n",
    "print(\"95 coverage\",coverage)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (rblock1): ResidualBlock(\n",
      "    (fc1): Linear(in_features=8, out_features=32, bias=True)\n",
      "    (fc2): Linear(in_features=8, out_features=32, bias=True)\n",
      "  )\n",
      "  (rblock2): ResidualBlock(\n",
      "    (fc1): Linear(in_features=32, out_features=8, bias=True)\n",
      "    (fc2): Linear(in_features=32, out_features=8, bias=True)\n",
      "  )\n",
      "  (fc3): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0\n",
      "            Train set - loss: 0.5712760090827942\n",
      "            Test  set - loss: 0.5936006307601929\n",
      "            \n",
      "epoch 10\n",
      "            Train set - loss: 0.31345823407173157\n",
      "            Test  set - loss: 0.4415057599544525\n",
      "            \n",
      "epoch 20\n",
      "            Train set - loss: 0.3499375879764557\n",
      "            Test  set - loss: 0.4140929877758026\n",
      "            \n",
      "epoch 30\n",
      "            Train set - loss: 0.9471092224121094\n",
      "            Test  set - loss: 0.40349408984184265\n",
      "            \n",
      "epoch 40\n",
      "            Train set - loss: 0.38021886348724365\n",
      "            Test  set - loss: 0.36472147703170776\n",
      "            \n",
      "epoch 50\n",
      "            Train set - loss: 0.31228506565093994\n",
      "            Test  set - loss: 0.3572384715080261\n",
      "            \n",
      "epoch 60\n",
      "            Train set - loss: 0.20291508734226227\n",
      "            Test  set - loss: 0.34148889780044556\n",
      "            \n",
      "epoch 70\n",
      "            Train set - loss: 0.5432549715042114\n",
      "            Test  set - loss: 0.3259957432746887\n",
      "            \n",
      "epoch 80\n",
      "            Train set - loss: 0.5266123414039612\n",
      "            Test  set - loss: 0.33181700110435486\n",
      "            \n",
      "epoch 90\n",
      "            Train set - loss: 0.5554761290550232\n",
      "            Test  set - loss: 0.32128432393074036\n",
      "            \n",
      "epoch 100\n",
      "            Train set - loss: 0.4695252478122711\n",
      "            Test  set - loss: 0.30743589997291565\n",
      "            \n",
      "epoch 110\n",
      "            Train set - loss: 0.3801424205303192\n",
      "            Test  set - loss: 0.3012714684009552\n",
      "            \n",
      "epoch 120\n",
      "            Train set - loss: 0.6687572598457336\n",
      "            Test  set - loss: 0.30853796005249023\n",
      "            \n",
      "epoch 130\n",
      "            Train set - loss: 0.19053566455841064\n",
      "            Test  set - loss: 0.3051826059818268\n",
      "            \n",
      "epoch 140\n",
      "            Train set - loss: 0.4239228069782257\n",
      "            Test  set - loss: 0.29400867223739624\n",
      "            \n",
      "epoch 150\n",
      "            Train set - loss: 0.3068535625934601\n",
      "            Test  set - loss: 0.2982334792613983\n",
      "            \n",
      "epoch 160\n",
      "            Train set - loss: 0.23332037031650543\n",
      "            Test  set - loss: 0.2977282404899597\n",
      "            \n",
      "epoch 170\n",
      "            Train set - loss: 0.26671960949897766\n",
      "            Test  set - loss: 0.2983308732509613\n",
      "            \n",
      "epoch 180\n",
      "            Train set - loss: 0.3203788697719574\n",
      "            Test  set - loss: 0.2981795370578766\n",
      "            \n",
      "epoch 190\n",
      "            Train set - loss: 0.39421096444129944\n",
      "            Test  set - loss: 0.2945088744163513\n",
      "            \n",
      "epoch 200\n",
      "            Train set - loss: 0.23927634954452515\n",
      "            Test  set - loss: 0.2943491041660309\n",
      "            \n",
      "epoch 210\n",
      "            Train set - loss: 0.27173149585723877\n",
      "            Test  set - loss: 0.2955836057662964\n",
      "            \n",
      "epoch 220\n",
      "            Train set - loss: 0.49347543716430664\n",
      "            Test  set - loss: 0.292876273393631\n",
      "            \n",
      "epoch 230\n",
      "            Train set - loss: 0.46606793999671936\n",
      "            Test  set - loss: 0.2943406105041504\n",
      "            \n",
      "epoch 240\n",
      "            Train set - loss: 0.33218783140182495\n",
      "            Test  set - loss: 0.2924155592918396\n",
      "            \n",
      "epoch 250\n",
      "            Train set - loss: 0.2785899341106415\n",
      "            Test  set - loss: 0.2940584719181061\n",
      "            \n",
      "epoch 260\n",
      "            Train set - loss: 0.34034696221351624\n",
      "            Test  set - loss: 0.2917174994945526\n",
      "            \n",
      "epoch 270\n",
      "            Train set - loss: 0.24116398394107819\n",
      "            Test  set - loss: 0.29294490814208984\n",
      "            \n",
      "epoch 280\n",
      "            Train set - loss: 0.3255390226840973\n",
      "            Test  set - loss: 0.29170653223991394\n",
      "            \n",
      "epoch 290\n",
      "            Train set - loss: 0.2127106636762619\n",
      "            Test  set - loss: 0.292572557926178\n",
      "            \n",
      "epoch 300\n",
      "            Train set - loss: 0.41069385409355164\n",
      "            Test  set - loss: 0.29268601536750793\n",
      "            \n",
      "epoch 310\n",
      "            Train set - loss: 0.24053102731704712\n",
      "            Test  set - loss: 0.29290342330932617\n",
      "            \n",
      "epoch 320\n",
      "            Train set - loss: 0.32889580726623535\n",
      "            Test  set - loss: 0.29197463393211365\n",
      "            \n",
      "epoch 330\n",
      "            Train set - loss: 0.27228590846061707\n",
      "            Test  set - loss: 0.2919963598251343\n",
      "            \n",
      "epoch 340\n",
      "            Train set - loss: 0.23435766994953156\n",
      "            Test  set - loss: 0.2916329503059387\n",
      "            \n",
      "epoch 350\n",
      "            Train set - loss: 0.3912099599838257\n",
      "            Test  set - loss: 0.29222163558006287\n",
      "            \n",
      "epoch 360\n",
      "            Train set - loss: 0.45250895619392395\n",
      "            Test  set - loss: 0.29189711809158325\n",
      "            \n",
      "epoch 370\n",
      "            Train set - loss: 0.32885900139808655\n",
      "            Test  set - loss: 0.2917781472206116\n",
      "            \n",
      "epoch 380\n",
      "            Train set - loss: 0.4908333718776703\n",
      "            Test  set - loss: 0.2918611466884613\n",
      "            \n",
      "epoch 390\n",
      "            Train set - loss: 0.3758639395236969\n",
      "            Test  set - loss: 0.2919352054595947\n",
      "            \n",
      "epoch 400\n",
      "            Train set - loss: 0.29181209206581116\n",
      "            Test  set - loss: 0.29157158732414246\n",
      "            \n",
      "epoch 410\n",
      "            Train set - loss: 0.37180933356285095\n",
      "            Test  set - loss: 0.2916111648082733\n",
      "            \n",
      "epoch 420\n",
      "            Train set - loss: 0.607815146446228\n",
      "            Test  set - loss: 0.29155340790748596\n",
      "            \n",
      "epoch 430\n",
      "            Train set - loss: 0.3160138428211212\n",
      "            Test  set - loss: 0.2914867699146271\n",
      "            \n",
      "epoch 440\n",
      "            Train set - loss: 0.17528070509433746\n",
      "            Test  set - loss: 0.2914986312389374\n",
      "            \n",
      "epoch 450\n",
      "            Train set - loss: 0.49370285868644714\n",
      "            Test  set - loss: 0.2913651764392853\n",
      "            \n",
      "epoch 460\n",
      "            Train set - loss: 0.1697753667831421\n",
      "            Test  set - loss: 0.29137930274009705\n",
      "            \n",
      "epoch 470\n",
      "            Train set - loss: 0.4297865331172943\n",
      "            Test  set - loss: 0.29143026471138\n",
      "            \n",
      "epoch 480\n",
      "            Train set - loss: 0.2163267731666565\n",
      "            Test  set - loss: 0.29143401980400085\n",
      "            \n",
      "epoch 490\n",
      "            Train set - loss: 0.24870820343494415\n",
      "            Test  set - loss: 0.2914661765098572\n",
      "            \n",
      "ResNet complexity and model fitted in 185.520 s\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(mydataset(nntrain_x, nntrain_y),batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(mydataset(nntest_x, nntest_y),batch_size=100, shuffle=False)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,infeatures,outfeatures):\n",
    "        super(ResidualBlock,self).__init__()\n",
    "        self.infeatures = infeatures\n",
    "        self.outfeatures = outfeatures\n",
    "        self.fc1 = nn.Linear(infeatures,outfeatures)\n",
    "        self.fc2 = nn.Linear(infeatures,outfeatures)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.fc1(x)\n",
    "        y= F.relu(y)\n",
    "        x = self.fc2(x)\n",
    "        return F.relu(x+y)\n",
    "\n",
    "\n",
    "class ResNet(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.rblock1 = ResidualBlock(8,32)\n",
    "        self.rblock2 = ResidualBlock(32,8)\n",
    "        self.fc3 = nn.Linear(8,1)\n",
    "    \n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.rblock1(x)\n",
    "        x = self.rblock2(x)\n",
    "        return self.fc3(x)\n",
    "#initialize\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.normal_(m.weight,mean=0,std=0.5)\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.uniform_(m.weight,a=0,b=0.1)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = ResNet()\n",
    "net = net.to(device)\n",
    "torch.manual_seed(0)\n",
    "net.apply(init_weights)\n",
    "print(net)\n",
    "criterion=nn.MSELoss() \n",
    "optimizer=optim.SGD([{'params': net.parameters(), 'initial_lr': 5e-4}],lr=5e-4,momentum=0.9,weight_decay=1e-2) #optim.Adam(...)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=50,gamma=0.5,last_epoch=300) \n",
    "res_trainloss=[]\n",
    "res_testloss=[]\n",
    "t0=time.time()\n",
    "for epoch in range(500):\n",
    "    for x, y in train_loader: #for batch, (x, y) in enumerate(train_loader): \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        y_pred = net(x)\n",
    "        y_pred = torch.squeeze(y_pred)\n",
    "        train_loss = criterion(y_pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_test_pred = net(x)\n",
    "        y_test_pred = torch.squeeze(y_test_pred)\n",
    "    \n",
    "        test_loss = criterion(y_test_pred,y)\n",
    "    \n",
    "    if epoch>50 and float(test_loss)>max(res_testloss[-50:-1]):\n",
    "        break\n",
    "    \n",
    "    if epoch % 10 == 0:        \n",
    "        print(f'''epoch {epoch}\n",
    "            Train set - loss: {train_loss}\n",
    "            Test  set - loss: {test_loss}\n",
    "            ''')\n",
    "    \n",
    "    res_trainloss.append(float(train_loss))\n",
    "    res_testloss.append(float(test_loss))\n",
    "            \n",
    "dnn_fit = time.time() - t0\n",
    "print(\"ResNet complexity and model fitted in %.3f s\" % dnn_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.837086   0.72108847 2.2756755  ... 1.426889   1.3431073  2.0315123 ]\n",
      "[3.55  0.707 2.294 ... 1.098 1.625 1.667]\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "x0=torch.from_numpy(test_x[:].to_numpy()).float()\n",
    "with torch.no_grad():\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    print(np.array(pred).reshape(-1))\n",
    "    print(test_y[:].to_numpy())\n",
    "    bootbase=np.array(pred.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14448/14448 [02:36<00:00, 92.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14448, 1113])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6192/6192 [00:09<00:00, 647.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-p: 13335  mark: 6192\n",
      "length 2.1347017366069867\n",
      "95 coverage 0.9297480620155039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "par=optimizer.param_groups[0]['params']\n",
    "\n",
    "for i in tqdm(range(len(train_x))):\n",
    "    x0=torch.from_numpy(train_x[i:1+i].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    fi=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()   \n",
    "    for j in range(len(par)): \n",
    "        fi=torch.cat([fi,par[j].grad.reshape(-1)])\n",
    "    fi=fi.reshape(1,-1)\n",
    "    if i==0:\n",
    "        Fi=fi\n",
    "    else:\n",
    "        Fi=torch.cat([Fi,fi])   \n",
    "print(Fi.shape)\n",
    "\n",
    "temp=torch.linalg.pinv(Fi.T @ Fi)\n",
    "\n",
    "length=[]\n",
    "coverage=0\n",
    "mark=0\n",
    "for i in tqdm(range(len(test_x))):\n",
    "    x0=torch.from_numpy(test_x[i:i+1].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    #print(pred.detach().numpy()[0][0],test_y[i])\n",
    "    par=optimizer.param_groups[0]['params']\n",
    "    f0=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()\n",
    "    for j in range(len(par)):\n",
    "        f0=torch.cat([f0,par[j].grad.reshape(-1)])\n",
    "    f0=f0.reshape(-1,1)\n",
    "\n",
    "    fFFf=f0.T @ temp @ f0\n",
    "    #print(2*1.96*np.sqrt(float(fFFf+1))) #approximate with df(infinity)\n",
    "    \n",
    "    if fFFf < 0:\n",
    "        continue\n",
    "    mark=mark+1\n",
    "    length.append(2*1.96*np.sqrt(float(fFFf+1))*np.sqrt(res_testloss[-1]))\n",
    "    \n",
    "    #coverage\n",
    "    if pred.detach().numpy()[0][0]-1.96*np.sqrt(float(fFFf+1))*np.sqrt(res_testloss[-1])<test_y[i] and pred.detach().numpy()[0][0]+1.96*np.sqrt(float(fFFf+1))*np.sqrt(res_testloss[-1])>test_y[i]:\n",
    "        coverage=coverage+1\n",
    "coverage=coverage/mark\n",
    "\n",
    "print(\"n-p:\",len(train_x)-f0.shape[0],\" mark:\",mark) \n",
    "print(\"length\",np.mean(length))\n",
    "print(\"95 coverage\",coverage)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "definition of random feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_1d(pdf, gamma):\n",
    "    if pdf=='G':\n",
    "        w=torch.randn(1)*gamma\n",
    "        return w\n",
    "    elif pdf=='L':\n",
    "        w=torch.distributions.laplace.Laplace(torch.tensor([0.0]), torch.tensor([1.0])).sample()*gamma\n",
    "        return w\n",
    "    elif pdf=='C':\n",
    "        w=torch.distributions.cauchy.Cauchy(torch.tensor([0.0]), torch.tensor([1.0])).sample()*gamma\n",
    "        return w\n",
    "    \n",
    "def sample(pdf, gamma, d):\n",
    "    return torch.tensor([sample_1d(pdf, gamma) for _ in range(d)])\n",
    "\n",
    "class RandomFourierFeature:\n",
    "    \"\"\"Random Fourier Feature\n",
    "    Parameters\n",
    "    ----------\n",
    "    d : int\n",
    "        Input space dimension\n",
    "    D : int\n",
    "        Feature space dimension\n",
    "    W : shape (D,d)\n",
    "    b : shape (D)\n",
    "    kernel : char\n",
    "        Kernel to use; 'G', 'L', or 'C'\n",
    "    gamma : float\n",
    "        pdf parameter\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d, D, W=None, b=None, kernel='G', gamma=1):\n",
    "\n",
    "        self.d = d\n",
    "        self.D = D\n",
    "        self.gamma = gamma\n",
    "\n",
    "        kernel = kernel.upper()\n",
    "        if kernel not in ['G', 'L', 'C']:\n",
    "            raise Exception('Invalid Kernel')\n",
    "        self.kernel = kernel\n",
    "\n",
    "        if W is None or b is None:\n",
    "            self.create()\n",
    "        else:\n",
    "            self.__load(W, b)\n",
    "\n",
    "    def __load(self, W, b):\n",
    "        \"\"\"Load from existing Arrays\"\"\"\n",
    "\n",
    "        self.W = W.reshape([self.D, self.d])\n",
    "        self.b = b\n",
    "    \n",
    "\n",
    "    def create(self):\n",
    "        \"\"\"Create a d->D fourier random feature\"\"\"\n",
    "\n",
    "        self.b = torch.rand(self.D)*2*torch.pi\n",
    "        self.W = sample(self.kernel, self.gamma, self.d*self.D).reshape(self.D,self.d)\n",
    "\n",
    "    def transform(self, x):\n",
    "        \"\"\"Transform a vector using this feature\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (shape=(n,d))\n",
    "            to transform; must be single dimension vector\n",
    "        Returns\n",
    "        -------\n",
    "        x : (shape=(n,D))\n",
    "            Feature space transformation of x\n",
    "        \"\"\"\n",
    "        #print(self.W.shape,self.b.reshape(-1,1).shape,x.shape)\n",
    "        #print((self.W @ x.T).shape)\n",
    "       \n",
    "        result=torch.sqrt(torch.tensor([2.0/self.D])) * torch.cos( self.W @ x.T  + (self.b.reshape(-1,1) @ torch.ones(len(x)).reshape(1,-1))) \n",
    "        #print(result.T.shape)\n",
    "        return result.T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. multilayer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KernelNet(\n",
      "  (fc1): Linear(in_features=32, out_features=8, bias=True)\n",
      "  (fc2): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0\n",
      "            Train set - loss: 1.5061477422714233\n",
      "            Test  set - loss: 1.0955873727798462\n",
      "            \n",
      "epoch 10\n",
      "            Train set - loss: 0.7542936205863953\n",
      "            Test  set - loss: 0.9550679326057434\n",
      "            \n",
      "epoch 20\n",
      "            Train set - loss: 0.6316452622413635\n",
      "            Test  set - loss: 0.6123295426368713\n",
      "            \n",
      "epoch 30\n",
      "            Train set - loss: 0.6232777237892151\n",
      "            Test  set - loss: 0.5516456365585327\n",
      "            \n",
      "epoch 40\n",
      "            Train set - loss: 0.7915704846382141\n",
      "            Test  set - loss: 0.4892044961452484\n",
      "            \n",
      "epoch 50\n",
      "            Train set - loss: 0.407598614692688\n",
      "            Test  set - loss: 0.4496401846408844\n",
      "            \n",
      "epoch 60\n",
      "            Train set - loss: 0.7176603674888611\n",
      "            Test  set - loss: 0.42994850873947144\n",
      "            \n",
      "epoch 70\n",
      "            Train set - loss: 0.75468510389328\n",
      "            Test  set - loss: 0.4140830636024475\n",
      "            \n",
      "epoch 80\n",
      "            Train set - loss: 0.6760627627372742\n",
      "            Test  set - loss: 0.41209524869918823\n",
      "            \n",
      "epoch 90\n",
      "            Train set - loss: 0.5197771191596985\n",
      "            Test  set - loss: 0.3832603991031647\n",
      "            \n",
      "epoch 100\n",
      "            Train set - loss: 0.27266648411750793\n",
      "            Test  set - loss: 0.37989604473114014\n",
      "            \n",
      "epoch 110\n",
      "            Train set - loss: 0.6768698692321777\n",
      "            Test  set - loss: 0.382974237203598\n",
      "            \n",
      "epoch 120\n",
      "            Train set - loss: 0.4600323438644409\n",
      "            Test  set - loss: 0.40426358580589294\n",
      "            \n",
      "epoch 130\n",
      "            Train set - loss: 0.26362696290016174\n",
      "            Test  set - loss: 0.3822185695171356\n",
      "            \n",
      "epoch 140\n",
      "            Train set - loss: 0.581943929195404\n",
      "            Test  set - loss: 0.37227270007133484\n",
      "            \n",
      "epoch 150\n",
      "            Train set - loss: 0.6478642821311951\n",
      "            Test  set - loss: 0.38438722491264343\n",
      "            \n",
      "epoch 160\n",
      "            Train set - loss: 0.22745706140995026\n",
      "            Test  set - loss: 0.3834931254386902\n",
      "            \n",
      "epoch 170\n",
      "            Train set - loss: 0.421550989151001\n",
      "            Test  set - loss: 0.3946284055709839\n",
      "            \n",
      "epoch 180\n",
      "            Train set - loss: 0.4450916349887848\n",
      "            Test  set - loss: 0.37135231494903564\n",
      "            \n",
      "epoch 190\n",
      "            Train set - loss: 0.5004255175590515\n",
      "            Test  set - loss: 0.37949368357658386\n",
      "            \n",
      "epoch 200\n",
      "            Train set - loss: 0.311811238527298\n",
      "            Test  set - loss: 0.3744446635246277\n",
      "            \n",
      "epoch 210\n",
      "            Train set - loss: 0.8550828099250793\n",
      "            Test  set - loss: 0.37650054693222046\n",
      "            \n",
      "epoch 220\n",
      "            Train set - loss: 0.3509237468242645\n",
      "            Test  set - loss: 0.37407824397087097\n",
      "            \n",
      "epoch 230\n",
      "            Train set - loss: 0.44298961758613586\n",
      "            Test  set - loss: 0.374470591545105\n",
      "            \n",
      "epoch 240\n",
      "            Train set - loss: 0.3188031017780304\n",
      "            Test  set - loss: 0.3724542260169983\n",
      "            \n",
      "KernelNet complexity and model fitted in 95.840 s\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(mydataset(nntrain_x, nntrain_y),batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(mydataset(nntest_x, nntest_y),batch_size=100, shuffle=False)\n",
    "\n",
    "rff1=RandomFourierFeature(8,32,kernel='C',gamma=0.05)\n",
    "rff2=RandomFourierFeature(8,8,kernel='G',gamma=0.5)\n",
    "\n",
    "class KernelNet(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(KernelNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(32, 8)\n",
    "        self.fc2 = nn.Linear(8, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = rff1.transform(x)\n",
    "        x=self.fc1(x)\n",
    "        x = rff2.transform(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "#initialize\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.normal_(m.weight,mean=0,std=0.5)\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.uniform_(m.weight,a=-0.1,b=0.1)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = KernelNet()\n",
    "net = net.to(device)\n",
    "torch.manual_seed(1)\n",
    "net.apply(init_weights)\n",
    "print(net)\n",
    "criterion=nn.MSELoss() \n",
    "optimizer=optim.SGD([{'params': net.parameters(), 'initial_lr': 8e-4}],lr=8e-4,momentum=0.9,weight_decay=1e-4) #optim.Adam(...)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=100,gamma=0.5,last_epoch=300) \n",
    "loss=[]\n",
    "kernelnn_trainloss=[]\n",
    "kernelnn_testloss=[]\n",
    "t0 = time.time()\n",
    "for epoch in range(500): \n",
    "    for x, y in train_loader: #for batch, (x, y) in enumerate(train_loader): \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        y_pred = net(x)\n",
    "        y_pred = torch.squeeze(y_pred)\n",
    "        train_loss = criterion(y_pred, y)\n",
    "        loss.append(train_loss)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_test_pred = net(x)\n",
    "        y_test_pred = torch.squeeze(y_test_pred)\n",
    "\n",
    "        test_loss = criterion(y_test_pred,y)\n",
    "            \n",
    "    if epoch>50 and float(test_loss)>max(kernelnn_testloss[-50:-1]):\n",
    "        break\n",
    "    \n",
    "    \n",
    "    if epoch % 10 == 0:         \n",
    "        print(f'''epoch {epoch}\n",
    "            Train set - loss: {train_loss}\n",
    "            Test  set - loss: {test_loss}\n",
    "            ''')\n",
    "   \n",
    "    kernelnn_trainloss.append(float(train_loss))\n",
    "    kernelnn_testloss.append(float(test_loss))\n",
    "        \n",
    "    \n",
    "dnn_fit = time.time() - t0\n",
    "print(\"KernelNet complexity and model fitted in %.3f s\" % dnn_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.2063332 0.7083667 2.779955  ... 1.7587211 1.6556816 1.6504678]\n",
      "[3.55  0.707 2.294 ... 1.098 1.625 1.667]\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "x0=torch.from_numpy(test_x[:].to_numpy()).float()\n",
    "with torch.no_grad():\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    print(np.array(pred).reshape(-1))\n",
    "    print(test_y[:].to_numpy())\n",
    "    bootbase=np.array(pred.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14448/14448 [00:45<00:00, 319.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14448, 273])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6192/6192 [00:04<00:00, 1480.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-p: 14175  mark: 6188\n",
      "length 2.3465136426584454\n",
      "95 coverage 0.9290562378797673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "par=optimizer.param_groups[0]['params']\n",
    "\n",
    "for i in tqdm(range(len(train_x))):\n",
    "    x0=torch.from_numpy(train_x[i:1+i].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    fi=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()   \n",
    "    for j in range(len(par)): \n",
    "        fi=torch.cat([fi,par[j].grad.reshape(-1)])\n",
    "    fi=fi.reshape(1,-1)\n",
    "    if i==0:\n",
    "        Fi=fi\n",
    "    else:\n",
    "        Fi=torch.cat([Fi,fi])   \n",
    "print(Fi.shape)\n",
    "\n",
    "temp=torch.linalg.inv(Fi.T @ Fi)\n",
    "\n",
    "length=[]\n",
    "coverage=0\n",
    "mark=0\n",
    "for i in tqdm(range(len(test_x))):\n",
    "    x0=torch.from_numpy(test_x[i:i+1].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    #print(pred.detach().numpy()[0][0],test_y[i])\n",
    "    par=optimizer.param_groups[0]['params']\n",
    "    f0=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()\n",
    "    for j in range(len(par)):\n",
    "        f0=torch.cat([f0,par[j].grad.reshape(-1)])\n",
    "    f0=f0.reshape(-1,1)\n",
    "\n",
    "    fFFf=f0.T @ temp @ f0\n",
    "    #print(2*1.96*np.sqrt(float(fFFf+1))) #approximate with df(infinity)\n",
    "    \n",
    "    if fFFf < 0:\n",
    "        continue\n",
    "    mark=mark+1\n",
    "    length.append(2*1.96*np.sqrt(float(fFFf+1))*np.sqrt(kernelnn_testloss[-1]))\n",
    "    \n",
    "    #coverage\n",
    "    if pred.detach().numpy()[0][0]-1.96*np.sqrt(float(fFFf+1))*np.sqrt(kernelnn_testloss[-1])<test_y[i] and pred.detach().numpy()[0][0]+1.96*np.sqrt(float(fFFf+1))*np.sqrt(kernelnn_testloss[-1])>test_y[i]:\n",
    "        coverage=coverage+1\n",
    "coverage=coverage/mark\n",
    "\n",
    "print(\"n-p:\",len(train_x)-f0.shape[0],\" mark:\",mark) \n",
    "print(\"length\",np.mean(length))\n",
    "print(\"95 coverage\",coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14448/14448 [00:42<00:00, 343.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14448, 273])\n",
      "14268.250122070312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6192/6192 [00:03<00:00, 1715.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-p: 14175  mark: 2849\n",
      "length 2.3564324941723127\n",
      "95 coverage 0.9427869427869427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#penalty\n",
    "#predict\n",
    "wei=1e-4\n",
    "par=optimizer.param_groups[0]['params']\n",
    "\n",
    "for i in tqdm(range(len(train_x))):\n",
    "    x0=torch.from_numpy(train_x[i:1+i].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    fi=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()   \n",
    "    for j in range(len(par)): \n",
    "        fi=torch.cat([fi,par[j].grad.reshape(-1)])\n",
    "    fi=fi.reshape(1,-1)\n",
    "    if i==0:\n",
    "        Fi=fi\n",
    "    else:\n",
    "        Fi=torch.cat([Fi,fi])   \n",
    "print(Fi.shape)\n",
    "\n",
    "\n",
    "temp2=torch.linalg.inv(Fi.T @ Fi+wei *np.eye(Fi.T.shape[0]))\n",
    "temp2=temp2.float()\n",
    "temp=temp2@Fi.T @ Fi@temp2\n",
    "p=Fi @temp2 @Fi.T\n",
    "aa=len(train_x)-np.trace(2*p-p@p)\n",
    "print(aa)\n",
    "corr=(len(train_x)-f0.shape[0])/aa\n",
    "\n",
    "length=[]\n",
    "coverage=0\n",
    "mark=0\n",
    "for i in tqdm(range(len(test_x))):\n",
    "    x0=torch.from_numpy(test_x[i:i+1].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    #print(pred.detach().numpy()[0][0],test_y[i])\n",
    "    par=optimizer.param_groups[0]['params']\n",
    "    f0=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()\n",
    "    for j in range(len(par)):\n",
    "        f0=torch.cat([f0,par[j].grad.reshape(-1)])\n",
    "    f0=f0.reshape(-1,1)\n",
    "\n",
    "    fFFf=f0.T @ temp @ f0\n",
    "    #print(2*1.96*np.sqrt(float(fFFf+1))) #approximate with df(infinity)\n",
    "    \n",
    "    if fFFf < 0:\n",
    "        continue\n",
    "    mark=mark+1\n",
    "    dd=1.96*np.sqrt(float(fFFf+1))*np.sqrt(kernelnn_testloss[-1])*corr\n",
    "    length.append(2*dd)\n",
    "    \n",
    "    #coverage\n",
    "    if pred.detach().numpy()[0][0]-dd<test_y[i] and pred.detach().numpy()[0][0]+dd>test_y[i]:\n",
    "        coverage=coverage+1\n",
    "coverage=coverage/mark\n",
    "\n",
    "print(\"n-p:\",len(train_x)-f0.shape[0],\" mark:\",mark) \n",
    "print(\"length\",np.mean(length))\n",
    "print(\"95 coverage\",coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14448/14448 [00:42<00:00, 339.31it/s]\n",
      "100%|██████████| 14448/14448 [00:10<00:00, 1425.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13727.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6192/6192 [00:04<00:00, 1514.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length 2.739484041832922\n",
      "95 coverage 0.9508524679307924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##conformal prediction\n",
    "#predict\n",
    "par=optimizer.param_groups[0]['params']\n",
    "for i in tqdm(range(len(train_x))):\n",
    "    x0=torch.from_numpy(train_x[i:1+i].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    fi=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()   \n",
    "    for j in range(len(par)): \n",
    "        fi=torch.cat([fi,par[j].grad.reshape(-1)])\n",
    "    fi=fi.reshape(1,-1)\n",
    "    if i==0:\n",
    "        Fi=fi\n",
    "    else:\n",
    "        Fi=torch.cat([Fi,fi])   \n",
    "temp=torch.linalg.inv(Fi.T @ Fi)\n",
    "\n",
    "mark=0\n",
    "score=torch.tensor([])\n",
    "for i in tqdm(range(len(train_x))):\n",
    "    x0=torch.from_numpy(train_x[i:i+1].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    par=optimizer.param_groups[0]['params']\n",
    "    f0=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()\n",
    "    for j in range(len(par)):\n",
    "        f0=torch.cat([f0,par[j].grad.reshape(-1)])\n",
    "    f0=f0.reshape(-1,1)\n",
    "    fFFf=f0.T @ temp @ f0\n",
    "    score=torch.cat([score,np.abs(pred.detach().numpy()[0][0]-train_y[i])/np.sqrt(kernelnn_testloss[-1])/np.sqrt(fFFf+1)])\n",
    "    if fFFf < 0:\n",
    "        continue\n",
    "    mark=mark+1\n",
    "score=score.reshape(-1)\n",
    "sorted_score, sorted_indices=torch.sort(score)\n",
    "q=(len(train_x)+1)*0.95\n",
    "print(np.ceil(q))\n",
    "a=sorted_score[int(np.ceil(q))]\n",
    "\n",
    "\n",
    "mark=0\n",
    "length=[]\n",
    "for i in tqdm(range(len(test_x))):\n",
    "    x0=torch.from_numpy(test_x[i:i+1].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    par=optimizer.param_groups[0]['params']\n",
    "    f0=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()\n",
    "    for j in range(len(par)):\n",
    "        f0=torch.cat([f0,par[j].grad.reshape(-1)])\n",
    "    f0=f0.reshape(-1,1)\n",
    "\n",
    "    fFFf=f0.T @ temp @ f0\n",
    "    \n",
    "    if fFFf < 0:\n",
    "        continue\n",
    "    mark=mark+1\n",
    "    dd=(np.sqrt(kernelnn_testloss[-1])*np.sqrt(fFFf+1)*a).detach().numpy()[0][0]\n",
    "    length.append(2*dd)\n",
    "    \n",
    "    #coverage\n",
    "    if pred.detach().numpy()[0][0]-dd<test_y[i] and pred.detach().numpy()[0][0]+dd>test_y[i]:\n",
    "        coverage=coverage+1\n",
    "coverage=coverage/mark\n",
    "\n",
    "\n",
    "print(\"length\",np.mean(length))\n",
    "print(\"95 coverage\",coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13727.0\n",
      "length tensor(2.7298, dtype=torch.float64)\n",
      "95 coverage 0.8313953488372093\n"
     ]
    }
   ],
   "source": [
    "##conformal prediction\n",
    "#predict\n",
    "x0=torch.from_numpy(train_x[:].to_numpy()).float()\n",
    "with torch.no_grad():\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    score=np.abs(pred.reshape(-1)-train_y[:].to_numpy())\n",
    "sorted_score, sorted_indices=torch.sort(score)\n",
    "q=(len(train_x)+1)*0.95\n",
    "print(np.ceil(q))\n",
    "a=sorted_score[int(np.ceil(q))]\n",
    "\n",
    "coverage=0\n",
    "for i in range(len(test_x)):\n",
    "    if pred.detach().numpy()[0][0]-a<test_y[i] and pred.detach().numpy()[0][0]+a>test_y[i]:\n",
    "        coverage=coverage+1\n",
    "coverage=coverage/len(test_x)\n",
    "\n",
    "print(\"length\",2*a)\n",
    "print(\"95 coverage\",coverage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(bootlist).T\n",
    "sorted_bootlist = [sorted(x)[:] for x in np.array(bootlist).T]\n",
    "sorted_bootlist=np.array(sorted_bootlist)\n",
    "\n",
    "lower=bootbase-(sorted_bootlist[:,38]-bootbase)\n",
    "upper=bootbase-(sorted_bootlist[:,1]-bootbase)\n",
    "print(\"confidence interval length\",sorted_bootlist[:,38]-sorted_bootlist[:,1])\n",
    "kernelnet_length=(sorted_bootlist[:,38]-sorted_bootlist[:,1]).mean()\n",
    "print(\"average confidence interval length\",kernelnet_length)\n",
    "\n",
    "cover=0\n",
    "for i in range(len(test_y)):\n",
    "    if lower[i]<=test_y[i] and upper[i]>=test_y[i]:\n",
    "        cover=cover+1\n",
    "kernelnet_coverage=cover/len(test_y)\n",
    "print(\"95 coverage\",kernelnet_coverage)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Residual multilayer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResKernelNet(\n",
      "  (rblock1): ResidualBlock(\n",
      "    (fc1): Linear(in_features=32, out_features=8, bias=True)\n",
      "    (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "  )\n",
      "  (fc2): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0\n",
      "            Train set - loss: 2.9326369762420654\n",
      "            Test  set - loss: 2.6718900203704834\n",
      "            \n",
      "epoch 10\n",
      "            Train set - loss: 0.5328090786933899\n",
      "            Test  set - loss: 0.573363184928894\n",
      "            \n",
      "epoch 20\n",
      "            Train set - loss: 0.42666542530059814\n",
      "            Test  set - loss: 0.4792711138725281\n",
      "            \n",
      "epoch 30\n",
      "            Train set - loss: 0.34947076439857483\n",
      "            Test  set - loss: 0.45658621191978455\n",
      "            \n",
      "epoch 40\n",
      "            Train set - loss: 0.6628111004829407\n",
      "            Test  set - loss: 0.4383874833583832\n",
      "            \n",
      "epoch 50\n",
      "            Train set - loss: 0.686427116394043\n",
      "            Test  set - loss: 0.41908156871795654\n",
      "            \n",
      "epoch 60\n",
      "            Train set - loss: 0.5818543434143066\n",
      "            Test  set - loss: 0.4210948944091797\n",
      "            \n",
      "epoch 70\n",
      "            Train set - loss: 0.5152994990348816\n",
      "            Test  set - loss: 0.41704726219177246\n",
      "            \n",
      "epoch 80\n",
      "            Train set - loss: 0.5984522700309753\n",
      "            Test  set - loss: 0.42554083466529846\n",
      "            \n",
      "epoch 90\n",
      "            Train set - loss: 0.4913042485713959\n",
      "            Test  set - loss: 0.413687139749527\n",
      "            \n",
      "epoch 100\n",
      "            Train set - loss: 0.7148397564888\n",
      "            Test  set - loss: 0.41426220536231995\n",
      "            \n",
      "epoch 110\n",
      "            Train set - loss: 0.2879389226436615\n",
      "            Test  set - loss: 0.4125474989414215\n",
      "            \n",
      "epoch 120\n",
      "            Train set - loss: 0.9677883982658386\n",
      "            Test  set - loss: 0.41312655806541443\n",
      "            \n",
      "epoch 130\n",
      "            Train set - loss: 0.5254122018814087\n",
      "            Test  set - loss: 0.41379514336586\n",
      "            \n",
      "epoch 140\n",
      "            Train set - loss: 0.6558010578155518\n",
      "            Test  set - loss: 0.40693187713623047\n",
      "            \n",
      "epoch 150\n",
      "            Train set - loss: 0.4435255825519562\n",
      "            Test  set - loss: 0.409850537776947\n",
      "            \n",
      "epoch 160\n",
      "            Train set - loss: 0.4290793240070343\n",
      "            Test  set - loss: 0.4080146849155426\n",
      "            \n",
      "epoch 170\n",
      "            Train set - loss: 0.47496315836906433\n",
      "            Test  set - loss: 0.4083755612373352\n",
      "            \n",
      "epoch 180\n",
      "            Train set - loss: 0.3270769417285919\n",
      "            Test  set - loss: 0.410442978143692\n",
      "            \n",
      "epoch 190\n",
      "            Train set - loss: 0.4776228964328766\n",
      "            Test  set - loss: 0.40339916944503784\n",
      "            \n",
      "epoch 200\n",
      "            Train set - loss: 0.8555579781532288\n",
      "            Test  set - loss: 0.4031086266040802\n",
      "            \n",
      "epoch 210\n",
      "            Train set - loss: 0.44906020164489746\n",
      "            Test  set - loss: 0.4046308696269989\n",
      "            \n",
      "epoch 220\n",
      "            Train set - loss: 0.2533016502857208\n",
      "            Test  set - loss: 0.40403851866722107\n",
      "            \n",
      "epoch 230\n",
      "            Train set - loss: 0.7455430030822754\n",
      "            Test  set - loss: 0.4010256826877594\n",
      "            \n",
      "epoch 240\n",
      "            Train set - loss: 0.3769552707672119\n",
      "            Test  set - loss: 0.40336620807647705\n",
      "            \n",
      "epoch 250\n",
      "            Train set - loss: 0.5598212480545044\n",
      "            Test  set - loss: 0.4027031660079956\n",
      "            \n",
      "epoch 260\n",
      "            Train set - loss: 0.403643935918808\n",
      "            Test  set - loss: 0.4023090898990631\n",
      "            \n",
      "epoch 270\n",
      "            Train set - loss: 0.6903861165046692\n",
      "            Test  set - loss: 0.40333980321884155\n",
      "            \n",
      "epoch 280\n",
      "            Train set - loss: 0.518799364566803\n",
      "            Test  set - loss: 0.40240606665611267\n",
      "            \n",
      "epoch 290\n",
      "            Train set - loss: 0.5398228764533997\n",
      "            Test  set - loss: 0.4029901623725891\n",
      "            \n",
      "epoch 300\n",
      "            Train set - loss: 0.6274598240852356\n",
      "            Test  set - loss: 0.40257728099823\n",
      "            \n",
      "epoch 310\n",
      "            Train set - loss: 0.23787887394428253\n",
      "            Test  set - loss: 0.402772456407547\n",
      "            \n",
      "epoch 320\n",
      "            Train set - loss: 0.5808829665184021\n",
      "            Test  set - loss: 0.4026629328727722\n",
      "            \n",
      "epoch 330\n",
      "            Train set - loss: 0.6602599620819092\n",
      "            Test  set - loss: 0.40200039744377136\n",
      "            \n",
      "epoch 340\n",
      "            Train set - loss: 0.4626127779483795\n",
      "            Test  set - loss: 0.4023746848106384\n",
      "            \n",
      "epoch 350\n",
      "            Train set - loss: 0.5918717384338379\n",
      "            Test  set - loss: 0.4023642838001251\n",
      "            \n",
      "epoch 360\n",
      "            Train set - loss: 0.5354612469673157\n",
      "            Test  set - loss: 0.4022314250469208\n",
      "            \n",
      "epoch 370\n",
      "            Train set - loss: 0.5337490439414978\n",
      "            Test  set - loss: 0.4021616280078888\n",
      "            \n",
      "epoch 380\n",
      "            Train set - loss: 0.7061697840690613\n",
      "            Test  set - loss: 0.40221574902534485\n",
      "            \n",
      "epoch 390\n",
      "            Train set - loss: 0.4893101155757904\n",
      "            Test  set - loss: 0.4023859202861786\n",
      "            \n",
      "epoch 400\n",
      "            Train set - loss: 0.6207717657089233\n",
      "            Test  set - loss: 0.402324914932251\n",
      "            \n",
      "epoch 410\n",
      "            Train set - loss: 0.39226773381233215\n",
      "            Test  set - loss: 0.4023769497871399\n",
      "            \n",
      "epoch 420\n",
      "            Train set - loss: 0.5604762434959412\n",
      "            Test  set - loss: 0.4022744297981262\n",
      "            \n",
      "epoch 430\n",
      "            Train set - loss: 0.4490528106689453\n",
      "            Test  set - loss: 0.4022196829319\n",
      "            \n",
      "epoch 440\n",
      "            Train set - loss: 0.32083991169929504\n",
      "            Test  set - loss: 0.4023481011390686\n",
      "            \n",
      "epoch 450\n",
      "            Train set - loss: 0.37974247336387634\n",
      "            Test  set - loss: 0.402238667011261\n",
      "            \n",
      "epoch 460\n",
      "            Train set - loss: 0.3591371774673462\n",
      "            Test  set - loss: 0.402226984500885\n",
      "            \n",
      "epoch 470\n",
      "            Train set - loss: 0.35636448860168457\n",
      "            Test  set - loss: 0.4022156298160553\n",
      "            \n",
      "epoch 480\n",
      "            Train set - loss: 0.2684538662433624\n",
      "            Test  set - loss: 0.40220907330513\n",
      "            \n",
      "epoch 490\n",
      "            Train set - loss: 0.5584642887115479\n",
      "            Test  set - loss: 0.4021027982234955\n",
      "            \n",
      "Residual KernelNet complexity and model fitted in 169.990 s\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(mydataset(nntrain_x, nntrain_y),batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(mydataset(nntest_x, nntest_y),batch_size=100, shuffle=False)\n",
    "\n",
    "rff0=RandomFourierFeature(8,32,kernel='C',gamma=0.05)\n",
    "rff1=RandomFourierFeature(8,8,kernel='G',gamma=5)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,infeatures,outfeatures,rff):\n",
    "        super(ResidualBlock,self).__init__()\n",
    "        self.infeatures = infeatures\n",
    "        self.outfeatures = outfeatures\n",
    "        self.rff=rff\n",
    "        \n",
    "        self.fc1 = nn.Linear(infeatures,outfeatures)\n",
    "        self.fc2 = nn.Linear(outfeatures,outfeatures)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        rff=self.rff\n",
    "        x = self.fc1(x)\n",
    "        y = rff.transform(x)\n",
    "        y = self.fc2(y)\n",
    "        return x+y\n",
    "\n",
    "class ResKernelNet(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(ResKernelNet, self).__init__()\n",
    "        self.rblock1 = ResidualBlock(32,8,rff1)\n",
    "        self.fc2 =nn.Linear(8,1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = rff0.transform(x)\n",
    "        x = self.rblock1(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "#initialize\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.normal_(m.weight,mean=0,std=0.5)\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.uniform_(m.weight,a=0,b=0.1)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = ResKernelNet()\n",
    "net = net.to(device)\n",
    "torch.manual_seed(1)\n",
    "#net.apply(init_weights)\n",
    "print(net)\n",
    "criterion=nn.MSELoss() \n",
    "optimizer=optim.SGD([{'params': net.parameters(), 'initial_lr': 5e-5}],lr=5e-5,momentum=0.9,weight_decay=1e-4) #optim.Adam(...)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=50,gamma=0.5,last_epoch=300) \n",
    "#lr_scheduler.StepLR(optimizer,step_size=50,gamma=0.2) \n",
    "#lr_scheduler.MultiStepLR(optimizer, milestones=[30,80], gamma=0.5)\n",
    "#lr_scheduler.ExponentialLR(optimizer, gamma=0.9) \n",
    "#lr_scheduler.LinearLR(optimizer,start_factor=1,end_factor=0.1,total_iters=80)\n",
    "\n",
    "loss=[]\n",
    "reskernel_trainloss=[]\n",
    "reskernel_testloss=[]\n",
    "t0 = time.time()\n",
    "for epoch in range(500): \n",
    "    for x, y in train_loader: #for batch, (x, y) in enumerate(train_loader): \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        y_pred = net(x)\n",
    "        y_pred = torch.squeeze(y_pred)\n",
    "        train_loss = criterion(y_pred, y)\n",
    "        loss.append(train_loss)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_test_pred = net(x)\n",
    "        y_test_pred = torch.squeeze(y_test_pred)\n",
    "\n",
    "        test_loss = criterion(y_test_pred,y)\n",
    "            \n",
    "    if epoch>50 and float(test_loss)>max(reskernel_testloss[-50:-1]):\n",
    "        break\n",
    "    \n",
    "    \n",
    "    if epoch % 10 == 0:         \n",
    "        print(f'''epoch {epoch}\n",
    "            Train set - loss: {train_loss}\n",
    "            Test  set - loss: {test_loss}\n",
    "            ''')\n",
    "    reskernel_trainloss.append(float(train_loss))\n",
    "    reskernel_testloss.append(float(test_loss))\n",
    "        \n",
    "    \n",
    "dnn_fit = time.time() - t0\n",
    "print(\"Residual KernelNet complexity and model fitted in %.3f s\" % dnn_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0778198 0.7626394 2.8846967 ... 1.5474226 1.5113223 1.3297695]\n",
      "[3.55  0.707 2.294 ... 1.098 1.625 1.667]\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "x0=torch.from_numpy(test_x[:].to_numpy()).float()\n",
    "with torch.no_grad():\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    print(np.array(pred).reshape(-1))\n",
    "    print(test_y[:].to_numpy())\n",
    "    bootbase=np.array(pred.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14448/14448 [00:52<00:00, 273.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14448, 345])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6192/6192 [00:04<00:00, 1281.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-p: 14103  mark: 6192\n",
      "length 2.4907132125459315\n",
      "95 coverage 0.9329780361757106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "par=optimizer.param_groups[0]['params']\n",
    "\n",
    "for i in tqdm(range(len(train_x))):\n",
    "    x0=torch.from_numpy(train_x[i:1+i].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    fi=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()   \n",
    "    for j in range(len(par)): \n",
    "        fi=torch.cat([fi,par[j].grad.reshape(-1)])\n",
    "    fi=fi.reshape(1,-1)\n",
    "    if i==0:\n",
    "        Fi=fi\n",
    "    else:\n",
    "        Fi=torch.cat([Fi,fi])   \n",
    "print(Fi.shape)\n",
    "\n",
    "temp=torch.linalg.pinv(Fi.T @ Fi)\n",
    "\n",
    "length=[]\n",
    "coverage=0\n",
    "mark=0\n",
    "for i in tqdm(range(len(test_x))):\n",
    "    x0=torch.from_numpy(test_x[i:i+1].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    #print(pred.detach().numpy()[0][0],test_y[i])\n",
    "    par=optimizer.param_groups[0]['params']\n",
    "    f0=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()\n",
    "    for j in range(len(par)):\n",
    "        f0=torch.cat([f0,par[j].grad.reshape(-1)])\n",
    "    f0=f0.reshape(-1,1)\n",
    "\n",
    "    fFFf=f0.T @ temp @ f0\n",
    "    #print(2*1.96*np.sqrt(float(fFFf+1))) #approximate with df(infinity)\n",
    "    \n",
    "    if fFFf < 0:\n",
    "        continue\n",
    "    mark=mark+1\n",
    "    length.append(2*1.96*np.sqrt(float(fFFf+1))*np.sqrt(reskernel_testloss[-1]))\n",
    "    \n",
    "    #coverage\n",
    "    if pred.detach().numpy()[0][0]-1.96*np.sqrt(float(fFFf+1))*np.sqrt(reskernel_testloss[-1])<test_y[i] and pred.detach().numpy()[0][0]+1.96*np.sqrt(float(fFFf+1))*np.sqrt(reskernel_testloss[-1])>test_y[i]:\n",
    "        coverage=coverage+1\n",
    "coverage=coverage/mark\n",
    "\n",
    "print(\"n-p:\",len(train_x)-f0.shape[0],\" mark:\",mark) \n",
    "print(\"length\",np.mean(length))\n",
    "print(\"95 coverage\",coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14448/14448 [00:49<00:00, 293.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14448, 345])\n",
      "14388.782176971436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6192/6192 [00:04<00:00, 1259.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-p: 14103  mark: 3253\n",
      "length 2.672453529411732\n",
      "95 coverage 0.9363664309867814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#penalty\n",
    "#predict\n",
    "wei=1e-4\n",
    "par=optimizer.param_groups[0]['params']\n",
    "\n",
    "for i in tqdm(range(len(train_x))):\n",
    "    x0=torch.from_numpy(train_x[i:1+i].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    fi=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()   \n",
    "    for j in range(len(par)): \n",
    "        fi=torch.cat([fi,par[j].grad.reshape(-1)])\n",
    "    fi=fi.reshape(1,-1)\n",
    "    if i==0:\n",
    "        Fi=fi\n",
    "    else:\n",
    "        Fi=torch.cat([Fi,fi])   \n",
    "print(Fi.shape)\n",
    "\n",
    "\n",
    "temp2=torch.linalg.inv(Fi.T @ Fi+wei *np.eye(Fi.T.shape[0]))\n",
    "temp2=temp2.float()\n",
    "temp=temp2@Fi.T @ Fi@temp2\n",
    "p=Fi @temp2 @Fi.T\n",
    "aa=len(train_x)-np.trace(2*p-p@p)\n",
    "print(aa)\n",
    "corr=(len(train_x)-f0.shape[0])/aa\n",
    "\n",
    "length=[]\n",
    "coverage=0\n",
    "mark=0\n",
    "for i in tqdm(range(len(test_x))):\n",
    "    x0=torch.from_numpy(test_x[i:i+1].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    #print(pred.detach().numpy()[0][0],test_y[i])\n",
    "    par=optimizer.param_groups[0]['params']\n",
    "    f0=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()\n",
    "    for j in range(len(par)):\n",
    "        f0=torch.cat([f0,par[j].grad.reshape(-1)])\n",
    "    f0=f0.reshape(-1,1)\n",
    "\n",
    "    fFFf=f0.T @ temp @ f0\n",
    "    #print(2*1.96*np.sqrt(float(fFFf+1))) #approximate with df(infinity)\n",
    "    \n",
    "    if fFFf < 0:\n",
    "        continue\n",
    "    mark=mark+1\n",
    "    dd=1.96*np.sqrt(float(fFFf+1))*np.sqrt(reskernel_trainloss[-1])*corr\n",
    "    length.append(2*dd)\n",
    "    \n",
    "    #coverage\n",
    "    if pred.detach().numpy()[0][0]-dd<test_y[i] and pred.detach().numpy()[0][0]+dd>test_y[i]:\n",
    "        coverage=coverage+1\n",
    "coverage=coverage/mark\n",
    "\n",
    "print(\"n-p:\",len(train_x)-f0.shape[0],\" mark:\",mark) \n",
    "print(\"length\",np.mean(length))\n",
    "print(\"95 coverage\",coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14448/14448 [00:49<00:00, 292.46it/s]\n",
      "100%|██████████| 14448/14448 [00:13<00:00, 1103.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13727.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6192/6192 [00:05<00:00, 1237.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length 2.9016222554117346\n",
      "95 coverage 0.9514460139855966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##conformal prediction\n",
    "#predict\n",
    "par=optimizer.param_groups[0]['params']\n",
    "for i in tqdm(range(len(train_x))):\n",
    "    x0=torch.from_numpy(train_x[i:1+i].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    fi=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()   \n",
    "    for j in range(len(par)): \n",
    "        fi=torch.cat([fi,par[j].grad.reshape(-1)])\n",
    "    fi=fi.reshape(1,-1)\n",
    "    if i==0:\n",
    "        Fi=fi\n",
    "    else:\n",
    "        Fi=torch.cat([Fi,fi])   \n",
    "temp=torch.linalg.inv(Fi.T @ Fi)\n",
    "\n",
    "mark=0\n",
    "score=torch.tensor([])\n",
    "for i in tqdm(range(len(train_x))):\n",
    "    x0=torch.from_numpy(train_x[i:i+1].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    par=optimizer.param_groups[0]['params']\n",
    "    f0=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()\n",
    "    for j in range(len(par)):\n",
    "        f0=torch.cat([f0,par[j].grad.reshape(-1)])\n",
    "    f0=f0.reshape(-1,1)\n",
    "    fFFf=f0.T @ temp @ f0\n",
    "    score=torch.cat([score,np.abs(pred.detach().numpy()[0][0]-train_y[i])/np.sqrt(reskernel_trainloss[-1])/np.sqrt(fFFf+1)])\n",
    "    if fFFf < 0:\n",
    "        continue\n",
    "    mark=mark+1\n",
    "score=score.reshape(-1)\n",
    "sorted_score, sorted_indices=torch.sort(score)\n",
    "q=(len(train_x)+1)*0.95\n",
    "print(np.ceil(q))\n",
    "a=sorted_score[int(np.ceil(q))]\n",
    "\n",
    "\n",
    "\n",
    "mark=0\n",
    "length=[]\n",
    "for i in tqdm(range(len(test_x))):\n",
    "    x0=torch.from_numpy(test_x[i:i+1].to_numpy()).float()\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    par=optimizer.param_groups[0]['params']\n",
    "    f0=torch.tensor([])\n",
    "    for j in range(len(par)):\n",
    "        par[j].grad.data.zero_()\n",
    "    pred.backward()\n",
    "    for j in range(len(par)):\n",
    "        f0=torch.cat([f0,par[j].grad.reshape(-1)])\n",
    "    f0=f0.reshape(-1,1)\n",
    "\n",
    "    fFFf=f0.T @ temp @ f0\n",
    "    \n",
    "    if fFFf < 0:\n",
    "        continue\n",
    "    mark=mark+1\n",
    "    dd=(np.sqrt(reskernel_trainloss[-1])*np.sqrt(fFFf+1)*a).detach().numpy()[0][0]\n",
    "    length.append(2*dd)\n",
    "    \n",
    "    #coverage\n",
    "    if pred.detach().numpy()[0][0]-dd<test_y[i] and pred.detach().numpy()[0][0]+dd>test_y[i]:\n",
    "        coverage=coverage+1\n",
    "coverage=coverage/mark\n",
    "\n",
    "\n",
    "print(\"length\",np.mean(length))\n",
    "print(\"95 coverage\",coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13727.0\n",
      "length tensor(2.8790, dtype=torch.float64)\n",
      "95 coverage 0.838501291989664\n"
     ]
    }
   ],
   "source": [
    "##conformal prediction\n",
    "#predict\n",
    "x0=torch.from_numpy(train_x[:].to_numpy()).float()\n",
    "with torch.no_grad():\n",
    "    x0 = x0.to(device)\n",
    "    pred = net(x0)\n",
    "    score=np.abs(pred.reshape(-1)-train_y[:].to_numpy())\n",
    "sorted_score, sorted_indices=torch.sort(score)\n",
    "q=(len(train_x)+1)*0.95\n",
    "print(np.ceil(q))\n",
    "a=sorted_score[int(np.ceil(q))]\n",
    "\n",
    "coverage=0\n",
    "for i in range(len(test_x)):\n",
    "    if pred.detach().numpy()[0][0]-a<test_y[i] and pred.detach().numpy()[0][0]+a>test_y[i]:\n",
    "        coverage=coverage+1\n",
    "coverage=coverage/len(test_x)\n",
    "\n",
    "print(\"length\",2*a)\n",
    "print(\"95 coverage\",coverage)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "            Train set - loss: 1.312590479850769\n",
      "            Test  set - loss: 1.1127039194107056\n",
      "            \n",
      "epoch 10\n",
      "            Train set - loss: 0.888997495174408\n",
      "            Test  set - loss: 0.48912161588668823\n",
      "            \n",
      "epoch 20\n",
      "            Train set - loss: 0.8583675622940063\n",
      "            Test  set - loss: 0.4511373043060303\n",
      "            \n",
      "epoch 30\n",
      "            Train set - loss: 0.956074595451355\n",
      "            Test  set - loss: 0.460627943277359\n",
      "            \n",
      "epoch 40\n",
      "            Train set - loss: 0.4315779507160187\n",
      "            Test  set - loss: 0.47071152925491333\n",
      "            \n",
      "epoch 50\n",
      "            Train set - loss: 0.5028590559959412\n",
      "            Test  set - loss: 0.45038920640945435\n",
      "            \n",
      "epoch 60\n",
      "            Train set - loss: 0.814349889755249\n",
      "            Test  set - loss: 0.44884002208709717\n",
      "            \n",
      "epoch 70\n",
      "            Train set - loss: 0.7365018725395203\n",
      "            Test  set - loss: 0.4395692050457001\n",
      "            \n",
      "epoch 80\n",
      "            Train set - loss: 0.8676408529281616\n",
      "            Test  set - loss: 0.4354609549045563\n",
      "            \n",
      "epoch 90\n",
      "            Train set - loss: 0.673255205154419\n",
      "            Test  set - loss: 0.4298461079597473\n",
      "            \n",
      "epoch 100\n",
      "            Train set - loss: 0.41470587253570557\n",
      "            Test  set - loss: 0.4266154170036316\n",
      "            \n",
      "epoch 110\n",
      "            Train set - loss: 0.5851938724517822\n",
      "            Test  set - loss: 0.42468708753585815\n",
      "            \n",
      "epoch 120\n",
      "            Train set - loss: 0.42190876603126526\n",
      "            Test  set - loss: 0.4193231463432312\n",
      "            \n",
      "epoch 130\n",
      "            Train set - loss: 0.3568800091743469\n",
      "            Test  set - loss: 0.4162531793117523\n",
      "            \n",
      "epoch 140\n",
      "            Train set - loss: 0.45884835720062256\n",
      "            Test  set - loss: 0.41985997557640076\n",
      "            \n",
      "epoch 150\n",
      "            Train set - loss: 0.5977818965911865\n",
      "            Test  set - loss: 0.4177212417125702\n",
      "            \n",
      "epoch 160\n",
      "            Train set - loss: 0.29481250047683716\n",
      "            Test  set - loss: 0.41858458518981934\n",
      "            \n",
      "KernelNet(data splitting) complexity and model fitted in 86.044 s\n"
     ]
    }
   ],
   "source": [
    "#multi\n",
    "split=2\n",
    "layer=2\n",
    "length=int(len(train_x)/layer)\n",
    "test_loader = DataLoader(mydataset(nntest_x, nntest_y),batch_size=100, shuffle=False)\n",
    "\n",
    "rff1=RandomFourierFeature(8,32,kernel='C',gamma=0.05)\n",
    "rff2=RandomFourierFeature(8,8,kernel='G',gamma=0.5)\n",
    "\n",
    "class KernelNet(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(KernelNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(32, 8)\n",
    "        self.fc2 = nn.Linear(8, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = rff1.transform(x)\n",
    "        x=self.fc1(x)\n",
    "        x = rff2.transform(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "#initialize\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.normal_(m.weight,mean=0,std=0.5)\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.uniform_(m.weight,a=-0.1,b=0.1)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "train_loaderset=[] # there are \"split\" elements\n",
    "netset=[]\n",
    "optimizerset=[]\n",
    "schedulerset=[]\n",
    "train_loss=[]\n",
    "test_loss=[]\n",
    "\n",
    "np.random.seed(0)\n",
    "row_rand_array = np.arange(train_x.shape[0])\n",
    "np.random.shuffle(row_rand_array)\n",
    "\n",
    "train_loaderset1=[]\n",
    "for l in range(layer): #split into different dataset\n",
    "    curx=train_x.values[row_rand_array[l*length:(l+1)*length]]\n",
    "    cury=train_y.values[row_rand_array[l*length:(l+1)*length]]\n",
    "    nnx = torch.from_numpy(curx).float()\n",
    "    nny = torch.squeeze(torch.from_numpy(cury).float()) \n",
    "    train_loader = DataLoader(mydataset(nnx, nny),batch_size=100, shuffle=True)\n",
    "    train_loaderset1.append(train_loader)\n",
    "    \n",
    "for i in range(split):\n",
    "    train_loaderset1=train_loaderset1[1:]+train_loaderset1[0:1]\n",
    "    train_loaderset.append(train_loaderset1)\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net = KernelNet()\n",
    "    net = net.to(device)\n",
    "    torch.manual_seed(1)\n",
    "    net.apply(init_weights)\n",
    "    netset.append(net)      \n",
    "   \n",
    "    optimizer1=[]\n",
    "    scheduler1=[]\n",
    "    optimizer1.append(optim.SGD([{'params': net.fc1.parameters(), 'initial_lr': 5e-3}],lr=5e-3,momentum=0.9,weight_decay=1e-4) )\n",
    "    scheduler1.append(torch.optim.lr_scheduler.StepLR(optimizer1[-1],step_size=50,gamma=0.5,last_epoch=300))\n",
    "    optimizer1.append(optim.SGD([{'params': net.fc2.parameters(), 'initial_lr': 5e-3}],lr=5e-3,momentum=0.9,weight_decay=1e-4) )\n",
    "    scheduler1.append(torch.optim.lr_scheduler.StepLR(optimizer1[-1],step_size=50,gamma=0.5,last_epoch=300))\n",
    "    optimizerset.append(optimizer1)\n",
    "    schedulerset.append(scheduler1)\n",
    "    \n",
    "    train_loss.append(0)\n",
    "    test_loss.append(0)\n",
    "    \n",
    "criterion=nn.MSELoss()    \n",
    "loss=[]\n",
    "splkernel_trainloss=[]\n",
    "splkernel_testloss=[]\n",
    "t0 = time.time()\n",
    "for epoch in range(500): \n",
    "    for i in range(split):\n",
    "        for l in range(layer):\n",
    "            for x, y in train_loaderset[i][l]: #for batch, (x, y) in enumerate(train_loader): \n",
    "                # Data Splitting\n",
    "                # every step, update a layer with smaller dataset \n",
    "                x, y = x.to(device), y.to(device)\n",
    "                # Compute prediction error\n",
    "                y_pred = netset[i](x)\n",
    "                y_pred = torch.squeeze(y_pred)\n",
    "                train_loss[i] = criterion(y_pred, y)\n",
    "                \n",
    "                # Backpropagation\n",
    "                optimizerset[i][l].zero_grad() \n",
    "                train_loss[i].backward()\n",
    "                optimizerset[i][l].step()\n",
    "            schedulerset[i][l].step() \n",
    "    \n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        for i in range(split):\n",
    "            y_test_pred = netset[i](x)\n",
    "            y_test_pred = torch.squeeze(y_test_pred)\n",
    "            \n",
    "            test_loss[i] = criterion(y_test_pred,y)\n",
    "    \n",
    "    if epoch>50 and float(sum(test_loss)/len(test_loss))>max(splkernel_testloss[-50:-1]):\n",
    "        break\n",
    "    \n",
    "    if epoch % 10 == 0: \n",
    "        print(f'''epoch {epoch}\n",
    "            Train set - loss: {sum(train_loss)/len(train_loss)}\n",
    "            Test  set - loss: {sum(test_loss)/len(test_loss)}\n",
    "            ''')\n",
    "    splkernel_trainloss.append(float(sum(train_loss)/len(train_loss)))\n",
    "    splkernel_testloss.append(float(sum(test_loss)/len(test_loss)))\n",
    "            \n",
    "        \n",
    "dnn_fit = time.time() - t0\n",
    "print(\"KernelNet(data splitting) complexity and model fitted in %.3f s\" % dnn_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14448, 273])\n",
      "torch.Size([14448, 273])\n",
      "n-p: 14175  mark: 5901\n",
      "length 2.598259856932876\n",
      "95 coverage 0.9442467378410438\n"
     ]
    }
   ],
   "source": [
    "#original\n",
    "#predict\n",
    "temp=[]\n",
    "parset=[]\n",
    "for spl in range(split):\n",
    "    par1=optimizerset[spl][0].param_groups[0]['params']\n",
    "    for jj in range(1,split):\n",
    "        par1=par1+optimizerset[spl][jj].param_groups[0]['params']\n",
    "    parset.append(par1)\n",
    "        \n",
    "for spl in range(split):\n",
    "    par=parset[spl]\n",
    "    for i in range(len(train_x)):\n",
    "        x0=torch.from_numpy(train_x[i:1+i].to_numpy()).float()\n",
    "        x0 = x0.to(device)\n",
    "        pred = netset[spl](x0)\n",
    "        fi=torch.tensor([])\n",
    "        for j in range(len(par)):\n",
    "            par[j].grad.data.zero_()\n",
    "        pred.backward()   \n",
    "        for j in range(len(par)): \n",
    "            fi=torch.cat([fi,par[j].grad.reshape(-1)])\n",
    "        fi=fi.reshape(1,-1)\n",
    "        if i==0:\n",
    "            Fi=fi\n",
    "        else:\n",
    "            Fi=torch.cat([Fi,fi])   \n",
    "    print(Fi.shape)\n",
    "    temp.append(torch.linalg.inv(Fi.T @ Fi))\n",
    "    \n",
    "length=[]\n",
    "coverage=0\n",
    "mark=0\n",
    "for i in range(len(test_x)):\n",
    "    fFFf=0\n",
    "    for spl in range(split):\n",
    "        par=parset[spl]\n",
    "        x0=torch.from_numpy(test_x[i:i+1].to_numpy()).float()\n",
    "        x0 = x0.to(device)\n",
    "        pred = netset[spl](x0)\n",
    "        #print(pred.detach().numpy()[0][0],test_y[i])\n",
    "        par=parset[spl]\n",
    "        f0=torch.tensor([])\n",
    "        for j in range(len(par)):\n",
    "            par[j].grad.data.zero_()\n",
    "        pred.backward()\n",
    "        for j in range(len(par)):\n",
    "            f0=torch.cat([f0,par[j].grad.reshape(-1)])\n",
    "        f0=f0.reshape(-1,1)\n",
    "        fFFf=fFFf+f0.T @ temp[spl] @ f0\n",
    "    \n",
    "    if fFFf < 0:\n",
    "        continue\n",
    "    mark=mark+1\n",
    "    dd=1.96*np.sqrt(float(fFFf/split**2+1))*np.sqrt(splkernel_trainloss[-1])\n",
    "    length.append(2*dd)\n",
    "    \n",
    "    #coverage\n",
    "    if pred.detach().numpy()[0][0]-dd<test_y[i] and pred.detach().numpy()[0][0]+dd>test_y[i]:\n",
    "        coverage=coverage+1\n",
    "coverage=coverage/mark\n",
    "\n",
    "print(\"n-p:\",len(train_x)-f0.shape[0],\" mark:\",mark) \n",
    "print(\"length\",np.mean(length))\n",
    "print(\"95 coverage\",coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "            Train set - loss: 2.1451306343078613\n",
      "            Test  set - loss: 1.7412149906158447\n",
      "            \n",
      "epoch 10\n",
      "            Train set - loss: 1.2084100246429443\n",
      "            Test  set - loss: 0.9962939023971558\n",
      "            \n",
      "epoch 20\n",
      "            Train set - loss: 1.0154739618301392\n",
      "            Test  set - loss: 0.787600040435791\n",
      "            \n",
      "epoch 30\n",
      "            Train set - loss: 0.7645998597145081\n",
      "            Test  set - loss: 0.6055824756622314\n",
      "            \n",
      "epoch 40\n",
      "            Train set - loss: 0.5447102785110474\n",
      "            Test  set - loss: 0.5004091262817383\n",
      "            \n",
      "epoch 50\n",
      "            Train set - loss: 0.6608152389526367\n",
      "            Test  set - loss: 0.40854114294052124\n",
      "            \n",
      "epoch 60\n",
      "            Train set - loss: 0.36724644899368286\n",
      "            Test  set - loss: 0.36062541604042053\n",
      "            \n",
      "epoch 70\n",
      "            Train set - loss: 0.5414552688598633\n",
      "            Test  set - loss: 0.33254751563072205\n",
      "            \n",
      "epoch 80\n",
      "            Train set - loss: 0.3757402002811432\n",
      "            Test  set - loss: 0.32623720169067383\n",
      "            \n",
      "epoch 90\n",
      "            Train set - loss: 0.3859059810638428\n",
      "            Test  set - loss: 0.32908809185028076\n",
      "            \n",
      "epoch 100\n",
      "            Train set - loss: 0.35962915420532227\n",
      "            Test  set - loss: 0.3218279480934143\n",
      "            \n",
      "epoch 110\n",
      "            Train set - loss: 0.4192446768283844\n",
      "            Test  set - loss: 0.3209873139858246\n",
      "            \n",
      "epoch 120\n",
      "            Train set - loss: 0.37767577171325684\n",
      "            Test  set - loss: 0.320393830537796\n",
      "            \n",
      "epoch 130\n",
      "            Train set - loss: 0.38736337423324585\n",
      "            Test  set - loss: 0.3236390948295593\n",
      "            \n",
      "epoch 140\n",
      "            Train set - loss: 0.40261149406433105\n",
      "            Test  set - loss: 0.3193463981151581\n",
      "            \n",
      "epoch 150\n",
      "            Train set - loss: 0.6173035502433777\n",
      "            Test  set - loss: 0.3205576539039612\n",
      "            \n",
      "ResKernelNet(data splitting) complexity and model fitted in 100.845 s\n"
     ]
    }
   ],
   "source": [
    "#res\n",
    "split=2\n",
    "layer=2\n",
    "length=int(len(train_x)/layer)\n",
    "test_loader = DataLoader(mydataset(nntest_x, nntest_y),batch_size=100, shuffle=False)\n",
    "\n",
    "rff0=RandomFourierFeature(8,32,kernel='C',gamma=0.05)\n",
    "rff1=RandomFourierFeature(8,8,kernel='G',gamma=0.5)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,infeatures,outfeatures,rff):\n",
    "        super(ResidualBlock,self).__init__()\n",
    "        self.infeatures = infeatures\n",
    "        self.outfeatures = outfeatures\n",
    "        self.rff=rff\n",
    "        \n",
    "        self.fc1 = nn.Linear(infeatures,outfeatures)\n",
    "        self.fc2 = nn.Linear(outfeatures,outfeatures)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        rff=self.rff\n",
    "        x = self.fc1(x)\n",
    "        y = rff.transform(x)\n",
    "        y = self.fc2(y)\n",
    "        return x+y\n",
    "\n",
    "class ResKernelNet(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(ResKernelNet, self).__init__()\n",
    "        self.rblock1 = ResidualBlock(32,8,rff1)\n",
    "        self.fc2 =nn.Linear(8,1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = rff0.transform(x)\n",
    "        x = self.rblock1(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "#initialize\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.normal_(m.weight,mean=0,std=0.5)\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.uniform_(m.weight,a=-0.1,b=0.1)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "train_loaderset=[] # there are \"split\" elements\n",
    "netset=[]\n",
    "optimizerset=[]\n",
    "schedulerset=[]\n",
    "train_loss=[]\n",
    "test_loss=[]\n",
    "\n",
    "np.random.seed(0)\n",
    "row_rand_array = np.arange(train_x.shape[0])\n",
    "np.random.shuffle(row_rand_array)\n",
    "\n",
    "train_loaderset1=[]\n",
    "for l in range(layer): #split into different dataset\n",
    "    curx=train_x.values[row_rand_array[l*length:(l+1)*length]]\n",
    "    cury=train_y.values[row_rand_array[l*length:(l+1)*length]]\n",
    "    nnx = torch.from_numpy(curx).float()\n",
    "    nny = torch.squeeze(torch.from_numpy(cury).float()) \n",
    "    train_loader = DataLoader(mydataset(nnx, nny),batch_size=100, shuffle=True)\n",
    "    train_loaderset1.append(train_loader)\n",
    "    \n",
    "for i in range(split):\n",
    "    train_loaderset1=train_loaderset1[1:]+train_loaderset1[0:1]\n",
    "    train_loaderset.append(train_loaderset1)\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net = ResKernelNet()\n",
    "    net = net.to(device)\n",
    "    torch.manual_seed(1)\n",
    "    net.apply(init_weights)\n",
    "    netset.append(net)      \n",
    "   \n",
    "    optimizer1=[]\n",
    "    scheduler1=[]\n",
    "    \n",
    "    optimizer1.append(optim.SGD([{'params': net.rblock1.parameters(), 'initial_lr': 5e-4}],lr=5e-4,momentum=0.9,weight_decay=1e-4) )\n",
    "    scheduler1.append(torch.optim.lr_scheduler.StepLR(optimizer1[-1],step_size=50,gamma=0.5,last_epoch=300))\n",
    "    optimizer1.append(optim.SGD([{'params': net.fc2.parameters(), 'initial_lr': 5e-4}],lr=5e-4,momentum=0.9,weight_decay=1e-4) )\n",
    "    scheduler1.append(torch.optim.lr_scheduler.StepLR(optimizer1[-1],step_size=50,gamma=0.5,last_epoch=300))\n",
    "    optimizerset.append(optimizer1)\n",
    "    schedulerset.append(scheduler1)\n",
    "    \n",
    "    train_loss.append(0)\n",
    "    test_loss.append(0)\n",
    "    \n",
    "criterion=nn.MSELoss()    \n",
    "loss=[]\n",
    "splreskernel_trainloss=[]\n",
    "splreskernel_testloss=[]\n",
    "t0 = time.time()\n",
    "for epoch in range(500): \n",
    "    for i in range(split):\n",
    "        for l in range(layer):\n",
    "            for x, y in train_loaderset[i][l]: #for batch, (x, y) in enumerate(train_loader): \n",
    "                # Data Splitting\n",
    "                # every step, update a layer with smaller dataset \n",
    "                x, y = x.to(device), y.to(device)\n",
    "                # Compute prediction error\n",
    "                y_pred = netset[i](x)\n",
    "                y_pred = torch.squeeze(y_pred)\n",
    "                train_loss[i] = criterion(y_pred, y)\n",
    "                \n",
    "                # Backpropagation\n",
    "                optimizerset[i][l].zero_grad() \n",
    "                train_loss[i].backward()\n",
    "                optimizerset[i][l].step()\n",
    "            \n",
    "    \n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        for i in range(split):\n",
    "            y_test_pred = netset[i](x)\n",
    "            y_test_pred = torch.squeeze(y_test_pred)\n",
    "            \n",
    "            test_loss[i] = criterion(y_test_pred,y)\n",
    "    \n",
    "    if epoch>50 and float(sum(test_loss)/len(test_loss))>max(splreskernel_testloss[-50:-1]):\n",
    "        break\n",
    "    \n",
    "    if epoch % 10 == 0: \n",
    "        print(f'''epoch {epoch}\n",
    "            Train set - loss: {sum(train_loss)/len(train_loss)}\n",
    "            Test  set - loss: {sum(test_loss)/len(test_loss)}\n",
    "            ''')\n",
    "    splreskernel_trainloss.append(float(sum(train_loss)/len(train_loss)))\n",
    "    splreskernel_testloss.append(float(sum(test_loss)/len(test_loss)))\n",
    "            \n",
    "        \n",
    "dnn_fit = time.time() - t0\n",
    "print(\"ResKernelNet(data splitting) complexity and model fitted in %.3f s\" % dnn_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14448, 345])\n",
      "torch.Size([14448, 345])\n",
      "n-p: 14103  mark: 4215\n",
      "length 2.2800333092223606\n",
      "95 coverage 0.9281138790035587\n"
     ]
    }
   ],
   "source": [
    "#original\n",
    "#predict\n",
    "temp=[]\n",
    "parset=[]\n",
    "for spl in range(split):\n",
    "    par1=optimizerset[spl][0].param_groups[0]['params']\n",
    "    for jj in range(1,split):\n",
    "        par1=par1+optimizerset[spl][jj].param_groups[0]['params']\n",
    "    parset.append(par1)\n",
    "        \n",
    "for spl in range(split):\n",
    "    par=parset[spl]\n",
    "    for i in range(len(train_x)):\n",
    "        x0=torch.from_numpy(train_x[i:1+i].to_numpy()).float()\n",
    "        x0 = x0.to(device)\n",
    "        pred = netset[spl](x0)\n",
    "        fi=torch.tensor([])\n",
    "        for j in range(len(par)):\n",
    "            par[j].grad.data.zero_()\n",
    "        pred.backward()   \n",
    "        for j in range(len(par)): \n",
    "            fi=torch.cat([fi,par[j].grad.reshape(-1)])\n",
    "        fi=fi.reshape(1,-1)\n",
    "        if i==0:\n",
    "            Fi=fi\n",
    "        else:\n",
    "            Fi=torch.cat([Fi,fi])   \n",
    "    print(Fi.shape)\n",
    "    temp.append(torch.linalg.inv(Fi.T @ Fi))\n",
    "    \n",
    "length=[]\n",
    "coverage=0\n",
    "mark=0\n",
    "for i in range(len(test_x)):\n",
    "    fFFf=0\n",
    "    for spl in range(split):\n",
    "        par=parset[spl]\n",
    "        x0=torch.from_numpy(test_x[i:i+1].to_numpy()).float()\n",
    "        x0 = x0.to(device)\n",
    "        pred = netset[spl](x0)\n",
    "        #print(pred.detach().numpy()[0][0],test_y[i])\n",
    "        par=parset[spl]\n",
    "        f0=torch.tensor([])\n",
    "        for j in range(len(par)):\n",
    "            par[j].grad.data.zero_()\n",
    "        pred.backward()\n",
    "        for j in range(len(par)):\n",
    "            f0=torch.cat([f0,par[j].grad.reshape(-1)])\n",
    "        f0=f0.reshape(-1,1)\n",
    "        fFFf=fFFf+f0.T @ temp[spl] @ f0\n",
    "    \n",
    "    if fFFf < 0:\n",
    "        continue\n",
    "    mark=mark+1\n",
    "    dd=1.96*np.sqrt(float(fFFf/split**2+1))*np.sqrt(splreskernel_testloss[-1])\n",
    "    length.append(2*dd)\n",
    "    \n",
    "    #coverage\n",
    "    if pred.detach().numpy()[0][0]-dd<test_y[i] and pred.detach().numpy()[0][0]+dd>test_y[i]:\n",
    "        coverage=coverage+1\n",
    "coverage=coverage/mark\n",
    "\n",
    "print(\"n-p:\",len(train_x)-f0.shape[0],\" mark:\",mark) \n",
    "print(\"length\",np.mean(length))\n",
    "print(\"95 coverage\",coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_choose(fig,start,data):\n",
    "    fig.plot(np.arange(start,len(data)),data[start:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "plot_choose(ax[0],10,dnn_testloss)\n",
    "plot_choose(ax[0],10,kernelnn_testloss)\n",
    "plot_choose(ax[0],10,splkernel_testloss)\n",
    "ax[0].legend([\"DNN\",\"Multi_Kernel\",\"+split\"])\n",
    "ax[0].set_xlabel(\"epoch\")\n",
    "ax[0].set_ylabel(\"Test MSE\")\n",
    "ax[0].set_title(\"Test MSE with different structure\")\n",
    "\n",
    "plot_choose(ax[0],10,res_testloss)\n",
    "plot_choose(ax[0],10,reskernel_testloss)\n",
    "plot_choose(ax[0],10,splreskernel_testloss)\n",
    "ax[0].legend([\"Resnet\",\"Residual_Multi_Kernel\",\"+split\"])\n",
    "ax[0].set_xlabel(\"epoch\")\n",
    "ax[0].set_ylabel(\"Test MSE\")\n",
    "ax[0].set_title(\"Test MSE with different structure\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
